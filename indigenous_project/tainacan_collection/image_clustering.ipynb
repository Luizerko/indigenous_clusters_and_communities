{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ef9795d-0158-4db9-8f3a-ec9139c1f258",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe columns: \n",
      "Index(['url', 'thumbnail', 'creation_date', 'modification_date',\n",
      "       'numero_do_item', 'tripticos', 'categoria', 'nome_do_item',\n",
      "       'nome_do_item_dic', 'colecao', 'coletor', 'doador', 'modo_de_aquisicao',\n",
      "       'data_de_aquisicao', 'ano_de_aquisicao', 'data_de_confeccao', 'autoria',\n",
      "       'nome_etnico', 'descricao', 'dimensoes', 'funcao', 'materia_prima',\n",
      "       'tecnica_confeccao', 'descritor_tematico', 'descritor_comum',\n",
      "       'numero_de_pecas', 'itens_relacionados', 'responsavel_guarda',\n",
      "       'inst_detentora', 'povo', 'autoidentificacao', 'lingua',\n",
      "       'estado_de_origem', 'geolocalizacao', 'pais_de_origem', 'exposicao',\n",
      "       'referencias', 'disponibilidade', 'qualificacao', 'historia_adm',\n",
      "       'notas_gerais', 'observacao', 'conservacao', 'image_path'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Loading dataset\n",
    "ind_df = pd.read_csv('data/indigenous_collection_processed.csv', index_col='id')\n",
    "print(f'Dataframe columns: \\n{ind_df.columns}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dc645b59-5f75-4644-8482-a2aeb836d2ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.magic import register_cell_magic\n",
    "\n",
    "# Creating skip cell command\n",
    "@register_cell_magic\n",
    "def skip(line, cell):\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "561deae7-07e5-412b-86b4-4f42b9ce3b52",
   "metadata": {},
   "source": [
    "# Image Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79dfc3e7-84fa-48da-a625-239a17b0e021",
   "metadata": {},
   "source": [
    "Clustering experiments with image feature extractors. The idea is to fine-tune some pre-trained models on our dataset and then remove the last layer of the model to cluster on the embedding space projections."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ecfe485-3be5-4694-8198-c50a3d03555e",
   "metadata": {},
   "source": [
    "## Dataset Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31a1f9c8-c490-4d69-a6e1-f8aa2d1280e7",
   "metadata": {},
   "source": [
    "For fine-tuning the model on our dataset, we are going to try a few different labels and study how they affect the generated emebdding space. For now, we focus *povo* and *categoria*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7ce8b405-dd10-44cd-8d6c-31d4c4d81131",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 corrupted images\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "\n",
    "# Filtering out corrupted images\n",
    "corrupted_images = []\n",
    "for index, row in ind_df.loc[ind_df['image_path'].notna()].iterrows():\n",
    "    try:\n",
    "        Image.open(row['image_path'])\n",
    "    except Exception as e:\n",
    "        # print(e)\n",
    "        corrupted_images.append(row['image_path'])\n",
    "        ind_df.loc[index, 'image_path'] = pd.NA\n",
    "print(f'{len(corrupted_images)} corrupted images')\n",
    "\n",
    "# Creating 'image_path_br' column\n",
    "ind_df['image_path_br'] = ind_df['image_path'].values\n",
    "ind_df.loc[ind_df['image_path_br'].notna(), 'image_path_br'] = \\\n",
    "    ind_df.loc[ind_df['image_path_br'].notna(), \\\n",
    "               'image_path'].apply(lambda path: \\\n",
    "                                   f\"data/br_images/{path.split('/')[-1].split('.')[0]}.png\")\n",
    "\n",
    "# Preparing labels for dataset training\n",
    "label_column = 'povo' # 'categoria', 'povo', 'ano_de_aquisicao'\n",
    "name_to_num = {c: i for i, c in enumerate(ind_df[label_column].unique())}\n",
    "labels = {row['image_path_br']: name_to_num[row[label_column]] \\\n",
    "          for index, row in ind_df.loc[ind_df['image_path_br'].notna()].iterrows()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c79fb07d-8b55-4074-9035-1cf0bbd9b9cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "# Getting the proper device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Creating the ImageDataset class and the DataLoader object to avoid loading all the images\n",
    "# simultaneously and run out of GPU memory\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, image_dir, labels, transform=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.image_files = [f for f in os.listdir(image_dir) \\\n",
    "                            if f.endswith(('.png', '.jpg', '.jpeg'))]\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path = os.path.join(self.image_dir, self.image_files[idx])\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        label = self.labels.get(self.image_files[idx], -1)\n",
    "        return image, label\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "])\n",
    "dataset = ImageDataset(\"data/br_images/\", labels, transform=transform)\n",
    "dataloader = DataLoader(dataset, batch_size=128, shuffle=True, num_workers=0, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd9804fc-d317-492a-88c1-72cad645cfb4",
   "metadata": {},
   "source": [
    "## ViT Base Patch-16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99dd4318-0161-463e-94dc-ba1987cdb366",
   "metadata": {},
   "source": [
    "### Pre-trained Embedding Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "34e247fc-988b-4850-b908-ed363357f2b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████| 89/89 [02:39<00:00,  1.80s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([11274, 768])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Projecting data onto the off-the-shelf pre-trained embedding space from ViT\n",
    "from transformers import ViTImageProcessor, ViTModel\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Loading model\n",
    "model = ViTModel.from_pretrained('google/vit-base-patch16-224-in21k')\n",
    "model.to(device)\n",
    "\n",
    "# Iterating over data\n",
    "image_embeddings = []\n",
    "for batch_images, _ in tqdm(dataloader):\n",
    "    batch_images = batch_images.to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(batch_images)\n",
    "    \n",
    "    # Do I get the last_hidden_state of CLS token or the pooler_output?\n",
    "    embeddings = outputs['last_hidden_state'][:, 0, :]\n",
    "    # embeddings = outputs['pooler_output']\n",
    "    image_embeddings.append(embeddings.cpu())\n",
    "\n",
    "    # # Freeing space\n",
    "    # del batch_images, outputs, embeddings\n",
    "    # torch.cuda.empty_cache()\n",
    "\n",
    "image_embeddings = torch.cat(image_embeddings, dim=0)\n",
    "image_embeddings.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3d4acc8b-4cab-4d5d-8c9c-a0651243e56a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing data projection\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f18e80f-1305-4675-8f89-e70f6ee00187",
   "metadata": {},
   "source": [
    "### Fine-tuning Embedding Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "397cc9e7-c214-4327-bc09-827d8b0413c2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
