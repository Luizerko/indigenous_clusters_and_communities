{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ef9795d-0158-4db9-8f3a-ec9139c1f258",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe columns: \n",
      "Index(['url', 'thumbnail', 'creation_date', 'modification_date',\n",
      "       'numero_do_item', 'tripticos', 'categoria', 'nome_do_item',\n",
      "       'nome_do_item_dic', 'colecao', 'coletor', 'doador', 'modo_de_aquisicao',\n",
      "       'data_de_aquisicao', 'ano_de_aquisicao', 'data_de_confeccao', 'autoria',\n",
      "       'nome_etnico', 'descricao', 'dimensoes', 'funcao', 'materia_prima',\n",
      "       'tecnica_confeccao', 'descritor_tematico', 'descritor_comum',\n",
      "       'numero_de_pecas', 'itens_relacionados', 'responsavel_guarda',\n",
      "       'inst_detentora', 'povo', 'autoidentificacao', 'lingua',\n",
      "       'estado_de_origem', 'geolocalizacao', 'pais_de_origem', 'exposicao',\n",
      "       'referencias', 'disponibilidade', 'qualificacao', 'historia_adm',\n",
      "       'notas_gerais', 'observacao', 'conservacao', 'image_path'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Loading dataset\n",
    "ind_df = pd.read_csv('data/indigenous_collection_processed.csv', index_col='id')\n",
    "print(f'Dataframe columns: \\n{ind_df.columns}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dc645b59-5f75-4644-8482-a2aeb836d2ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.magic import register_cell_magic\n",
    "\n",
    "# Creating skip cell command\n",
    "@register_cell_magic\n",
    "def skip(line, cell):\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "561deae7-07e5-412b-86b4-4f42b9ce3b52",
   "metadata": {},
   "source": [
    "# Image Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79dfc3e7-84fa-48da-a625-239a17b0e021",
   "metadata": {},
   "source": [
    "Clustering experiments with image feature extractors. The idea is to fine-tune some pre-trained models on our dataset and then remove the last layer of the model to cluster on the embedding space projections."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ecfe485-3be5-4694-8198-c50a3d03555e",
   "metadata": {},
   "source": [
    "## Dataset Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31a1f9c8-c490-4d69-a6e1-f8aa2d1280e7",
   "metadata": {},
   "source": [
    "For fine-tuning the model on our dataset, we are going to try a few different labels and study how they affect the generated emebdding space. For now, we focus *povo* and *categoria*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7ce8b405-dd10-44cd-8d6c-31d4c4d81131",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 corrupted images\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "\n",
    "# Filtering out corrupted images\n",
    "corrupted_images = []\n",
    "for index, row in ind_df.loc[ind_df['image_path'].notna()].iterrows():\n",
    "    try:\n",
    "        Image.open(row['image_path'])\n",
    "    except Exception as e:\n",
    "        corrupted_images.append(row['image_path'])\n",
    "        ind_df.loc[index, 'image_path'] = pd.NA\n",
    "print(f'{len(corrupted_images)} corrupted images')\n",
    "\n",
    "# Creating 'image_path_br' column\n",
    "ind_df['image_path_br'] = ind_df['image_path'].values\n",
    "ind_df.loc[ind_df['image_path_br'].notna(), 'image_path_br'] = \\\n",
    "    ind_df.loc[ind_df['image_path_br'].notna(), \\\n",
    "               'image_path'].apply(lambda path: \\\n",
    "                                   f\"data/br_images/{path.split('/')[-1].split('.')[0]}.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd9804fc-d317-492a-88c1-72cad645cfb4",
   "metadata": {},
   "source": [
    "## ViT Base Patch-16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99dd4318-0161-463e-94dc-ba1987cdb366",
   "metadata": {},
   "source": [
    "### Pre-trained Embedding Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8eb70f6d-7d44-40f5-b2f5-94053a767665",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import transforms\n",
    "from training_utils import preparing_image_labels, ImageDataset\n",
    "\n",
    "# Getting the proper device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Building dataset for column 'povo' (though no specific column is used on off-the-shelf model)\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "])\n",
    "labels, name_to_num, num_to_name = preparing_image_labels(ind_df, 'povo')\n",
    "dataset = ImageDataset(labels, transform=transform, augment=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e247fc-988b-4850-b908-ed363357f2b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing embeddings: 100%|████| 23/23 [02:40<00:00,  6.98s/it]\n",
      "/home/lui/anaconda3/envs/ind_thesis/lib/python3.10/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Projecting data onto the off-the-shelf pre-trained embedding space from ViT\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import ViTImageProcessor, ViTModel\n",
    "from training_utils import get_vit_embeddings, data_projections\n",
    "\n",
    "# Loading model\n",
    "model = ViTModel.from_pretrained('google/vit-base-patch16-224-in21k')\n",
    "model.to(device)\n",
    "\n",
    "# Getting data\n",
    "dataloader = DataLoader(dataset, batch_size=512, shuffle=True, num_workers=0, pin_memory=True)\n",
    "\n",
    "# Computing image embeddings\n",
    "image_embeddings = np.concatenate(get_vit_embeddings(model, dataloader, device), axis=0)\n",
    "\n",
    "# Computing data projection\n",
    "vanilla_vit_trimap, vanilla_vit_tsne, vanilla_vit_umap = data_projections(image_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e4ef9ea-b565-4e07-b343-c0a32c8709a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from training_utils import clean_mem\n",
    "\n",
    "# Cleaning up memory\n",
    "clean_mem([model, image_embeddings])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f18e80f-1305-4675-8f89-e70f6ee00187",
   "metadata": {},
   "source": [
    "### Fine-tuning Embedding Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "397cc9e7-c214-4327-bc09-827d8b0413c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating our own ViT classifier head for fine-tuning\n",
    "import torch.nn as nn\n",
    "\n",
    "class ViTClassifier(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(ViTClassifier, self).__init__()\n",
    "        self.vit = ViTModel.from_pretrained('google/vit-base-patch16-224-in21k')\n",
    "        self.classifier = nn.Linear(self.vit.config.hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        outputs = self.vit(x)\n",
    "        \n",
    "        # Do I get the last_hidden_state of CLS token or the pooler_output?\n",
    "        embeddings = outputs['last_hidden_state'][:, 0, :]\n",
    "        # embeddings = outputs['pooler_output']\n",
    "\n",
    "        logits = self.classifier(embeddings)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0093a1e-f85f-48b9-b75e-391fb8cd390b",
   "metadata": {},
   "source": [
    "#### *povo* Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f1eed02-7da5-4671-8136-390d5530246d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Counting categories\n",
    "categories = {}\n",
    "for l in labels.values():\n",
    "    try:\n",
    "        categories[l] += 1\n",
    "    except:\n",
    "        categories[l] = 1\n",
    "categories = dict(sorted(categories.items()))\n",
    "categories_keys = list(categories.keys())\n",
    "categories_freq = np.array(list(categories.values()))\n",
    "\n",
    "# Studying data distribution to filter out rare classes\n",
    "total_data = categories_freq.sum()\n",
    "q_25, q_50, q_75, q_90 = np.quantile(categories_freq, 0.25), \\\n",
    "np.quantile(categories_freq, 0.50), np.quantile(categories_freq, 0.75), \\\n",
    "np.quantile(categories_freq, 0.90)\n",
    "mask_25, mask_50, mask_75, mask_90 = np.where(categories_freq > q_25), \\\n",
    "np.where(categories_freq > q_50), np.where(categories_freq > q_75), \\\n",
    "np.where(categories_freq > q_90)\n",
    "\n",
    "print('Quantile X Data Percentage:')\n",
    "print(f'''Q-25: {q_25:.2f}, {categories_freq[mask_25].sum()/total_data*100:.2f}% of data''')\n",
    "print(f'''Q-50: {q_50:.2f}, {categories_freq[mask_50].sum()/total_data*100:.2f}% of data''')\n",
    "print(f'''Q-75: {q_75:.2f}, {categories_freq[mask_75].sum()/total_data*100:.2f}% of data''')\n",
    "print(f'''Q-90: {q_90:.2f}, {categories_freq[mask_90].sum()/total_data*100:.2f}% of data\\n''')\n",
    "\n",
    "# Filtering classes so that we retain around 85% of data\n",
    "filtered_categories = {}\n",
    "filtered_categories_names = {}\n",
    "for c in mask_75[0]:\n",
    "    filtered_categories[categories_keys[c]] = categories[categories_keys[c]]\n",
    "    filtered_categories_names[num_to_name[categories_keys[c]]] = categories[categories_keys[c]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd6a5e63-5680-4a09-a96e-c6fbb91b28b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import ConcatDataset\n",
    "\n",
    "# Filtering dataframe for selected categories\n",
    "filtered_povo_ind_df = ind_df[ind_df['povo'].isin(list(filtered_categories_names.keys())) & \\\n",
    "                              ind_df['image_path'].notna()]\n",
    "\n",
    "# Selecting minority and majority classes\n",
    "filtered_categories_freq = np.array(list(filtered_categories_names.values()))\n",
    "threshold = 2*np.median(filtered_categories_freq)\n",
    "\n",
    "minority_classes = []\n",
    "majority_classes = []\n",
    "for k, v in filtered_categories_names.items():\n",
    "    if v <= threshold:\n",
    "        minority_classes.append(k)\n",
    "    else:\n",
    "        majority_classes.append(k)\n",
    "\n",
    "minority_povo_ind_df=filtered_povo_ind_df[filtered_povo_ind_df['povo'].isin(minority_classes)]\n",
    "majority_povo_ind_df=filtered_povo_ind_df[filtered_povo_ind_df['povo'].isin(majority_classes)]\n",
    "\n",
    "# Undersampling majority classes\n",
    "undersampled_majority_povo_ind_df = (\n",
    "    majority_povo_ind_df\n",
    "    .groupby('povo', group_keys=False)\n",
    "    .apply(lambda x: x.sample(n=min(300, len(x)), replace=False))\n",
    ")\n",
    "\n",
    "# Creating augmented dataset for training\n",
    "labels_minority, _, _ = preparing_image_labels(minority_povo_ind_df, 'povo')\n",
    "labels_majority, _, _ = preparing_image_labels(undersampled_majority_povo_ind_df, 'povo')\n",
    "\n",
    "minority_multiplier = 2\n",
    "minority_datasets = [ImageDataset(labels_minority, transform=transform, augment=True) \\\n",
    "                     for i in range(minority_multiplier)]\n",
    "minority_datasets.append(ImageDataset(labels_minority, transform=transform, augment=False))\n",
    "\n",
    "majority_multiplier = 1\n",
    "majority_datasets = [ImageDataset(labels_majority, transform=transform, augment=True) \\\n",
    "                     for i in range(majority_multiplier)]\n",
    "majority_datasets.append(ImageDataset(labels_majority, transform=transform, augment=False))\n",
    "\n",
    "augmented_dataset = ConcatDataset(minority_datasets + majority_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d0fba47-a954-4cbc-b7ef-eed1ba38cd27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# labels_majority"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8af9ef38-1ea5-4ccb-b8df-c281525c86f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "dataloader = DataLoader(augmented_dataset, batch_size=16, shuffle=True, \\\n",
    "                        num_workers=0, pin_memory=True)\n",
    "print(len(dataloader))\n",
    "for batch_images, batch_labels, batch_idx in dataloader:\n",
    "    print(batch_idx[0])\n",
    "    # print()\n",
    "    mean=torch.tensor([0.5, 0.5, 0.5]).view(3,1,1)\n",
    "    std=torch.tensor([0.5, 0.5, 0.5]).view(3,1,1)\n",
    "    image = batch_images[0]*std + mean\n",
    "    plt.imshow(image.permute(1, 2, 0).numpy())\n",
    "    plt.axis('off')\n",
    "    plt.show\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be368210-2317-4991-aac4-12dc1fd6e9dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# Class weights in loss function\n",
    "num_classes = len(filtered_categories)\n",
    "class_weights = compute_class_weight(class_weight='balanced', classes=np.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "08fe3bdc-df2c-40ed-b0c7-a2ac7989e256",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training model:   0%|                   | 0/30 [04:10<?, ?it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 3, got 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 17\u001b[0m\n\u001b[1;32m     14\u001b[0m opt \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5e-5\u001b[39m, weight_decay\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     15\u001b[0m epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m30\u001b[39m\n\u001b[0;32m---> 17\u001b[0m losses, accuracies, class_precisions, class_recalls \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m                                                                 \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m                                                                 \u001b[49m\u001b[43mval_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m                                                                 \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m                                                                 \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mvit_povo\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m plot_train_curves(losses, accuracies, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mViT Fine-Tuned on \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpovo\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPer class precision: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclass_precisions[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/uva/thesis/indigenous_project/tainacan_collection/training_utils.py:164\u001b[0m, in \u001b[0;36mtrain_loop\u001b[0;34m(model, num_classes, train_dataloader, val_dataloader, device, criterion, opt, model_name, epochs)\u001b[0m\n\u001b[1;32m    161\u001b[0m     val_rec \u001b[38;5;241m=\u001b[39m rec_metric(all_preds, all_labels)\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[1;32m    163\u001b[0m accuracies\u001b[38;5;241m.\u001b[39mappend(torch\u001b[38;5;241m.\u001b[39mtensor(val_acc, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat16)\u001b[38;5;241m.\u001b[39mitem())\n\u001b[0;32m--> 164\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, prec, rec \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mzip\u001b[39m(val_prec, val_rec)):\n\u001b[1;32m    165\u001b[0m     class_precisions[i]\u001b[38;5;241m.\u001b[39mappend(torch\u001b[38;5;241m.\u001b[39mtensor(prec, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat16)\u001b[38;5;241m.\u001b[39mitem())\n\u001b[1;32m    166\u001b[0m     class_recalls[i]\u001b[38;5;241m.\u001b[39mappend(torch\u001b[38;5;241m.\u001b[39mtensor(rec, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat16)\u001b[38;5;241m.\u001b[39mitem())\n",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 3, got 2)"
     ]
    }
   ],
   "source": [
    "from training_utils import get_train_val_test_split, train_loop\n",
    "from training_utils import plot_train_curves, evaluate_model\n",
    "from torch.utils.data import random_split\n",
    "import torch.optim as optim\n",
    "\n",
    "# Creating training, validation and test datasets\n",
    "train_size = int(0.75*len(dataset))\n",
    "val_size = int(0.15*len(dataset))\n",
    "batch_size = 32\n",
    "train_dataloader, val_dataloader, \\\n",
    "test_dataloader = get_train_val_test_split(dataset, train_size, val_size, batch_size)\n",
    "\n",
    "# Training set-up and execution for 'povo'\n",
    "num_classes = ind_df['povo'].nunique()\n",
    "model = ViTClassifier(num_classes).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "opt = optim.Adam(model.parameters(), lr=5e-5, weight_decay=0)\n",
    "epochs = 30\n",
    "\n",
    "losses, accuracies, class_precisions, class_recalls = train_loop(model, num_classes, \\\n",
    "                                                                 train_dataloader, \\\n",
    "                                                                 val_dataloader, device, \\\n",
    "                                                                 criterion, opt, \\\n",
    "                                                                 'vit_povo', epochs)\n",
    "plot_train_curves(losses, accuracies, \"ViT Fine-Tuned on 'povo'\")\n",
    "print(f'Per class precision: {class_precisions[-1]}')\n",
    "print(f'Per class recall: {class_recalls[-1]}')\n",
    "\n",
    "# Evaluating model on test dataset\n",
    "test_acc, test_prec, test_rec = evaluate_model(model, 'vit_povo', num_classes, \\\n",
    "                                               test_dataloader, device)\n",
    "print(f'Test accuracy: {test_acc}')\n",
    "print(f'Test per class precisions: {test_prec}')\n",
    "print(f'Test per class recalls: {test_rec}')\n",
    "\n",
    "# Computing image embeddings\n",
    "model.classifier = nn.Identity()\n",
    "image_embeddings = np.concatenate(get_vit_embeddings(model, dataloader, device, True), axis=0)\n",
    "\n",
    "# Computing data projection\n",
    "povo_vit_trimap, povo_vit_tsne, povo_vit_umap = data_projections(image_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0871d528-56a5-4773-8e47-92426a19c1cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retraining model with augmented dataset to see the difference in the results\n",
    "train_size = int(0.75*len(augmented_dataset))\n",
    "val_size = int(0.15*len(augmented_dataset))\n",
    "batch_size = 32\n",
    "train_dataloader, val_dataloader, \\\n",
    "test_dataloader = get_train_val_test_split(augmented_dataset, train_size, val_size, batch_size)\n",
    "\n",
    "# Training set-up and execution for 'povo'\n",
    "num_classes = ind_df['povo'].nunique()\n",
    "model = ViTClassifier(num_classes).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "opt = optim.Adam(model.parameters(), lr=5e-5, weight_decay=0)\n",
    "epochs = 30\n",
    "\n",
    "losses, accuracies, class_precisions, class_recalls = train_loop(model, num_classes, \\\n",
    "                                                                 train_dataloader, \\\n",
    "                                                                 val_dataloader, device, \\\n",
    "                                                                 criterion, opt, \\\n",
    "                                                                 'balanced_vit_povo', epochs)\n",
    "plot_train_curves(losses, accuracies, \"ViT Fine-Tuned on 'povo'\")\n",
    "print(f'Per class precision: {class_precisions[-1]}')\n",
    "print(f'Per class recall: {class_recalls[-1]}')\n",
    "\n",
    "# Evaluating model on test dataset\n",
    "test_acc, test_prec, test_rec = evaluate_model(model, 'vit_povo', num_classes, \\\n",
    "                                               test_dataloader, device)\n",
    "print(f'Test accuracy: {test_acc}')\n",
    "print(f'Test per class precisions: {test_prec}')\n",
    "print(f'Test per class recalls: {test_rec}')\n",
    "\n",
    "# Computing image embeddings\n",
    "model.classifier = nn.Identity()\n",
    "image_embeddings = np.concatenate(get_vit_embeddings(model, dataloader, device, True), axis=0)\n",
    "\n",
    "# Computing data projection\n",
    "povo_vit_trimap, povo_vit_tsne, povo_vit_umap = data_projections(image_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef0a9205-5eaf-4bbe-ba8c-79f0d81653d8",
   "metadata": {},
   "source": [
    "#### *categoria* Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b47f319d-f46f-4197-bc9b-7006224bac68",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Cleaning up memory\n",
    "clean_mem([model, image_embeddings])\n",
    "\n",
    "# Preparing dataset for next training process\n",
    "labels, name_to_num, num_to_name = preparing_image_labels(ind_df, 'categoria')\n",
    "dataset = ImageDataset(labels, transform=transform)\n",
    "\n",
    "train_size = int(0.75*len(dataset))\n",
    "val_size = int(0.15*len(dataset))\n",
    "batch_size = 32\n",
    "train_dataloader, val_dataloader, \\\n",
    "test_dataloader = get_train_val_test_split(dataset, train_size, val_size, batch_size)\n",
    "\n",
    "# Training set-up and execution for 'categoria'\n",
    "num_classes = ind_df['categoria'].nunique()\n",
    "model = ViTClassifier(num_classes).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "opt = optim.Adam(model.parameters(), lr=2e-5, weight_decay=0)\n",
    "epochs = 30\n",
    "\n",
    "losses, accuracies, class_precisions, class_recalls = train_loop(model, num_classes, \\\n",
    "                                                                 train_dataloader, \\\n",
    "                                                                 val_dataloader, device, \\\n",
    "                                                                 criterion, opt, \\\n",
    "                                                                 'vit_categoria', epochs)\n",
    "plot_train_curves(losses, accuracies, \"ViT Fine-Tuned on 'categoria'\")\n",
    "print(f'Per class precision: {class_precisions[-1]}')\n",
    "print(f'Per class recall: {class_recalls[-1]}')\n",
    "\n",
    "# Evaluating model on test dataset\n",
    "test_acc, test_prec, test_rec = evaluate_model(model, 'vit_categoria', num_classes, \\\n",
    "                                               test_dataloader, device)\n",
    "print(f'Test accuracy: {test_acc}')\n",
    "print(f'Test per class precisions: {test_prec}')\n",
    "print(f'Test per class recalls: {test_rec}')\n",
    "\n",
    "# Computing image embeddings\n",
    "model.classifier = nn.Identity()\n",
    "image_embeddings = np.concatenate(get_vit_embeddings(model, dataloader, device, True), axis=0)\n",
    "\n",
    "# Computing data projection\n",
    "categoria_vit_trimap, categoria_vit_tsne, \\\n",
    "categoria_vit_umap = data_projections(image_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f58fdaa-04d0-44fc-93a0-5ebbf9b7a4d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning up memory\n",
    "clean_mem([model, image_embeddings])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81730faf-1ab3-4473-9268-04b25559bc14",
   "metadata": {},
   "source": [
    "### Visualizing and Comparing Projections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7622319c-08ac-475d-8376-c66bd3e76fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from training_utils import normalize\n",
    "\n",
    "# Normalizing data for later plot on tool\n",
    "norm_factor = 12\n",
    "vanilla_vit_trimap = normalize(vanilla_vit_trimap, norm_factor)\n",
    "vanilla_vit_tsne = normalize(vanilla_vit_tsne, norm_factor)\n",
    "vanilla_vit_umap = normalize(vanilla_vit_umap, norm_factor)\n",
    "\n",
    "povo_vit_trimap = normalize(povo_vit_trimap, norm_factor)\n",
    "povo_vit_tsne = normalize(povo_vit_tsne, norm_factor)\n",
    "povo_vit_umap = normalize(povo_vit_umap, norm_factor)\n",
    "\n",
    "categoria_vit_trimap = normalize(categoria_vit_trimap, norm_factor)\n",
    "categoria_vit_tsne = normalize(categoria_vit_tsne, norm_factor)\n",
    "categoria_vit_umap = normalize(categoria_vit_umap, norm_factor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63fce310-fb5f-4218-afb7-5ffcb02dbf1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing resulting projections\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.suptitle('Comparing Projections of ViT Models')\n",
    "\n",
    "# Plotting vanilla ViT projections\n",
    "for i, (vanilla_vit, proj_name) in enumerate(zip([vanilla_vit_trimap, \\\n",
    "                                                  vanilla_vit_tsne, vanilla_vit_umap], \\\n",
    "                                                 ['TriMap', 't-SNE', 'UMAP'])):\n",
    "    plt.subplot(3, 3, i+1)\n",
    "    plt.scatter(vanilla_vit[:, 0], vanilla_vit[:, 1], c='b')\n",
    "    plt.title(\"Vanilla ViT with \" + proj_name)\n",
    "    plt.xlabel(\"\")\n",
    "    plt.ylabel(\"\")\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "\n",
    "# Plotting ViT fine-tuned on 'povo' projections\n",
    "for i, (povo_vit, proj_name) in enumerate(zip([povo_vit_trimap, \\\n",
    "                                               povo_vit_tsne, povo_vit_umap], \\\n",
    "                                              ['TriMap', 't-SNE', 'UMAP'])):\n",
    "    plt.subplot(3, 3, i+4)\n",
    "    plt.scatter(povo_vit[:, 0], povo_vit[:, 1], c='r')\n",
    "    plt.title(\"ViT Fine-Tuned on 'povo' with \" + proj_name)\n",
    "    plt.xlabel(\"\")\n",
    "    plt.ylabel(\"\")\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "\n",
    "# Plotting ViT fine-tuned on 'categoria' projections\n",
    "for i, (categoria_vit, proj_name) in enumerate(zip([categoria_vit_trimap, \\\n",
    "                                                    categoria_vit_tsne, categoria_vit_umap], \\\n",
    "                                                   ['TriMap', 't-SNE', 'UMAP'])):\n",
    "    plt.subplot(3, 3, i+7)\n",
    "    plt.scatter(categoria_vit[:, 0], categoria_vit[:, 1], c='g')\n",
    "    plt.title(\"ViT Fine-Tuned on 'categoria' with \" + proj_name)\n",
    "    plt.xlabel(\"\")\n",
    "    plt.ylabel(\"\")\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d358406-41c1-40ba-8580-398ff90a1872",
   "metadata": {},
   "source": [
    "### Visualizing Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a072ba0-1e61-4288-b4ff-27ad22e1c3dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering dataframe to get only the part that contains images\n",
    "filtered_df = ind_df.loc[ind_df['image_path'].notna()]\n",
    "\n",
    "# Building colormap for cluster visualization\n",
    "column = 'categoria' # 'povo', 'categoria', 'ano_de_aquisicao'\n",
    "unique_values = filtered_df[column].unique()\n",
    "colors = plt.cm.gnuplot(np.linspace(0, 1, len(unique_values)))\n",
    "color_dict = {cluster: colors[i] for i, cluster in enumerate(unique_values)}\n",
    "\n",
    "# Plotting projections with clusters\n",
    "plt.figure(figsize=(10,4))\n",
    "\n",
    "for cluster in unique_values:\n",
    "    mask = filtered_df.index[filtered_df[column] == cluster].tolist()\n",
    "    sequential_indices = np.array([filtered_df.index.get_loc(idx) for idx in mask])\n",
    "    plt.scatter(categoria_vit_umap[sequential_indices, 0], \\\n",
    "                categoria_vit_umap[sequential_indices, 1], \n",
    "                color=color_dict[cluster], label=f\"{cluster.title()}\", alpha=0.7)\n",
    "\n",
    "plt.title(f\"Visualizing Clusters for Categoria on UMAP Projection\")\n",
    "plt.xlabel(\"\")\n",
    "plt.ylabel(\"\")\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.legend(title=\"Clusters\", bbox_to_anchor=(1.05, 1), loc=\"upper left\", \\\n",
    "           fontsize=8, frameon=True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4cda5f78-4cf1-4d8f-b366-48c92c2b340e",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 3, got 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[44], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m a, b, c \u001b[38;5;129;01min\u001b[39;00m dataloader:\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28mprint\u001b[39m(c)\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 3, got 2)"
     ]
    }
   ],
   "source": [
    "for a, b, c in dataloader:\n",
    "    print(c)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0aa67146-3566-4c89-b9ee-7fa183cabeb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "# Saving outputs for visualization tool\n",
    "# print(labels)\n",
    "mask1 = filtered_df.index[filtered_df[column] == unique_values[5]].tolist()\n",
    "mask2 = filtered_df.index[filtered_df[column] == unique_values[8]].tolist()\n",
    "\n",
    "sequence1 = np.array([filtered_df.index.get_loc(idx) for idx in mask1])\n",
    "sequence2 = np.array([filtered_df.index.get_loc(idx) for idx in mask2])\n",
    "\n",
    "print(len(set(sequence1).union(set(sequence2))) == len(sequence1)+len(sequence2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
