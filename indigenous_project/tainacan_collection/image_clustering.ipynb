{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ef9795d-0158-4db9-8f3a-ec9139c1f258",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe columns: \n",
      "Index(['url', 'thumbnail', 'creation_date', 'modification_date',\n",
      "       'numero_do_item', 'tripticos', 'categoria', 'nome_do_item',\n",
      "       'nome_do_item_dic', 'colecao', 'coletor', 'doador', 'modo_de_aquisicao',\n",
      "       'data_de_aquisicao', 'ano_de_aquisicao', 'data_de_confeccao', 'autoria',\n",
      "       'nome_etnico', 'descricao', 'dimensoes', 'funcao', 'materia_prima',\n",
      "       'tecnica_confeccao', 'descritor_tematico', 'descritor_comum',\n",
      "       'numero_de_pecas', 'itens_relacionados', 'responsavel_guarda',\n",
      "       'inst_detentora', 'povo', 'autoidentificacao', 'lingua',\n",
      "       'estado_de_origem', 'geolocalizacao', 'pais_de_origem', 'exposicao',\n",
      "       'referencias', 'disponibilidade', 'qualificacao', 'historia_adm',\n",
      "       'notas_gerais', 'observacao', 'conservacao', 'image_path'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Loading dataset\n",
    "ind_df = pd.read_csv('data/indigenous_collection_processed.csv', index_col='id')\n",
    "print(f'Dataframe columns: \\n{ind_df.columns}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dc645b59-5f75-4644-8482-a2aeb836d2ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.magic import register_cell_magic\n",
    "\n",
    "# Creating skip cell command\n",
    "@register_cell_magic\n",
    "def skip(line, cell):\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "561deae7-07e5-412b-86b4-4f42b9ce3b52",
   "metadata": {},
   "source": [
    "# Image Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79dfc3e7-84fa-48da-a625-239a17b0e021",
   "metadata": {},
   "source": [
    "Clustering experiments with image feature extractors. The idea is to fine-tune some pre-trained models on our dataset and then remove the last layer of the model to cluster on the embedding space projections."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ecfe485-3be5-4694-8198-c50a3d03555e",
   "metadata": {},
   "source": [
    "## Dataset Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31a1f9c8-c490-4d69-a6e1-f8aa2d1280e7",
   "metadata": {},
   "source": [
    "For fine-tuning the model on our dataset, we are going to try a few different labels and study how they affect the generated emebdding space. For now, we focus *povo* and *categoria*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7ce8b405-dd10-44cd-8d6c-31d4c4d81131",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 corrupted images\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "\n",
    "# Filtering out corrupted images\n",
    "corrupted_images = []\n",
    "for index, row in ind_df.loc[ind_df['image_path'].notna()].iterrows():\n",
    "    try:\n",
    "        Image.open(row['image_path'])\n",
    "    except Exception as e:\n",
    "        # print(e)\n",
    "        corrupted_images.append(row['image_path'])\n",
    "        ind_df.loc[index, 'image_path'] = pd.NA\n",
    "print(f'{len(corrupted_images)} corrupted images')\n",
    "\n",
    "# Creating 'image_path_br' column\n",
    "ind_df['image_path_br'] = ind_df['image_path'].values\n",
    "ind_df.loc[ind_df['image_path_br'].notna(), 'image_path_br'] = \\\n",
    "    ind_df.loc[ind_df['image_path_br'].notna(), \\\n",
    "               'image_path'].apply(lambda path: \\\n",
    "                                   f\"data/br_images/{path.split('/')[-1].split('.')[0]}.png\")\n",
    "\n",
    "# Preparing labels for dataset training\n",
    "label_column = 'povo' # 'categoria', 'povo', 'ano_de_aquisicao'\n",
    "name_to_num = {c: i for i, c in enumerate(ind_df[label_column].unique())}\n",
    "labels = {row['image_path_br']: name_to_num[row[label_column]] \\\n",
    "          for index, row in ind_df.loc[ind_df['image_path_br'].notna()].iterrows()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c79fb07d-8b55-4074-9035-1cf0bbd9b9cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "# Getting the proper device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Creating the ImageDataset class and the DataLoader object to avoid loading all the images\n",
    "# simultaneously and run out of GPU memory\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, image_dir, labels, transform=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.image_files = [f for f in os.listdir(image_dir) \\\n",
    "                            if f.endswith(('.png', '.jpg', '.jpeg'))]\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path = os.path.join(self.image_dir, self.image_files[idx])\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        label = self.labels.get(image_path, -1)\n",
    "        return image, label\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "])\n",
    "dataset = ImageDataset(\"data/br_images/\", labels, transform=transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd9804fc-d317-492a-88c1-72cad645cfb4",
   "metadata": {},
   "source": [
    "## ViT Base Patch-16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99dd4318-0161-463e-94dc-ba1987cdb366",
   "metadata": {},
   "source": [
    "### Pre-trained Embedding Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "34e247fc-988b-4850-b908-ed363357f2b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%skip\n",
    "\n",
    "# Projecting data onto the off-the-shelf pre-trained embedding space from ViT\n",
    "import numpy as np\n",
    "from transformers import ViTImageProcessor, ViTModel\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Loading model\n",
    "model = ViTModel.from_pretrained('google/vit-base-patch16-224-in21k')\n",
    "model.to(device)\n",
    "\n",
    "# Getting data\n",
    "dataloader = DataLoader(dataset, batch_size=512, shuffle=True, num_workers=0, pin_memory=True)\n",
    "\n",
    "# Function to iterating over data to get projections\n",
    "def get_embeddings(model, dataloader):\n",
    "    image_embeddings = []\n",
    "    for batch_images, _ in tqdm(dataloader, desc=\"Computing embeddings\"):\n",
    "        batch_images = batch_images.to(device)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(batch_images)\n",
    "        \n",
    "        # Do I get the last_hidden_state of CLS token or the pooler_output?\n",
    "        # embeddings = outputs['last_hidden_state'][:, 0, :]\n",
    "        embeddings = outputs['pooler_output']\n",
    "        image_embeddings.append(embeddings.cpu())\n",
    "    return image_embeddings\n",
    "\n",
    "image_embeddings = np.concatenate(get_embeddings(model, dataloader), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3d4acc8b-4cab-4d5d-8c9c-a0651243e56a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%skip\n",
    "\n",
    "# Computing data projection\n",
    "import trimap\n",
    "\n",
    "proj_trimap = trimap.TRIMAP(n_dims=2, n_inliers=12, n_outliers=6, n_random=3,\\\n",
    "                            weight_temp=0.5, lr=0.1, apply_pca=True)\n",
    "vanilla_vit = proj_trimap.fit_transform(image_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f18e80f-1305-4675-8f89-e70f6ee00187",
   "metadata": {},
   "source": [
    "### Fine-tuning Embedding Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "06a33606-708d-4b97-80bb-981e9df208e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# REMEMBER TO REMOVI THIS PART AFTERWARDS\n",
    "\n",
    "import numpy as np\n",
    "from transformers import ViTImageProcessor, ViTModel\n",
    "from tqdm import tqdm\n",
    "import trimap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "397cc9e7-c214-4327-bc09-827d8b0413c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating our own ViT classifier head for fine-tuning\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class ViTClassifier(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(ViTClassifier, self).__init__()\n",
    "        self.vit = ViTModel.from_pretrained('google/vit-base-patch16-224-in21k')\n",
    "        self.classifier = nn.Linear(self.vit.config.hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        outputs = self.vit(x)\n",
    "        \n",
    "        # Do I get the last_hidden_state of CLS token or the pooler_output?\n",
    "        # embeddings = outputs['last_hidden_state'][:, 0, :]\n",
    "        embeddings = outputs['pooler_output']\n",
    "\n",
    "        logits = self.classifier(embeddings)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4dd2aef6-87b8-4dc4-81c7-1d8ccb5f6a0d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training model:   1%|       | 1/100 [04:33<7:30:53, 273.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 1106.7620862722397\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training model:   2%|▏      | 2/100 [09:03<7:22:57, 271.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Loss: 682.0712050795555\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training model:   3%|▏      | 3/100 [13:31<7:16:29, 269.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Loss: 423.51108530163765\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training model:   4%|▎      | 4/100 [17:56<7:08:59, 268.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Loss: 244.93481071293354\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training model:   5%|▎      | 5/100 [22:21<7:02:46, 267.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, Loss: 133.12320867925882\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training model:   6%|▍      | 6/100 [26:47<6:57:36, 266.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6, Loss: 74.50854260474443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training model:   7%|▍      | 7/100 [31:12<6:52:34, 266.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7, Loss: 39.70178194437176\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training model:   8%|▌      | 8/100 [35:38<6:47:54, 266.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8, Loss: 23.51130084041506\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training model:   9%|▋      | 9/100 [40:04<6:43:24, 265.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9, Loss: 13.998039931757376\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training model:  10%|▌     | 10/100 [44:29<6:38:35, 265.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Loss: 9.269791238941252\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training model:  11%|▋     | 11/100 [48:54<6:33:42, 265.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11, Loss: 6.521042430307716\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training model:  12%|▋     | 12/100 [53:19<6:28:58, 265.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12, Loss: 4.623428250430152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training model:  13%|▊     | 13/100 [57:44<6:24:36, 265.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13, Loss: 3.5104437932604924\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training model:  14%|▌   | 14/100 [1:02:09<6:19:56, 265.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14, Loss: 2.945191613282077\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training model:  15%|▌   | 15/100 [1:06:34<6:15:38, 265.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15, Loss: 67.57821324456017\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training model:  16%|▋   | 16/100 [1:10:59<6:11:16, 265.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16, Loss: 24.187576212454587\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training model:  17%|▋   | 17/100 [1:15:24<6:06:48, 265.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17, Loss: 7.673260277835652\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training model:  18%|▋   | 18/100 [1:19:50<6:02:41, 265.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18, Loss: 2.3075701690395363\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training model:  19%|▊   | 19/100 [1:24:15<5:58:00, 265.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19, Loss: 2.0234777142759413\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training model:  20%|▊   | 20/100 [1:28:40<5:53:39, 265.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20, Loss: 1.940120876795845\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training model:  21%|▊   | 21/100 [1:33:05<5:49:02, 265.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21, Loss: 1.306014057714492\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training model:  22%|▉   | 22/100 [1:37:30<5:44:31, 265.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22, Loss: 1.3107390307995956\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training model:  23%|▉   | 23/100 [1:41:56<5:40:40, 265.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23, Loss: 38.9049574767414\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training model:  24%|▉   | 24/100 [1:46:21<5:35:51, 265.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24, Loss: 17.384929275140166\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training model:  25%|█   | 25/100 [1:50:46<5:31:25, 265.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25, Loss: 3.154495512484573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training model:  26%|█   | 26/100 [1:55:11<5:26:57, 265.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26, Loss: 2.124788953922689\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training model:  27%|█   | 27/100 [1:59:36<5:22:37, 265.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27, Loss: 2.448073112696875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training model:  28%|█   | 28/100 [2:04:01<5:18:11, 265.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28, Loss: 3.648298494226765\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training model:  29%|█▏  | 29/100 [2:08:27<5:13:48, 265.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29, Loss: 11.853724699496524\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training model:  30%|█▏  | 30/100 [2:12:51<5:09:07, 264.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30, Loss: 14.2432885003509\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training model:  31%|█▏  | 31/100 [2:17:16<5:04:46, 265.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31, Loss: 3.7777347871742677\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training model:  32%|█▎  | 32/100 [2:21:41<5:00:12, 264.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32, Loss: 6.403127712314017\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training model:  33%|█▎  | 33/100 [2:26:06<4:55:53, 264.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33, Loss: 10.14333004981745\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training model:  34%|█▎  | 34/100 [2:30:31<4:51:22, 264.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34, Loss: 6.341247495263815\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training model:  35%|█▍  | 35/100 [2:34:56<4:47:09, 265.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35, Loss: 3.7394473184249364\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training model:  36%|█▍  | 36/100 [2:39:21<4:42:37, 264.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36, Loss: 11.23506930487929\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training model:  37%|█▍  | 37/100 [2:43:47<4:38:26, 265.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37, Loss: 2.4975830617768224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training model:  38%|█▌  | 38/100 [2:48:12<4:33:59, 265.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38, Loss: 1.9040524677548092\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training model:  39%|█▌  | 39/100 [2:52:37<4:29:34, 265.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39, Loss: 1.2415674035437405\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training model:  40%|█▌  | 40/100 [2:57:02<4:25:05, 265.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40, Loss: 0.8575672517763451\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training model:  41%|█▋  | 41/100 [3:01:27<4:20:35, 265.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 41, Loss: 0.7519402327816351\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training model:  42%|█▋  | 42/100 [3:05:52<4:16:17, 265.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 42, Loss: 0.7512947954965057\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training model:  43%|█▋  | 43/100 [3:10:17<4:11:55, 265.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 43, Loss: 0.7757628343460965\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training model:  44%|█▊  | 44/100 [3:14:43<4:07:35, 265.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 44, Loss: 0.7183772459902684\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training model:  45%|█▊  | 45/100 [3:19:09<4:03:18, 265.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45, Loss: 0.7762741249098326\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training model:  46%|█▊  | 46/100 [3:23:33<3:58:41, 265.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 46, Loss: 0.6612264084615163\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training model:  47%|█▉  | 47/100 [3:27:58<3:54:10, 265.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47, Loss: 0.7017418253599317\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training model:  48%|█▉  | 48/100 [3:32:24<3:49:47, 265.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 48, Loss: 0.7741451970905473\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training model:  49%|█▉  | 49/100 [3:36:49<3:45:20, 265.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49, Loss: 0.6838422860892024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training model:  50%|██  | 50/100 [3:41:14<3:40:59, 265.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50, Loss: 3.724436603632057\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training model:  51%|██  | 51/100 [3:45:39<3:36:32, 265.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 51, Loss: 43.46592753258301\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training model:  52%|██  | 52/100 [3:50:04<3:32:05, 265.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 52, Loss: 5.4184298063628376\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training model:  53%|██  | 53/100 [3:54:29<3:27:36, 265.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 53, Loss: 2.4749388908676337\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training model:  54%|██▏ | 54/100 [3:58:54<3:23:16, 265.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 54, Loss: 0.7032253124489216\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training model:  55%|██▏ | 55/100 [4:03:19<3:18:49, 265.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 55, Loss: 0.5909842712717364\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training model:  56%|██▏ | 56/100 [4:07:44<3:14:22, 265.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 56, Loss: 0.5805632082410739\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training model:  57%|██▎ | 57/100 [4:12:10<3:10:03, 265.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 57, Loss: 0.581200719345361\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training model:  58%|██▎ | 58/100 [4:16:35<3:05:42, 265.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 58, Loss: 0.5720634800600237\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training model:  59%|██▎ | 59/100 [4:21:01<3:01:21, 265.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59, Loss: 0.590335943561513\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training model:  60%|██▍ | 60/100 [4:25:27<2:56:58, 265.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 60, Loss: 0.5960298385180067\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training model:  61%|██▍ | 61/100 [4:29:52<2:52:37, 265.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 61, Loss: 0.5661173572370899\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training model:  62%|██▍ | 62/100 [4:34:18<2:48:12, 265.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 62, Loss: 0.6212568792107049\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training model:  63%|██▌ | 63/100 [4:38:44<2:43:49, 265.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 63, Loss: 0.6096626746257243\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training model:  64%|██▌ | 64/100 [4:43:08<2:39:12, 265.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 64, Loss: 0.5529065449554764\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training model:  65%|██▌ | 65/100 [4:47:34<2:34:44, 265.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 65, Loss: 0.5981217518383346\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training model:  66%|██▋ | 66/100 [4:51:59<2:30:19, 265.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 66, Loss: 0.573319471150171\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training model:  67%|██▋ | 67/100 [4:56:24<2:25:50, 265.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 67, Loss: 0.5932676561569679\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training model:  68%|██▋ | 68/100 [5:00:49<2:21:28, 265.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 68, Loss: 0.5786407347877685\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training model:  69%|██▊ | 69/100 [5:05:14<2:17:03, 265.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 69, Loss: 0.6661948120690795\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training model:  70%|██▊ | 70/100 [5:09:40<2:12:35, 265.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 70, Loss: 36.15616097455313\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training model:  71%|██▊ | 71/100 [5:14:04<2:08:06, 265.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 71, Loss: 17.655871852097334\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training model:  72%|██▉ | 72/100 [5:18:29<2:03:41, 265.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 72, Loss: 2.9922120133705903\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training model:  73%|██▉ | 73/100 [5:22:55<1:59:19, 265.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 73, Loss: 0.9442345785791986\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training model:  74%|██▉ | 74/100 [5:27:20<1:54:56, 265.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 74, Loss: 0.5539465547408327\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training model:  75%|███ | 75/100 [5:31:45<1:50:30, 265.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Loss: 0.5110765849385643\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training model:  76%|███ | 76/100 [5:36:11<1:46:06, 265.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 76, Loss: 0.5360656025004573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training model:  77%|███ | 77/100 [5:40:36<1:41:41, 265.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 77, Loss: 0.5350302212464157\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training model:  78%|███ | 78/100 [5:45:02<1:37:19, 265.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 78, Loss: 0.5062463817448588\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training model:  79%|███▏| 79/100 [5:49:27<1:32:51, 265.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 79, Loss: 0.5267671894325758\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training model:  80%|███▏| 80/100 [5:53:52<1:28:24, 265.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 80, Loss: 0.5102711971048848\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training model:  81%|███▏| 81/100 [5:58:17<1:23:58, 265.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 81, Loss: 0.5277219625349971\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training model:  82%|███▎| 82/100 [6:02:42<1:19:34, 265.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 82, Loss: 0.5417623401917808\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training model:  83%|███▎| 83/100 [6:07:08<1:15:10, 265.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 83, Loss: 0.5496685110629187\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training model:  84%|███▎| 84/100 [6:11:34<1:10:47, 265.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 84, Loss: 0.542251444348949\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training model:  85%|███▍| 85/100 [6:15:59<1:06:20, 265.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 85, Loss: 0.5284906592496554\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training model:  86%|███▍| 86/100 [6:20:24<1:01:56, 265.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 86, Loss: 0.545947688015076\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training model:  87%|█████▏| 87/100 [6:24:50<57:31, 265.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 87, Loss: 0.5564359660384071\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training model:  88%|█████▎| 88/100 [6:29:15<53:04, 265.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 88, Loss: 0.565048103524532\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training model:  89%|█████▎| 89/100 [6:33:40<48:37, 265.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 89, Loss: 0.5336749647321994\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training model:  90%|█████▍| 90/100 [6:38:06<44:13, 265.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 90, Loss: 0.5640550963107671\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training model:  91%|█████▍| 91/100 [6:42:31<39:49, 265.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 91, Loss: 0.5836756804128527\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training model:  92%|█████▌| 92/100 [6:46:57<35:23, 265.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 92, Loss: 0.6111725107602979\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training model:  93%|█████▌| 93/100 [6:51:22<30:58, 265.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 93, Loss: 44.02104978986608\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training model:  94%|█████▋| 94/100 [6:55:48<26:32, 265.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 94, Loss: 6.223761160072172\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training model:  95%|█████▋| 95/100 [7:00:14<22:07, 265.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 95, Loss: 2.179058180670836\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training model:  96%|█████▊| 96/100 [7:04:38<17:41, 265.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 96, Loss: 0.6687080589181278\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training model:  97%|█████▊| 97/100 [7:09:03<13:15, 265.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 97, Loss: 0.48848807318427134\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training model:  98%|█████▉| 98/100 [7:13:28<08:50, 265.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 98, Loss: 0.48489707658882253\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training model:  99%|█████▉| 99/100 [7:17:54<04:25, 265.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 99, Loss: 0.4746606232802151\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training model: 100%|█████| 100/100 [7:22:20<00:00, 265.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Loss: 0.4756871135250549\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from torchmetrics.classification import Accuracy, Precision\n",
    "\n",
    "# Training function\n",
    "def train_loop(model, num_classes, dataloader, epochs=20):\n",
    "    losses = []\n",
    "    accuracies = []\n",
    "    class_precisions = [[] for i in range(num_classes)]\n",
    "\n",
    "    # Early-stopping set up\n",
    "    best_val_acc = 0\n",
    "    patience = max(2, int(0.05*epochs))\n",
    "    patience_counter = 0\n",
    "    tolerance = 0.01\n",
    "\n",
    "    acc_metric = Accuracy(task=\"multiclass\", num_classes=num_classes).to(device)\n",
    "    prec_metric = Precision(task=\"multiclass\", num_classes=num_classes, \\\n",
    "                            average=None).to(device)\n",
    "    \n",
    "    for epoch in tqdm(range(epochs), desc=f\"Training model\", leave=True):\n",
    "        model.train()\n",
    "        epoch_loss = .0\n",
    "        for batch_images, batch_labels in dataloader:\n",
    "            batch_images, batch_labels = batch_images.to(device), batch_labels.to(device)\n",
    "            \n",
    "            opt.zero_grad()\n",
    "            logits = model(batch_images)\n",
    "            loss = criterion(logits, batch_labels)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "            # # Freeing space\n",
    "            # del batch_images, batch_labels, logits, loss\n",
    "            # torch.cuda.empty_cache()\n",
    "\n",
    "        losses.append(torch.tensor(epoch_loss, dtype=torch.float16).item())\n",
    "\n",
    "        # Validation set for early-stopping on metrics that are not directly optimized\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            all_preds = []\n",
    "            all_labels = []\n",
    "\n",
    "            for batch_images, batch_labels in val_dataloader:\n",
    "                logits = model(batch_images)\n",
    "                preds = torch.argmax(logits, dim=1)\n",
    "                \n",
    "                all_preds.append(preds)\n",
    "                all_labels.append(batch_labels)\n",
    "            \n",
    "            all_preds = torch.cat(all_preds)\n",
    "            all_labels = torch.cat(all_labels)\n",
    "            \n",
    "            val_acc = acc_metric(all_preds, all_labels).item()\n",
    "            val_prec = prec_metric(all_preds, all_labels).tolist()\n",
    "\n",
    "        accuracies.append(torch.tensor(val_acc, dtype=torch.float16).item())\n",
    "        for i, prec in enumerate(val_prec):\n",
    "            class_precisions[i].append(torch.tensor(prec, dtype=torch.float16).item())\n",
    "\n",
    "         # Early-stopping check\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            patience_counter = 0\n",
    "        \n",
    "        elif best_val_acc-val_acc > tolerance:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                tqdm.write(\"Early-stopping training.\")\n",
    "                break\n",
    "        \n",
    "        tqdm.write((f'Epoch {epoch+1}, Loss: {batch_loss:.4f}, '\n",
    "                    f'Validation Accuracy: {val_acc:.4f}'))\n",
    "\n",
    "    return losses, accuracies, class_precisions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08fe3bdc-df2c-40ed-b0c7-a2ac7989e256",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import random_split\n",
    "\n",
    "# Creating training and validation datasets\n",
    "train_size = int(0.85*len(dataset))\n",
    "val_size = len(dataset)-train_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size], \\\n",
    "                                          generator=torch.Generator().manual_seed(42))\n",
    "\n",
    "batch_size = 32\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, \\\n",
    "                              num_workers=0, pin_memory=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True, \\\n",
    "                            num_workers=0, pin_memory=True)\n",
    "\n",
    "# Training set-up and execution\n",
    "num_classes = ind_df['povo'].nunique()\n",
    "model = ViTClassifier(num_classes).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "opt = optim.Adam(model.parameters(), lr=5e-5, weight_decay=0)\n",
    "epochs = 100\n",
    "\n",
    "train_loop(model, num_classes, dataloader, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d25fb2-5b39-42f6-accd-60f647072303",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing image embeddings\n",
    "image_embeddings = np.concatenate(get_embeddings(model, dataloader), axis=0)\n",
    "\n",
    "# Computing data projection\n",
    "proj_trimap = trimap.TRIMAP(n_dims=2, n_inliers=12, n_outliers=6, n_random=3,\\\n",
    "                            weight_temp=0.5, lr=0.1, apply_pca=True)\n",
    "povo_vit = proj_trimap.fit_transform(image_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81730faf-1ab3-4473-9268-04b25559bc14",
   "metadata": {},
   "source": [
    "### Visualizing and Comparing Projections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63fce310-fb5f-4218-afb7-5ffcb02dbf1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing resulting projections\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(8,4))\n",
    "plt.suptitle('Comparing Projections of ViT Models')\n",
    "\n",
    "# Plotting vanilla ViT projections\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(vanilla_vit[:, 0], vanilla_vit[:, 1], c='b')\n",
    "plt.title(\"Vanilla ViT\")\n",
    "plt.xlabel(\"\")\n",
    "plt.ylabel(\"\")\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "\n",
    "# Plotting ViT fine-tuned on 'povo' projections\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(povo_vit[:, 0], povo_vit[:, 1], c='b')\n",
    "plt.title(\"ViT Fine-Tuned on Povo\")\n",
    "plt.xlabel(\"\")\n",
    "plt.ylabel(\"\")\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
