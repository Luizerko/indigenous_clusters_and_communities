{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d266052-1700-4a2e-959d-ecc9686d5d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Loading dataset\n",
    "ind_df = pd.read_csv('data/indigenous_collection.csv', index_col='id')\n",
    "print(f'Dataframe columns: \\n{ind_df.columns}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "390a784f-379a-4e12-aa7c-b6db239ac339",
   "metadata": {},
   "source": [
    "## Transforming Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b58c475-931f-4ad5-9316-0dc3c8088da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "\n",
    "# Visualizing first image\n",
    "first_image = ind_df['image_path'].loc[55663]\n",
    "plt.imshow(cv2.imread(first_image))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7fb83fb-e608-46aa-b5e0-4fa3c717886c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import scipy as scp\n",
    "import numpy as np\n",
    "\n",
    "# Extracting height and width information from entire dataset for later processing\n",
    "dimensions = {'height': [], 'width': []}\n",
    "for index, value in tqdm(ind_df['image_path'].dropna().items(), total=len(ind_df['image_path'].dropna()), desc='Reading images'):\n",
    "    try:\n",
    "        image = cv2.imread(value)\n",
    "        dimensions['height'].append(image.shape[0])\n",
    "        dimensions['width'].append(image.shape[1])\n",
    "    except:\n",
    "        print('Corrupted image: {}'.format(value))\n",
    "\n",
    "print('''\\nStatistics on height of images: \\n{}\n",
    "quantiles (0.25, 0.50, 0.75, 0.90) = {}, {}, {}, {}'''.format(scp.stats.describe(dimensions['height']), \\\n",
    "                                                              np.quantile(dimensions['height'], 0.25), \\\n",
    "                                                              np.quantile(dimensions['height'], 0.50), \\\n",
    "                                                              np.quantile(dimensions['height'], 0.75), \\\n",
    "                                                              np.quantile(dimensions['height'], 0.90)))\n",
    "print('''\\nStatistics on width of images: \\n{}\n",
    "quantiles (0.25, 0.50, 0.75, 0.90) = {}, {}, {}, {}'''.format(scp.stats.describe(dimensions['width']), \\\n",
    "                                                              np.quantile(dimensions['width'], 0.25), \\\n",
    "                                                              np.quantile(dimensions['width'], 0.50), \\\n",
    "                                                              np.quantile(dimensions['width'], 0.75), \\\n",
    "                                                              np.quantile(dimensions['width'], 0.90)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58b17a46-154d-4e46-9972-e07cb769b0e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from transformers import pipeline, AutoModelForImageSegmentation\n",
    "\n",
    "# UPDATE REQUIREMENTS IN WITH: scikit-image, timm, kornia\n",
    "\n",
    "# Removing backgroung\n",
    "seg_pipe = pipeline('image-segmentation', model='briaai/RMBG-1.4', trust_remote_code=True)\n",
    "pillow_image = seg_pipe(first_image)\n",
    "pillow_image.save(\"first_image_br_r.png\")\n",
    "\n",
    "\n",
    "model = AutoModelForImageSegmentation.from_pretrained('briaai/RMBG-2.0', trust_remote_code=True)\n",
    "torch.set_float32_matmul_precision(['high', 'highest'][0])\n",
    "model.to('cuda')\n",
    "model.eval()\n",
    "\n",
    "image_size = (np.quantile(dimensions['width'], 0.75), np.quantile(dimensions['height'], 0.75))\n",
    "transform_image = transforms.Compose([\n",
    "    transforms.Resize(image_size),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "image = Image.open(first_image)\n",
    "input_images = transform_image(image).unsqueeze(0).to('cuda')\n",
    "\n",
    "with torch.no_grad():\n",
    "    preds = model(input_images)[-1].sigmoid().cpu()\n",
    "pred = preds[0].squeeze()\n",
    "pred_pil = transforms.ToPILImage()(pred)\n",
    "mask = pred_pil.resize(image.size)\n",
    "image.putalpha(mask)\n",
    "image.save(\"first_image_br_r.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ind_thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
