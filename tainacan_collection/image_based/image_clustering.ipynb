{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef9795d-0158-4db9-8f3a-ec9139c1f258",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Loading dataset\n",
    "ind_df = pd.read_csv('data/indigenous_collection_processed.csv', index_col='id')\n",
    "print(f'Dataframe columns: \\n{ind_df.columns}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc645b59-5f75-4644-8482-a2aeb836d2ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "from IPython.core.magic import register_cell_magic\n",
    "\n",
    "# Creating skip cell command\n",
    "@register_cell_magic\n",
    "def skip(line, cell):\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd1297d5-556c-4811-b8d4-1761e8750542",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Centralizing main imports so we can run the models separately\n",
    "import random\n",
    "from tqdm.notebook import tqdm\n",
    "from PIL import Image\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from image_training_utils import *\n",
    "\n",
    "# import image_training_utils\n",
    "# importlib.reload(image_training_utils)\n",
    "# from image_training_utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "561deae7-07e5-412b-86b4-4f42b9ce3b52",
   "metadata": {},
   "source": [
    "# Image Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79dfc3e7-84fa-48da-a625-239a17b0e021",
   "metadata": {},
   "source": [
    "Clustering experiments with image feature extractors. The idea is to fine-tune some pre-trained transformer models on our dataset and then remove the last layer of the model to cluster on the embedding space projections."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ecfe485-3be5-4694-8198-c50a3d03555e",
   "metadata": {},
   "source": [
    "## Dataset Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31a1f9c8-c490-4d69-a6e1-f8aa2d1280e7",
   "metadata": {},
   "source": [
    "For fine-tuning the model on our dataset, we are going to try a couple different labels (*povo* and *categoria*) and study how they affect the generated emebdding space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ce8b405-dd10-44cd-8d6c-31d4c4d81131",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering out corrupted images\n",
    "corrupted_images = []\n",
    "for index, row in ind_df.loc[ind_df['image_path'].notna()].iterrows():\n",
    "    try:\n",
    "        Image.open(row['image_path'])\n",
    "    except Exception as e:\n",
    "        corrupted_images.append(row['image_path'])\n",
    "        ind_df.loc[index, 'image_path'] = pd.NA\n",
    "print(f'{len(corrupted_images)} corrupted images')\n",
    "\n",
    "# Creating 'image_path_br' column\n",
    "ind_df['image_path_br'] = ind_df['image_path'].values\n",
    "ind_df.loc[ind_df['image_path_br'].notna(), 'image_path_br'] = \\\n",
    "    ind_df.loc[ind_df['image_path_br'].notna(), \\\n",
    "               'image_path'].apply(lambda path: \\\n",
    "                                   f\"data/br_images/{path.split('/')[-1].split('.')[0]}.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd9804fc-d317-492a-88c1-72cad645cfb4",
   "metadata": {},
   "source": [
    "## ViT Base Patch-16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99dd4318-0161-463e-94dc-ba1987cdb366",
   "metadata": {},
   "source": [
    "### Pre-trained Embedding Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eb70f6d-7d44-40f5-b2f5-94053a767665",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Getting the proper device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Building dataset for column 'povo' (though no specific column is used on off-the-shelf model)\n",
    "vit_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "])\n",
    "povo_labels, povo_name_to_num, povo_num_to_name = preparing_image_labels(ind_df, 'povo')\n",
    "povo_dataset = ImageDataset(povo_labels, transform=vit_transform, augment=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e247fc-988b-4850-b908-ed363357f2b7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Projecting data onto the off-the-shelf pre-trained embedding space from ViT\n",
    "from transformers import ViTModel\n",
    "\n",
    "# Loading model\n",
    "model = ViTModel.from_pretrained('google/vit-base-patch16-224-in21k')\n",
    "model.to(device)\n",
    "\n",
    "# Getting data\n",
    "povo_dataloader = DataLoader(povo_dataset, batch_size=512, shuffle=True, \\\n",
    "                             num_workers=0, pin_memory=True)\n",
    "\n",
    "# Computing image embeddings\n",
    "image_embeddings, _ = get_embeddings(model, povo_dataloader, device)\n",
    "image_embeddings = np.concatenate(image_embeddings, axis=0)\n",
    "\n",
    "# Computing data projection\n",
    "vanilla_vit_trimap, vanilla_vit_tsne, vanilla_vit_umap = data_projections(image_embeddings)\n",
    "\n",
    "# Cleaning up memory\n",
    "clean_mem([model, image_embeddings])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f18e80f-1305-4675-8f89-e70f6ee00187",
   "metadata": {},
   "source": [
    "### Fine-Tuning Embedding Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "397cc9e7-c214-4327-bc09-827d8b0413c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating our own ViT classifier (multi-)head for fine-tuning\n",
    "class ViTClassifier(nn.Module):\n",
    "    def __init__(self, num_classes1, num_classes2=0, freeze=0):\n",
    "        super(ViTClassifier, self).__init__()\n",
    "        self.vit = ViTModel.from_pretrained('google/vit-base-patch16-224-in21k')\n",
    "        self.classifier1 = nn.Linear(self.vit.config.hidden_size, num_classes1)\n",
    "        self.multi_head = False\n",
    "        \n",
    "        # Multi-head architecture\n",
    "        if num_classes2 > 0:\n",
    "            self.classifier2 = nn.Linear(self.vit.config.hidden_size, num_classes2)\n",
    "            self.multi_head = True\n",
    "        \n",
    "        self.freeze_layers(freeze)\n",
    "\n",
    "    def forward(self, x):\n",
    "        outputs = self.vit(x)\n",
    "        \n",
    "        # Getting embeddings from last_hidden_state of CLS token (maybe pooler_output?)\n",
    "        embeddings = outputs['last_hidden_state'][:, 0, :]\n",
    "        # embeddings = outputs['pooler_output']\n",
    "\n",
    "        logits1 = self.classifier1(embeddings)\n",
    "        if self.multi_head:\n",
    "            logits2 = self.classifier2(embeddings)\n",
    "            return logits1, logits2\n",
    "        return logits1\n",
    "\n",
    "    # Freezing early layers so we don't lose generalization and speed up training\n",
    "    def freeze_layers(self, freeze):\n",
    "        if freeze <= 0:\n",
    "            return\n",
    "\n",
    "        # Accounting for the embedding freeze\n",
    "        freeze -= 1\n",
    "        \n",
    "        for name, param in self.vit.named_parameters():\n",
    "            if \"embeddings\" in name:\n",
    "                param.requires_grad = False\n",
    "            elif int(name.split('.')[2]) < freeze:\n",
    "                param.requires_grad = False\n",
    "            else:\n",
    "                break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0093a1e-f85f-48b9-b75e-391fb8cd390b",
   "metadata": {},
   "source": [
    "#### *povo* Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f1eed02-7da5-4671-8136-390d5530246d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Studying data distribution to filter out rare classes\n",
    "povo_categories, categories_keys, categories_freq, \\\n",
    "qs, masks = study_class_distribution(povo_labels)\n",
    "\n",
    "# Filtering classes so that we retain around 85% of data\n",
    "povo_filtered_categories = {}\n",
    "povo_filtered_categories_names = {}\n",
    "for c in masks[3][0]:\n",
    "    povo_filtered_categories[categories_keys[c]] = povo_categories[categories_keys[c]]\n",
    "    \n",
    "    povo_filtered_categories_names[povo_num_to_name[categories_keys[c]]] = \\\n",
    "    povo_categories[categories_keys[c]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd6a5e63-5680-4a09-a96e-c6fbb91b28b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering dataframe for selected categories\n",
    "threshold_multiplier = 2\n",
    "minority_classes, majority_classes, labels_minority, labels_majority, \\\n",
    "val_labels, test_labels, povo_augmented_dataset, povo_balanced_val_dataset, \\\n",
    "povo_balanced_test_dataset = filter_image_data_distribution(ind_df, \\\n",
    "                                                            povo_filtered_categories_names, \\\n",
    "                                                            vit_transform, \\\n",
    "                                                            threshold_multiplier, \\\n",
    "                                                            'povo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa94d49e-e893-4a83-8917-b34f6a83031e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plotting old and new class distributions\n",
    "plot_class_distributions(povo_categories, povo_filtered_categories, labels_minority, \\\n",
    "                         labels_majority, threshold_multiplier, 'povo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be368210-2317-4991-aac4-12dc1fd6e9dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Because the dataset is still unbalanced, we also create class weights for the loss function\n",
    "povo_class_weights = compute_class_weights(povo_filtered_categories, labels_minority, \\\n",
    "                                           labels_majority, device, threshold_multiplier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08fe3bdc-df2c-40ed-b0c7-a2ac7989e256",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Recreating datasets for proper training and testing\n",
    "povo_train_val_labels = povo_labels.copy()\n",
    "povo_test_labels_aux = random.sample(list(povo_train_val_labels), \\\n",
    "                                     int(0.1*len(povo_train_val_labels)))\n",
    "povo_test_labels = {}\n",
    "for key in povo_test_labels_aux:\n",
    "    povo_test_labels[key] = povo_train_val_labels[key]\n",
    "    del povo_train_val_labels[key]\n",
    "    \n",
    "povo_train_val_dataset = ImageDataset(povo_train_val_labels, transform=vit_transform, \\\n",
    "                                      augment=False)\n",
    "povo_test_dataset = ImageDataset(povo_test_labels, transform=vit_transform, augment=False)\n",
    "\n",
    "# Setting-up training, executing training and then running tests\n",
    "batch_size = 32\n",
    "epochs = 20\n",
    "num_classes = ind_df['povo'].dropna().nunique()\n",
    "model = ViTClassifier(num_classes).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "opt = optim.Adam(model.parameters(), lr=5e-5, weight_decay=2e-6)\n",
    "model_name = 'vit_povo'\n",
    "column_name = 'povo'\n",
    "\n",
    "test_prec, test_rec = execute_train_test(povo_train_val_dataset, povo_test_dataset, device, \\\n",
    "                                         batch_size, epochs, num_classes, model, criterion, \\\n",
    "                                         opt, model_name, column_name)\n",
    "prec_rec_on_selected_classes(povo_categories, povo_filtered_categories, test_prec, test_rec)\n",
    "\n",
    "# Cleaning up memory\n",
    "clean_mem([model])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0871d528-56a5-4773-8e47-92426a19c1cb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Retraining model with augmented dataset to see the difference in the results\n",
    "batch_size = 32\n",
    "epochs = 20\n",
    "num_classes = len(povo_filtered_categories)\n",
    "model = ViTClassifier(num_classes).to(device)\n",
    "criterion = nn.CrossEntropyLoss(weight=povo_class_weights)\n",
    "opt = optim.Adam(model.parameters(), lr=2e-5, weight_decay=2e-6)\n",
    "model_name = 'balanced_vit_povo'\n",
    "column_name = 'povo'\n",
    "\n",
    "execute_train_test(povo_augmented_dataset, povo_balanced_test_dataset, device, batch_size, \\\n",
    "                   epochs, num_classes, model, criterion, opt, model_name, column_name, \\\n",
    "                   povo_balanced_val_dataset)\n",
    "\n",
    "povo_vit_trimap, povo_vit_tsne, \\\n",
    "povo_vit_umap, povo_image_indices = compute_classifier_embeddings(povo_dataloader, model, \\\n",
    "                                                                  device)\n",
    "\n",
    "# Cleaning up memory\n",
    "clean_mem([model])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de4ae93b-0c01-4264-8334-042474bd2b70",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Training partially frozen model on balanced dataset to see the difference in results\n",
    "batch_size = 32\n",
    "epochs = 20\n",
    "num_classes = len(povo_filtered_categories)\n",
    "freeze = 7\n",
    "model = ViTClassifier(num_classes, freeze=freeze).to(device)\n",
    "criterion = nn.CrossEntropyLoss(weight=povo_class_weights)\n",
    "opt = optim.Adam(model.parameters(), lr=2e-5, weight_decay=2e-6)\n",
    "model_name = 'frozen_vit_povo'\n",
    "column_name = 'povo'\n",
    "\n",
    "execute_train_test(povo_augmented_dataset, povo_balanced_test_dataset, device, batch_size, \\\n",
    "                   epochs, num_classes, model, criterion, opt, model_name, column_name, \\\n",
    "                   povo_balanced_val_dataset)\n",
    "\n",
    "# Cleaning up memory\n",
    "clean_mem([model])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef0a9205-5eaf-4bbe-ba8c-79f0d81653d8",
   "metadata": {},
   "source": [
    "#### *categoria* Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04fade66-3cd8-4a50-a8f1-da9b84b1f780",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now rebalancing the 'categoria' column\n",
    "categoria_labels, categoria_name_to_num, categoria_num_to_name = \\\n",
    "preparing_image_labels(ind_df, 'categoria')\n",
    "categoria_dataset = ImageDataset(categoria_labels, transform=vit_transform)\n",
    "categoria_dataloader = DataLoader(categoria_dataset, batch_size=512, shuffle=True, \\\n",
    "                                  num_workers=0, pin_memory=True)\n",
    "\n",
    "categoria_categories, categories_keys, categories_freq, qs, \\\n",
    "masks = study_class_distribution(categoria_labels)\n",
    "\n",
    "# Filtering out 'etnobotânica' (and 'armas')\n",
    "filter_out = [categoria_name_to_num['etnobotânica']]\n",
    "categoria_filtered_categories = {}\n",
    "categoria_filtered_categories_names = {}\n",
    "for c in set(categoria_name_to_num.values()) - set(filter_out):\n",
    "    categoria_filtered_categories[categories_keys[c]] = \\\n",
    "    categoria_categories[categories_keys[c]]\n",
    "    \n",
    "    categoria_filtered_categories_names[categoria_num_to_name[categories_keys[c]]] = \\\n",
    "    categoria_categories[categories_keys[c]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9493d23c-372a-4c29-a1b1-13bb41f7f1f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering dataframe for selected categories\n",
    "threshold_multiplier = 1.5\n",
    "minority_classes, majority_classes, labels_minority, labels_majority, val_labels, \\\n",
    "test_labels, categoria_augmented_dataset, categoria_balanced_val_dataset, \\\n",
    "categoria_balanced_test_dataset = \\\n",
    "filter_image_data_distribution(ind_df, categoria_filtered_categories_names, vit_transform, \\\n",
    "                               threshold_multiplier, 'categoria')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "051ce382-430c-4026-8484-145c1da501d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting old and new class distributions\n",
    "plot_class_distributions(categoria_categories, categoria_filtered_categories, \\\n",
    "                         labels_minority, labels_majority, threshold_multiplier, 'categoria')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f03e8aa-db16-4cfc-a7f3-3b0dc41f4186",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Because the dataset is still unbalanced, we also create class weights for the loss function\n",
    "categoria_class_weights = compute_class_weights(categoria_filtered_categories, \\\n",
    "                                                labels_minority, labels_majority, device, \\\n",
    "                                                threshold_multiplier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b47f319d-f46f-4197-bc9b-7006224bac68",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Recreating datasets for proper training and testing\n",
    "categoria_train_val_labels = categoria_labels.copy()\n",
    "categoria_test_labels_aux = random.sample(list(categoria_train_val_labels), \\\n",
    "                                          int(0.1*len(categoria_train_val_labels)))\n",
    "categoria_test_labels = {}\n",
    "for key in categoria_test_labels_aux:\n",
    "    categoria_test_labels[key] = categoria_train_val_labels[key]\n",
    "    del categoria_train_val_labels[key]\n",
    "    \n",
    "categoria_train_val_dataset = ImageDataset(categoria_train_val_labels, \\\n",
    "                                           transform=vit_transform, augment=False)\n",
    "categoria_test_dataset = ImageDataset(categoria_test_labels, \\\n",
    "                                      transform=vit_transform, augment=False)\n",
    "\n",
    "# Setting-up training, executing training and then running tests\n",
    "batch_size = 32\n",
    "epochs = 20\n",
    "num_classes = ind_df['categoria'].dropna().nunique()\n",
    "model = ViTClassifier(num_classes).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "opt = optim.Adam(model.parameters(), lr=1e-5, weight_decay=2e-6)\n",
    "model_name = 'vit_categoria'\n",
    "column_name = 'categoria'\n",
    "\n",
    "test_prec, test_rec = execute_train_test(categoria_train_val_dataset, categoria_test_dataset, \\\n",
    "                                         device, batch_size, epochs, num_classes, model, \\\n",
    "                                         criterion, opt, model_name, column_name)\n",
    "prec_rec_on_selected_classes(categoria_categories, categoria_filtered_categories, test_prec, \\\n",
    "                             test_rec)\n",
    "\n",
    "# Cleaning up memory\n",
    "clean_mem([model])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ddf8fac-6f67-4ea7-ab8c-8d2be9add2e1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Retraining model with augmented dataset to see the difference in the results\n",
    "batch_size = 16\n",
    "epochs = 20\n",
    "num_classes = len(categoria_filtered_categories)\n",
    "model = ViTClassifier(num_classes).to(device)\n",
    "criterion = nn.CrossEntropyLoss(weight=categoria_class_weights)\n",
    "opt = optim.Adam(model.parameters(), lr=3e-6, weight_decay=1e-6)\n",
    "model_name = 'balanced_vit_categoria'\n",
    "column_name = 'categoria'\n",
    "\n",
    "execute_train_test(categoria_augmented_dataset, categoria_balanced_test_dataset, device, \\\n",
    "                   batch_size, epochs, num_classes, model, criterion, opt, model_name, \\\n",
    "                   column_name, categoria_balanced_val_dataset)\n",
    "\n",
    "categoria_vit_trimap, categoria_vit_tsne, categoria_vit_umap, \\\n",
    "categoria_image_indices = compute_classifier_embeddings(categoria_dataloader, model, device)\n",
    "\n",
    "# Cleaning up memory\n",
    "clean_mem([model])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f58fdaa-04d0-44fc-93a0-5ebbf9b7a4d0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Training partially frozen model on balanced dataset to see the difference in results\n",
    "batch_size = 16\n",
    "epochs = 20\n",
    "num_classes = len(categoria_filtered_categories)\n",
    "freeze = 7\n",
    "model = ViTClassifier(num_classes, freeze=freeze).to(device)\n",
    "criterion = nn.CrossEntropyLoss(weight=categoria_class_weights)\n",
    "opt = optim.Adam(model.parameters(), lr=3e-6, weight_decay=1e-6)\n",
    "model_name = 'frozen_vit_categoria'\n",
    "column_name = 'categoria'\n",
    "\n",
    "execute_train_test(categoria_augmented_dataset, categoria_balanced_test_dataset, device, \\\n",
    "                   batch_size, epochs, num_classes, model, criterion, opt, model_name, \\\n",
    "                   column_name, categoria_balanced_val_dataset)\n",
    "\n",
    "# Cleaning up memory\n",
    "clean_mem([model])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c773629-dfd3-43ec-8e5a-c9e8e4117fa7",
   "metadata": {},
   "source": [
    "#### Multi-Head Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a574bade-683d-42b5-805a-4e1a41a89c61",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Getting data that passes both heads' data-filters\n",
    "povo_set = set(np.array(list(povo_labels.keys()))\\\n",
    "                [np.where(np.isin(np.array(list(povo_labels.values())), \\\n",
    "                np.array(list(povo_filtered_categories.keys()))))[0]])\n",
    "categoria_set = set(np.array(list(categoria_labels.keys()))\\\n",
    "                    [np.where(np.isin(np.array(list(categoria_labels.values())), \\\n",
    "                    np.array(list(categoria_filtered_categories.keys()))))[0]])\n",
    "multi_head_keys = [str(x) for x in povo_set.intersection(categoria_set)]\n",
    "\n",
    "# Studying individual and joint classes' distributions\n",
    "povo_freq = {}\n",
    "categoria_freq = {}\n",
    "multi_freq = {}\n",
    "for key in multi_head_keys:\n",
    "    try:\n",
    "        povo_freq[povo_labels[key]] += 1\n",
    "    except:\n",
    "        povo_freq[povo_labels[key]] = 1\n",
    "\n",
    "    try:\n",
    "        categoria_freq[categoria_labels[key]] += 1\n",
    "    except:\n",
    "        categoria_freq[categoria_labels[key]] = 1\n",
    "\n",
    "    try:\n",
    "        multi_freq[f'{(povo_labels[key], categoria_labels[key])}'] += 1\n",
    "    except:\n",
    "        multi_freq[f'{(povo_labels[key], categoria_labels[key])}'] = 1\n",
    "\n",
    "# Utility function to plot rows on data distribution plot\n",
    "def row_bar_plot(col_freq, name, row, rows=3, cols=1, remove_xticks=False):\n",
    "    plt.subplot(rows, cols, row)\n",
    "    plt.bar(list(col_freq.keys()), list(col_freq.values()))\n",
    "    plt.title(name)\n",
    "\n",
    "    if remove_xticks:\n",
    "        plt.xlabel(\"\")\n",
    "        plt.xticks([])\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.suptitle('Classes Distributions for Multi-Head Data')\n",
    "\n",
    "row_bar_plot(povo_freq, \"'povo' Column\", 1)\n",
    "row_bar_plot(categoria_freq, \"'categoria' Column\", 2)\n",
    "row_bar_plot(multi_freq, \"Both Columns (Joint Distribution)\", 3, remove_xticks=True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa01612-aadc-4200-ae15-dcfb01d448bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Because of the even worse degree of unbalanced data on the joint distribution, we balance\n",
    "# the dataset taking the 'povo' column into consideration and study the new joint distribution\n",
    "multi_povo_labels = {}\n",
    "multi_categoria_labels = {}\n",
    "for key in multi_head_keys:\n",
    "    multi_povo_labels[key] = povo_labels[key]\n",
    "    multi_categoria_labels[key] = categoria_labels[key]\n",
    "\n",
    "# Reusing previous dataset infrastructure to build multi-label dataset\n",
    "multi_povo_categories, categories_keys, categories_freq, \\\n",
    "qs, masks = study_class_distribution(multi_povo_labels)\n",
    "multi_povo_filtered_categories = multi_povo_categories\n",
    "multi_povo_filtered_categories_names = {}\n",
    "for c in categories_keys:\n",
    "    multi_povo_filtered_categories_names[povo_num_to_name[c]] = multi_povo_categories[c]\n",
    "\n",
    "multi_categoria_categories, categories_keys, categories_freq, \\\n",
    "qs, masks = study_class_distribution(multi_categoria_labels)\n",
    "multi_categoria_filtered_categories = multi_categoria_categories\n",
    "multi_categoria_filtered_categories_names = {}\n",
    "for c in categories_keys:\n",
    "    multi_categoria_filtered_categories_names[categoria_num_to_name[c]] = \\\n",
    "    multi_categoria_categories[c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ffb1c1a-b60b-4a08-82f0-2484d837e78d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Filtering dataframe for selected categories\n",
    "threshold_multiplier = 2\n",
    "minority_classes, majority_classes, labels_minority, labels_majority, \\\n",
    "val_labels, test_labels, multi_augmented_dataset, multi_balanced_val_dataset, \\\n",
    "multi_balanced_test_dataset = \\\n",
    "multihead_filter_image_data_distribution(ind_df, [multi_povo_filtered_categories_names, \\\n",
    "                                         multi_categoria_filtered_categories_names], \\\n",
    "                                         vit_transform, threshold_multiplier, \\\n",
    "                                         ['povo', 'categoria'])\n",
    "\n",
    "# Plotting old and new class distributions\n",
    "multihead_plot_class_distributions(multi_povo_categories, multi_povo_filtered_categories, \\\n",
    "                                   labels_minority, labels_majority, threshold_multiplier, \\\n",
    "                                   'povo')\n",
    "\n",
    "multihead_plot_class_distributions(multi_categoria_categories, \\\n",
    "                                   multi_categoria_filtered_categories, \\\n",
    "                                   labels_minority, labels_majority, threshold_multiplier, \\\n",
    "                                   'categoria')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfb11d70-889b-43f1-819b-12118d21e6f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Because the dataset is still unbalanced, we also create class weights for the loss function\n",
    "multi_povo_class_weights = multihead_compute_class_weights(multi_povo_filtered_categories, \\\n",
    "                                                           labels_minority, labels_majority, \\\n",
    "                                                           device, threshold_multiplier, \\\n",
    "                                                           'povo')\n",
    "\n",
    "multi_categoria_class_weights = \\\n",
    "multihead_compute_class_weights(multi_categoria_filtered_categories, labels_minority, \\\n",
    "                                labels_majority, device, threshold_multiplier,'categoria')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28f343bf-bbd9-496e-9f95-b1a14b914287",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Training multi-head model on balanced dataset to see the difference in results\n",
    "batch_size = 16\n",
    "epochs = 30\n",
    "num_classes = [len(multi_povo_filtered_categories), len(multi_categoria_filtered_categories)]\n",
    "model = ViTClassifier(num_classes[0], num_classes[1]).to(device)\n",
    "criterions = [nn.CrossEntropyLoss(weight=multi_povo_class_weights), \\\n",
    "              nn.CrossEntropyLoss(weight=multi_categoria_class_weights)]\n",
    "opt = optim.Adam(model.parameters(), lr=1e-5, weight_decay=3e-6)\n",
    "model_name = 'multihead_vit'\n",
    "column_names = ['povo', 'categoria']\n",
    "arch_name = 'ViT'\n",
    "head_weights = [0.5, 0.5]\n",
    "\n",
    "multihead_execute_train_test(multi_augmented_dataset, multi_balanced_test_dataset, device, \\\n",
    "                             batch_size, epochs, num_classes, model, \\\n",
    "                             criterions, opt, model_name, column_names, \\\n",
    "                             multi_balanced_val_dataset, arch_name, head_weights)\n",
    "\n",
    "# Cleaning up memory\n",
    "clean_mem([model])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81730faf-1ab3-4473-9268-04b25559bc14",
   "metadata": {},
   "source": [
    "### Visualizing and Comparing Projections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7622319c-08ac-475d-8376-c66bd3e76fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizing data for later plot on tool\n",
    "norm_factor = 12\n",
    "vanilla_vit_trimap = normalize(vanilla_vit_trimap, norm_factor)\n",
    "vanilla_vit_tsne = normalize(vanilla_vit_tsne, norm_factor)\n",
    "vanilla_vit_umap = normalize(vanilla_vit_umap, norm_factor)\n",
    "\n",
    "povo_vit_trimap = normalize(povo_vit_trimap, norm_factor)\n",
    "povo_vit_tsne = normalize(povo_vit_tsne, norm_factor)\n",
    "povo_vit_umap = normalize(povo_vit_umap, norm_factor)\n",
    "\n",
    "categoria_vit_trimap = normalize(categoria_vit_trimap, norm_factor)\n",
    "categoria_vit_tsne = normalize(categoria_vit_tsne, norm_factor)\n",
    "categoria_vit_umap = normalize(categoria_vit_umap, norm_factor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63fce310-fb5f-4218-afb7-5ffcb02dbf1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility function to plot rows on projection comparison plot\n",
    "def row_scatter_plot(projs, proj_names, row, color, rows=3, cols=3):\n",
    "    for i, (proj, proj_name) in enumerate(zip(projs, proj_names)):\n",
    "        plt.subplot(rows, cols, i+1+(cols*(row-1)))\n",
    "        plt.scatter(proj[:, 0], proj[:, 1], c=color)\n",
    "        plt.title(f\"{proj_name}\")\n",
    "        plt.xlabel(\"\")\n",
    "        plt.ylabel(\"\")\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "\n",
    "# Visualizing resulting projections\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.suptitle('Comparing Projections of ViT Models')\n",
    "\n",
    "# Plotting vanilla ViT projections\n",
    "projs = [vanilla_vit_trimap, vanilla_vit_tsne, vanilla_vit_umap]\n",
    "proj_names = ['Vanilla ViT with TriMap', 'Vanilla ViT with t-SNE', 'Vanilla ViT with UMAP']\n",
    "row_scatter_plot(projs, proj_names, 1, 'b')\n",
    "\n",
    "# Plotting ViT fine-tuned on 'povo' projections\n",
    "projs = [povo_vit_trimap, povo_vit_tsne, povo_vit_umap]\n",
    "proj_names = [\"ViT Fine-Tuned on 'povo' with TriMap\", \"ViT Fine-Tuned on 'povo' with t-SNE\", \\\n",
    "              \"ViT Fine-Tuned on 'povo' with UMAP\"]\n",
    "row_scatter_plot(projs, proj_names, 2, 'r')\n",
    "\n",
    "# Plotting ViT fine-tuned on 'categoria' projections\n",
    "projs = [categoria_vit_trimap, categoria_vit_tsne, categoria_vit_umap]\n",
    "proj_names = [\"ViT Fine-Tuned on 'categoria' with TriMap\", \\\n",
    "              \"ViT Fine-Tuned on 'categoria' with t-SNE\", \\\n",
    "              \"ViT Fine-Tuned on 'categoria' with UMAP\"]\n",
    "row_scatter_plot(projs, proj_names, 3, 'g')\n",
    "\n",
    "# DECIDE IF I WANT TO PLOT FROZEN AND MULTI-HEAD MODELS' PROJECTIONS DEPENDING ON PERFORMANCE\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d358406-41c1-40ba-8580-398ff90a1872",
   "metadata": {},
   "source": [
    "### Visualizing Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b7ccb9a-0d2f-4f97-adc5-686a4310b555",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Filtering dataframe to get only the part that contains images\n",
    "filtered_df = ind_df.loc[ind_df['image_path'].notna()]\n",
    "\n",
    "# Visualizing 'povo' cluster\n",
    "visualizing_clusters(filtered_df, povo_vit_umap, povo_image_indices, 'povo', 'UMAP')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a072ba0-1e61-4288-b4ff-27ad22e1c3dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing 'categoria' cluster\n",
    "visualizing_clusters(filtered_df, categoria_vit_umap, categoria_image_indices, \\\n",
    "                     'categoria', 'UMAP')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09ba8e8d-256f-4681-9fa8-8c1026781225",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving outputs for visualization tool\n",
    "_ = saving_outputs(filtered_df, povo_labels, povo_vit_umap, povo_image_indices, 'povo', \\\n",
    "                   'povo_vit.csv')\n",
    "\n",
    "_ = saving_outputs(filtered_df, categoria_labels, categoria_vit_umap, \\\n",
    "                   categoria_image_indices, 'categoria', 'categoria_vit.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50709824-9e61-4a0e-9963-fd10b5a1f727",
   "metadata": {},
   "source": [
    "## DINOv2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9c9fd41-14e6-48a0-836f-d4ee23073892",
   "metadata": {},
   "source": [
    "### Pre-Trained Embedding Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c387c93-d1e2-458d-85d8-ebc65a74559b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoImageProcessor, AutoModel\n",
    "\n",
    "# Rebuilding dataset with DINO transform and dataloader with smaller batch_size because of \n",
    "# the model's size\n",
    "dino_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Resize(256, interpolation=transforms.InterpolationMode.BICUBIC),\n",
    "    transforms.CenterCrop((224, 224)),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "povo_dataset = ImageDataset(povo_labels, transform=dino_transform, augment=False)\n",
    "povo_dataloader = DataLoader(povo_dataset, batch_size=16, shuffle=True, \\\n",
    "                             num_workers=0, pin_memory=True)\n",
    "\n",
    "# Loading model and pre-processor\n",
    "dino_processor = AutoImageProcessor.from_pretrained('facebook/dinov2-base')\n",
    "model = AutoModel.from_pretrained('facebook/dinov2-base')\n",
    "model.to(device)\n",
    "\n",
    "# Projecting data onto the off-the-shelf pre-trained embedding space from DINOv2\n",
    "image_embeddings, _ = get_embeddings(model, povo_dataloader, device, model_name='dino')\n",
    "image_embeddings = np.concatenate(image_embeddings, axis=0)\n",
    "\n",
    "# Computing data reduced-dimensionality projections\n",
    "_, _, vanilla_dino_umap = data_projections(image_embeddings)\n",
    "\n",
    "# Cleaning up memory\n",
    "clean_mem([model, image_embeddings])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8fba030-df4f-434a-a90e-d08cebc2e38b",
   "metadata": {},
   "source": [
    "### Fine-Tuning Embedding Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96872a49-4b39-4693-872a-627e5191ca94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating our own DINO classifier head for fine-tuning\n",
    "class DINOClassifier(nn.Module):\n",
    "    def __init__(self, num_classes1, num_classes2=0, freeze=0):\n",
    "        super(DINOClassifier, self).__init__()\n",
    "        self.dino = AutoModel.from_pretrained('facebook/dinov2-base')\n",
    "        self.classifier1 = nn.Linear(self.dino.config.hidden_size, num_classes1)\n",
    "        self.multi_head = False\n",
    "        \n",
    "        # Multi-head architecture\n",
    "        if num_classes2 > 0:\n",
    "            self.classifier2 = nn.Linear(self.dino.config.hidden_size, num_classes2)\n",
    "            self.multi_head = True\n",
    "        \n",
    "        self.freeze_layers(freeze)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        outputs = self.dino(x)\n",
    "        \n",
    "        # Getting embeddings from last_hidden_state of CLS token (maybe pooler_output?)\n",
    "        embeddings = outputs['last_hidden_state'][:, 0, :]\n",
    "        # embeddings = outputs['pooler_output']\n",
    "\n",
    "        logits1 = self.classifier1(embeddings)\n",
    "        if self.multi_head:\n",
    "            logits2 = self.classifier2(embeddings)\n",
    "            return logits1, logits2\n",
    "        return logits1\n",
    "\n",
    "    # Freezing early layers so we don't lose generalization and speed up training\n",
    "    def freeze_layers(self, freeze):\n",
    "        if freeze <= 0:\n",
    "            return\n",
    "\n",
    "        # Accounting for the embedding freeze\n",
    "        freeze -= 1\n",
    "        \n",
    "        for name, param in self.vit.named_parameters():\n",
    "            if \"embeddings\" in name:\n",
    "                param.requires_grad = False\n",
    "            elif int(name.split('.')[2]) < freeze:\n",
    "                param.requires_grad = False\n",
    "            else:\n",
    "                break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6609639-35a5-4fae-b1a8-799d4a37a585",
   "metadata": {},
   "source": [
    "#### *povo* Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a39e11a4-c5cb-4524-9dc5-3f2fca00adaa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Recreating datasets for proper training and testing\n",
    "povo_train_val_dataset = ImageDataset(povo_train_val_labels, transform=dino_transform, \\\n",
    "                                      augment=False)\n",
    "povo_test_dataset = ImageDataset(povo_test_labels, transform=dino_transform, augment=False)\n",
    "\n",
    "# Setting-up training, executing training and then running tests\n",
    "batch_size = 16\n",
    "epochs = 20\n",
    "num_classes = ind_df['povo'].dropna().nunique()\n",
    "model = DINOClassifier(num_classes).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "opt = optim.Adam(model.parameters(), lr=2e-5, weight_decay=2e-6)\n",
    "model_name = 'dino_povo'\n",
    "column_name = 'povo'\n",
    "\n",
    "test_prec, test_rec = execute_train_test(povo_train_val_dataset, povo_test_dataset, device, \\\n",
    "                                        batch_size, epochs, num_classes, model, criterion, \\\n",
    "                                        opt, model_name, column_name, \\\n",
    "                                        architecture_name='DINOv2')\n",
    "prec_rec_on_selected_classes(povo_categories, povo_filtered_categories, test_prec, test_rec)\n",
    "\n",
    "# Cleaning up memory\n",
    "clean_mem([model])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66ed537e-c895-4f46-a748-97bb0cf56995",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering dataframe for selected categories. We need to recompute this part of the dataset\n",
    "# because of the different transform used\n",
    "threshold_multiplier = 2\n",
    "minority_classes, majority_classes, labels_minority, labels_majority, \\\n",
    "val_labels, test_labels, povo_augmented_dataset, povo_balanced_val_dataset, \\\n",
    "povo_balanced_test_dataset = filter_image_data_distribution(ind_df, \\\n",
    "                                                            povo_filtered_categories_names, \\\n",
    "                                                            dino_transform, \\\n",
    "                                                            threshold_multiplier, \\\n",
    "                                                            'povo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78db26c8-d6f3-4dbb-8f9f-b6804b352454",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Retraining model with augmented dataset to see the difference in the results\n",
    "batch_size = 16\n",
    "epochs = 20\n",
    "num_classes = len(povo_filtered_categories)\n",
    "model = DINOClassifier(num_classes).to(device)\n",
    "criterion = nn.CrossEntropyLoss(weight=povo_class_weights)\n",
    "opt = optim.Adam(model.parameters(), lr=2e-5, weight_decay=2e-6)\n",
    "model_name = 'balanced_dino_povo'\n",
    "column_name = 'povo'\n",
    "\n",
    "execute_train_test(povo_augmented_dataset, povo_balanced_test_dataset, device, batch_size, \\\n",
    "                   epochs, num_classes, model, criterion, opt, model_name, column_name, \\\n",
    "                   povo_balanced_val_dataset, architecture_name='DINOv2')\n",
    "\n",
    "_, _, \\\n",
    "povo_dino_umap, povo_image_indices = compute_classifier_embeddings(povo_dataloader, model, \\\n",
    "                                                                   device, 'dino')\n",
    "\n",
    "# Cleaning up memory\n",
    "clean_mem([model])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "38d89dd8-9eea-44e4-8f11-72af1825171b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training model:   0%|                                                        | 0/20 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "cross_entropy_loss(): argument 'input' (position 1) must be Tensor, not tuple",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[43], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m model_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfrozen_dino_povo\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     10\u001b[0m column_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpovo\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m---> 12\u001b[0m \u001b[43mexecute_train_test\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpovo_augmented_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpovo_balanced_test_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m                   \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m                   \u001b[49m\u001b[43mcolumn_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpovo_balanced_val_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marchitecture_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mDINOv2\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Cleaning up memory\u001b[39;00m\n\u001b[1;32m     17\u001b[0m clean_mem([model])\n",
      "File \u001b[0;32m~/Documents/uva/thesis/indigenous_project/tainacan_collection/image_training_utils.py:605\u001b[0m, in \u001b[0;36mexecute_train_test\u001b[0;34m(dataset, test_dataset, device, batch_size, epochs, num_classes, model, criterion, opt, model_name, column_name, val_dataset, architecture_name)\u001b[0m\n\u001b[1;32m    602\u001b[0m test_dataloader \u001b[38;5;241m=\u001b[39m DataLoader(test_dataset, batch_size\u001b[38;5;241m=\u001b[39mbatch_size, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, num_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, pin_memory\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    604\u001b[0m \u001b[38;5;66;03m# Training set-up and execution\u001b[39;00m\n\u001b[0;32m--> 605\u001b[0m losses, accuracies, class_precisions, class_recalls, best_ind \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    606\u001b[0m plot_train_curves(losses, accuracies, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00marchitecture_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Fine-Tuned on \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcolumn_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    607\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mValidation accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maccuracies[best_ind]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/uva/thesis/indigenous_project/tainacan_collection/image_training_utils.py:420\u001b[0m, in \u001b[0;36mtrain_loop\u001b[0;34m(model, num_classes, train_dataloader, val_dataloader, device, criterion, opt, model_name, epochs)\u001b[0m\n\u001b[1;32m    418\u001b[0m opt\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m    419\u001b[0m logits \u001b[38;5;241m=\u001b[39m model(batch_images)\n\u001b[0;32m--> 420\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mcriterion\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_labels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    421\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m    422\u001b[0m opt\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/anaconda3/envs/ind_thesis/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/ind_thesis/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/anaconda3/envs/ind_thesis/lib/python3.10/site-packages/torch/nn/modules/loss.py:1293\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1292\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m-> 1293\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1294\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1295\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1296\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1297\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1298\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1299\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1300\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/ind_thesis/lib/python3.10/site-packages/torch/nn/functional.py:3479\u001b[0m, in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3478\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m-> 3479\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy_loss\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3480\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3481\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3482\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3483\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3484\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3485\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3486\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: cross_entropy_loss(): argument 'input' (position 1) must be Tensor, not tuple"
     ]
    }
   ],
   "source": [
    "# Training partially frozen model on balanced dataset to see the difference in results\n",
    "batch_size = 16\n",
    "epochs = 20\n",
    "num_classes = len(povo_filtered_categories)\n",
    "freeze = 7\n",
    "model = DINOClassifier(num_classes, freeze).to(device)\n",
    "criterion = nn.CrossEntropyLoss(weight=povo_class_weights)\n",
    "opt = optim.Adam(model.parameters(), lr=2e-5, weight_decay=2e-6)\n",
    "model_name = 'frozen_dino_povo'\n",
    "column_name = 'povo'\n",
    "\n",
    "execute_train_test(povo_augmented_dataset, povo_balanced_test_dataset, device, \\\n",
    "                   batch_size, epochs, num_classes, model, criterion, opt, model_name, \\\n",
    "                   column_name, povo_balanced_val_dataset, architecture_name='DINOv2')\n",
    "\n",
    "# Cleaning up memory\n",
    "clean_mem([model])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "064d659b-247f-4ec8-996f-b0f50e36c3ed",
   "metadata": {},
   "source": [
    "#### *categoria* Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c80e65-f3e3-447b-84b6-af6d6f114a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now creating the 'categoria' dataset\n",
    "categoria_dataset = ImageDataset(categoria_labels, transform=dino_transform)\n",
    "categoria_dataloader = DataLoader(categoria_dataset, batch_size=8, shuffle=True, \\\n",
    "                                  num_workers=0, pin_memory=True)\n",
    "\n",
    "# Recreating datasets for proper training and testing\n",
    "categoria_train_val_dataset = ImageDataset(categoria_train_val_labels, \\\n",
    "                                           transform=dino_transform, augment=False)\n",
    "categoria_test_dataset = ImageDataset(categoria_test_labels, transform=dino_transform, \\\n",
    "                                      augment=False)\n",
    "\n",
    "# Setting-up training, executing training and then running tests\n",
    "batch_size = 16\n",
    "epochs = 20\n",
    "num_classes = ind_df['categoria'].dropna().nunique()\n",
    "model = DINOClassifier(num_classes).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "opt = optim.Adam(model.parameters(), lr=1e-5, weight_decay=2e-6)\n",
    "model_name = 'dino_categoria'\n",
    "column_name = 'categoria'\n",
    "\n",
    "test_prec, test_rec = execute_train_test(categoria_train_val_dataset, categoria_test_dataset, \\\n",
    "                                         device, batch_size, epochs, num_classes, model, \\\n",
    "                                         criterion, opt, model_name, column_name, \\\n",
    "                                         architecture_name='DINOv2')\n",
    "prec_rec_on_selected_classes(categoria_categories, categoria_filtered_categories, test_prec, \\\n",
    "                             test_rec)\n",
    "\n",
    "# Cleaning up memory\n",
    "clean_mem([model])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff0224d-df2c-432e-87c6-4d8f30906601",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering dataframe for selected categories. We need to recompute this part of the dataset\n",
    "# because of the different transform used\n",
    "threshold_multiplier = 1.5\n",
    "minority_classes, majority_classes, labels_minority, labels_majority, \\\n",
    "val_labels, test_labels, categoria_augmented_dataset, categoria_balanced_val_dataset, \\\n",
    "categoria_balanced_test_dataset = \\\n",
    "filter_image_data_distribution(ind_df, categoria_filtered_categories_names, dino_transform, \\\n",
    "                               threshold_multiplier, 'categoria')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "506c7d49-68a2-4593-86a0-b916298e888e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retraining model with augmented dataset to see the difference in the results\n",
    "batch_size = 16\n",
    "epochs = 20\n",
    "num_classes = len(categoria_filtered_categories)\n",
    "model = DINOClassifier(num_classes).to(device)\n",
    "criterion = nn.CrossEntropyLoss(weight=povo_class_weights)\n",
    "opt = optim.Adam(model.parameters(), lr=2e-5, weight_decay=2e-6)\n",
    "model_name = 'balanced_dino_categoria'\n",
    "column_name = 'categoria'\n",
    "\n",
    "execute_train_test(categoria_augmented_dataset, categoria_balanced_test_dataset, device, \\\n",
    "                   batch_size, epochs, num_classes, model, criterion, opt, model_name, \\\n",
    "                   column_name, categoria_balanced_val_dataset, architecture_name='DINOv2')\n",
    "\n",
    "_, _, \\\n",
    "categoria_dino_umap, categoria_image_indices = \\\n",
    "compute_classifier_embeddings(categoria_dataloader, model, device, 'dino')\n",
    "\n",
    "# Cleaning up memory\n",
    "clean_mem([model])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "498b0ccb-5116-4131-ae55-446a32140d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training partially frozen model on balanced dataset to see the difference in results\n",
    "batch_size = 16\n",
    "epochs = 20\n",
    "num_classes = len(categoria_filtered_categories)\n",
    "freeze = 7\n",
    "model = DINOClassifier(num_classes, freeze).to(device)\n",
    "criterion = nn.CrossEntropyLoss(weight=categoria_class_weights)\n",
    "opt = optim.Adam(model.parameters(), lr=2e-5, weight_decay=2e-6)\n",
    "model_name = 'frozen_dino_categoria'\n",
    "column_name = 'categoria'\n",
    "\n",
    "execute_train_test(categoria_augmented_dataset, categoria_balanced_test_dataset, device, \\\n",
    "                   batch_size, epochs, num_classes, model, criterion, opt, model_name, \\\n",
    "                   column_name, categoria_balanced_val_dataset, architecture_name='DINOv2')\n",
    "\n",
    "# Cleaning up memory\n",
    "clean_mem([model])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0973b009-6372-4d31-ab43-b0be7ad21ff5",
   "metadata": {},
   "source": [
    "#### Multi-Head Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2ba1ceba-58c4-412d-bf76-fa70f4f5cbf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering dataframe for selected categories\n",
    "threshold_multiplier = 2\n",
    "minority_classes, majority_classes, labels_minority, labels_majority, \\\n",
    "val_labels, test_labels, multi_augmented_dataset, multi_balanced_val_dataset, \\\n",
    "multi_balanced_test_dataset = \\\n",
    "multihead_filter_image_data_distribution(ind_df, [multi_povo_filtered_categories_names, \\\n",
    "                                         multi_categoria_filtered_categories_names], \\\n",
    "                                         dino_transform, threshold_multiplier, \\\n",
    "                                         ['povo', 'categoria'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "01a0004c-9c36-4920-866a-bb035d9985e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Because the dataset is still unbalanced, we also create class weights for the loss function\n",
    "multi_povo_class_weights = multihead_compute_class_weights(multi_povo_filtered_categories, \\\n",
    "                                                           labels_minority, labels_majority, \\\n",
    "                                                           device, threshold_multiplier, \\\n",
    "                                                           'povo')\n",
    "\n",
    "multi_categoria_class_weights = \\\n",
    "multihead_compute_class_weights(multi_categoria_filtered_categories, labels_minority, \\\n",
    "                                labels_majority, device, threshold_multiplier,'categoria')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "461388ef-352a-4ee7-acec-88a2205152cf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training model:   0%|                                                        | 0/20 [05:41<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[46], line 18\u001b[0m\n\u001b[1;32m     15\u001b[0m arch_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDINOv2\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     16\u001b[0m head_weights \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m0.5\u001b[39m, \u001b[38;5;241m0.5\u001b[39m]\n\u001b[0;32m---> 18\u001b[0m \u001b[43mmultihead_execute_train_test\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmulti_augmented_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmulti_balanced_test_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mcriterions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumn_names\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mmulti_balanced_val_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43march_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhead_weights\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# Cleaning up memory\u001b[39;00m\n\u001b[1;32m     24\u001b[0m clean_mem([model])\n",
      "File \u001b[0;32m~/Documents/uva/thesis/indigenous_project/tainacan_collection/image_training_utils.py:631\u001b[0m, in \u001b[0;36mmultihead_execute_train_test\u001b[0;34m(dataset, test_dataset, device, batch_size, epochs, num_classes, model, criterions, opt, model_name, column_names, val_dataset, architecture_name, head_weights)\u001b[0m\n\u001b[1;32m    628\u001b[0m test_dataloader \u001b[38;5;241m=\u001b[39m DataLoader(test_dataset, batch_size\u001b[38;5;241m=\u001b[39mbatch_size, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, num_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, pin_memory\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    630\u001b[0m \u001b[38;5;66;03m# Training set-up and execution\u001b[39;00m\n\u001b[0;32m--> 631\u001b[0m comb_losses, ind_losses, avg_accuracies, ind_accuracies, class_precisions, class_recalls, best_ind \u001b[38;5;241m=\u001b[39m \u001b[43mmultihead_train_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhead_weights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    632\u001b[0m plot_train_curves(comb_losses, avg_accuracies, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMulti-head \u001b[39m\u001b[38;5;132;01m{\u001b[39;00marchitecture_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Fine-Tuned on \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcolumn_names\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    634\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mValidation \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcolumn_names[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m head accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mind_accuracies[\u001b[38;5;241m0\u001b[39m][best_ind]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/uva/thesis/indigenous_project/tainacan_collection/image_training_utils.py:504\u001b[0m, in \u001b[0;36mmultihead_train_loop\u001b[0;34m(model, num_classes, train_dataloader, val_dataloader, device, criterions, opt, model_name, head_weights, epochs)\u001b[0m\n\u001b[1;32m    502\u001b[0m epoch_ind_loss1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m.0\u001b[39m\n\u001b[1;32m    503\u001b[0m epoch_ind_loss2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m.0\u001b[39m\n\u001b[0;32m--> 504\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_images, [batch_labels1, batch_labels2], _ \u001b[38;5;129;01min\u001b[39;00m train_dataloader:\n\u001b[1;32m    505\u001b[0m     batch_images, batch_labels1, batch_labels2 \u001b[38;5;241m=\u001b[39m batch_images\u001b[38;5;241m.\u001b[39mto(device), batch_labels1\u001b[38;5;241m.\u001b[39mto(device), batch_labels2\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m    507\u001b[0m     opt\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m~/anaconda3/envs/ind_thesis/lib/python3.10/site-packages/torch/utils/data/dataloader.py:701\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    699\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 701\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[1;32m    707\u001b[0m ):\n",
      "File \u001b[0;32m~/anaconda3/envs/ind_thesis/lib/python3.10/site-packages/torch/utils/data/dataloader.py:757\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    755\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    756\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 757\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    758\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    759\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/anaconda3/envs/ind_thesis/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/anaconda3/envs/ind_thesis/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/anaconda3/envs/ind_thesis/lib/python3.10/site-packages/torch/utils/data/dataset.py:350\u001b[0m, in \u001b[0;36mConcatDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    348\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    349\u001b[0m     sample_idx \u001b[38;5;241m=\u001b[39m idx \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcumulative_sizes[dataset_idx \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m--> 350\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdatasets\u001b[49m\u001b[43m[\u001b[49m\u001b[43mdataset_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43msample_idx\u001b[49m\u001b[43m]\u001b[49m\n",
      "File \u001b[0;32m~/Documents/uva/thesis/indigenous_project/tainacan_collection/image_training_utils.py:69\u001b[0m, in \u001b[0;36mImageDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, idx):\n\u001b[1;32m     68\u001b[0m     image_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimage_files[idx]\n\u001b[0;32m---> 69\u001b[0m     image \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrgb_with_br\u001b[49m\u001b[43m(\u001b[49m\u001b[43mImage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_path\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     71\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maugment:\n\u001b[1;32m     72\u001b[0m         augment_transform \u001b[38;5;241m=\u001b[39m transforms\u001b[38;5;241m.\u001b[39mCompose([\n\u001b[1;32m     73\u001b[0m             transforms\u001b[38;5;241m.\u001b[39mRandomHorizontalFlip(p\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.6\u001b[39m),\n\u001b[1;32m     74\u001b[0m             transforms\u001b[38;5;241m.\u001b[39mRandomVerticalFlip(p\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.6\u001b[39m),\n\u001b[1;32m     75\u001b[0m             transforms\u001b[38;5;241m.\u001b[39mRandomApply([transforms\u001b[38;5;241m.\u001b[39mGaussianBlur(kernel_size\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m5\u001b[39m), sigma\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m0.1\u001b[39m, \u001b[38;5;241m2.0\u001b[39m))], p\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.6\u001b[39m)\n\u001b[1;32m     76\u001b[0m         ])\n",
      "File \u001b[0;32m~/Documents/uva/thesis/indigenous_project/tainacan_collection/image_training_utils.py:87\u001b[0m, in \u001b[0;36mImageDataset.rgb_with_br\u001b[0;34m(self, image)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrgb_with_br\u001b[39m(\u001b[38;5;28mself\u001b[39m, image):\n\u001b[1;32m     86\u001b[0m     \u001b[38;5;66;03m# Creating white background to convert original image to RGB without reconstructing background\u001b[39;00m\n\u001b[0;32m---> 87\u001b[0m     background \u001b[38;5;241m=\u001b[39m \u001b[43mImage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnew\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mRGB\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m255\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m255\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m255\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     88\u001b[0m     image \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39malpha_composite(background\u001b[38;5;241m.\u001b[39mconvert(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRGBA\u001b[39m\u001b[38;5;124m\"\u001b[39m), image)\u001b[38;5;241m.\u001b[39mconvert(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRGB\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     89\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m image\n",
      "File \u001b[0;32m~/anaconda3/envs/ind_thesis/lib/python3.10/site-packages/PIL/Image.py:3136\u001b[0m, in \u001b[0;36mnew\u001b[0;34m(mode, size, color)\u001b[0m\n\u001b[1;32m   3134\u001b[0m         im\u001b[38;5;241m.\u001b[39mpalette \u001b[38;5;241m=\u001b[39m ImagePalette\u001b[38;5;241m.\u001b[39mImagePalette()\n\u001b[1;32m   3135\u001b[0m         color \u001b[38;5;241m=\u001b[39m im\u001b[38;5;241m.\u001b[39mpalette\u001b[38;5;241m.\u001b[39mgetcolor(color_ints)\n\u001b[0;32m-> 3136\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m im\u001b[38;5;241m.\u001b[39m_new(\u001b[43mcore\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfill\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolor\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import image_training_utils\n",
    "importlib.reload(image_training_utils)\n",
    "from image_training_utils import *\n",
    "\n",
    "# Training multi-head model on balanced dataset to see the difference in results\n",
    "batch_size = 8\n",
    "epochs = 20\n",
    "num_classes = [len(multi_povo_filtered_categories), len(multi_categoria_filtered_categories)]\n",
    "model = DINOClassifier(num_classes[0], num_classes[1]).to(device)\n",
    "criterions = [nn.CrossEntropyLoss(weight=multi_povo_class_weights), \\\n",
    "              nn.CrossEntropyLoss(weight=multi_categoria_class_weights)]\n",
    "opt = optim.Adam(model.parameters(), lr=3e-6, weight_decay=1e-6)\n",
    "model_name = 'multihead_dino'\n",
    "column_names = ['povo', 'categoria']\n",
    "arch_name = 'DINOv2'\n",
    "head_weights = [0.5, 0.5]\n",
    "\n",
    "multihead_execute_train_test(multi_augmented_dataset, multi_balanced_test_dataset, device, \\\n",
    "                             batch_size, epochs, num_classes, model, \\\n",
    "                             criterions, opt, model_name, column_names, \\\n",
    "                             multi_balanced_val_dataset, arch_name, head_weights)\n",
    "\n",
    "# Cleaning up memory\n",
    "clean_mem([model])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff69dbc0-c068-49e5-9ead-cba4df5f6621",
   "metadata": {},
   "source": [
    "### Visualizing and Comparing Projections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57420473-f668-471a-86fa-e3306d01f59c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizing data for later plot on tool\n",
    "vanilla_dino_umap = normalize(vanilla_dino_umap, norm_factor)\n",
    "povo_dino_umap = normalize(povo_dino_umap, norm_factor)\n",
    "categoria_dino_umap = normalize(categoria_dino_umap, norm_factor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ef94fc-64d8-4863-9695-7588e6ca47d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing resulting projections\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.suptitle('Comparing ViT and DINO Projections')\n",
    "\n",
    "# Plotting all ViT projections\n",
    "projs = [vanilla_vit_umap, povo_vit_umap, categoria_vit_umap]\n",
    "proj_names = [\"Vanilla ViT\", \"'povo' ViT\", \"'categoria' ViT\"]\n",
    "row_scatter_plot(projs, proj_names, 1, 'b', 2, 3)\n",
    "\n",
    "# Plotting all DINO projections\n",
    "projs = [vanilla_dino_umap, povo_dino_umap, categoria_dino_umap]\n",
    "proj_names = [\"Vanilla DINO\", \"'povo' DINO\", \"'categoria' DINO\"]\n",
    "row_scatter_plot(projs, proj_names, 2, 'r', 2, 3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
