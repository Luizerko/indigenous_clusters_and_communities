{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "12e8d3d3-5137-4c49-a9c7-4ef40f64c737",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe columns: \n",
      "Index(['url', 'thumbnail', 'creation_date', 'modification_date',\n",
      "       'numero_do_item', 'tripticos', 'categoria', 'nome_do_item',\n",
      "       'nome_do_item_dic', 'colecao', 'coletor', 'doador', 'modo_de_aquisicao',\n",
      "       'data_de_aquisicao', 'ano_de_aquisicao', 'data_de_confeccao', 'autoria',\n",
      "       'nome_etnico', 'descricao', 'dimensoes', 'funcao', 'materia_prima',\n",
      "       'tecnica_confeccao', 'descritor_tematico', 'descritor_comum',\n",
      "       'numero_de_pecas', 'itens_relacionados', 'responsavel_guarda',\n",
      "       'inst_detentora', 'povo', 'autoidentificacao', 'lingua',\n",
      "       'estado_de_origem', 'geolocalizacao', 'pais_de_origem', 'exposicao',\n",
      "       'referencias', 'disponibilidade', 'qualificacao', 'historia_adm',\n",
      "       'notas_gerais', 'observacao', 'conservacao', 'image_path'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Loading dataset\n",
    "ind_df = pd.read_csv('../data/indigenous_collection_processed.csv', index_col='id')\n",
    "print(f'Dataframe columns: \\n{ind_df.columns}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c7ccfb45-9155-48a9-b5fa-fbf4e585822c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "from IPython.core.magic import register_cell_magic\n",
    "\n",
    "# Creating skip cell command\n",
    "@register_cell_magic\n",
    "def skip(line, cell):\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0915c122-a4e2-4a09-896f-0c1579ae45c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Centralizing main imports so we can run the models separately\n",
    "import os\n",
    "import time\n",
    "import math\n",
    "import random\n",
    "from glob import glob\n",
    "from tqdm.notebook import tqdm\n",
    "from PIL import Image\n",
    "\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from captum.attr import LayerIntegratedGradients\n",
    "from openai import OpenAI\n",
    "\n",
    "from language_training_utils import *\n",
    "\n",
    "# import language_training_utils\n",
    "# importlib.reload(language_training_utils)\n",
    "# from language_training_utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a033d0f-bc94-45f8-8d78-5e6b93d6e582",
   "metadata": {},
   "source": [
    "# Language Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f46409cc-bfb6-4c8d-a12c-262e0b046f26",
   "metadata": {},
   "source": [
    "Clustering experiments with text feature extractors. The idea is to fine-tune some pre-trained transformer models on our dataset and then remove the last layer of the model to cluster on the embedding space projections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "126eed16-4cdc-4b27-9865-33531d0aae1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'descricao' token-length statistics:\n",
      "Min: 2; Max: 420; \n",
      "Mean: 56.91137187559626; std: 35.49477155731764; \n",
      "Mode: ModeResult(mode=33, count=380); \n",
      "Q1: 32.0; \n",
      "Q2: 49.0; \n",
      "Q3: 73.0;\n",
      "How many sentences longer than 128 tokens? 856\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1000x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Studying the distribution of sentence token-lengths on dataframe 'descricao' to make a \n",
    "# decision regarding the max sequence length to use for models (also with memory restrictions)\n",
    "sentence_list = list(ind_df['descricao'].dropna())\n",
    "tokenizer = AutoTokenizer.from_pretrained('neuralmind/bert-large-portuguese-cased', \\\n",
    "                                          do_lower_case=False)\n",
    "token_lengths = [len(tokenizer.tokenize(sentence)) for sentence in sentence_list]\n",
    "token_lengths = np.array(token_lengths)\n",
    "\n",
    "# Printing statistics and finding out that, given the small amount of sentences with more than\n",
    "# 128 tokens, maybe we don't need to run an entire LLM pipeline to reduce sentence lengths. \n",
    "# Having said that, sequence length of 64 is pretty tight and would make us lose around 30%\n",
    "# of our data with no LLM pipeline to reduce sentences\n",
    "print(\"'descricao' token-length statistics:\")\n",
    "print(f\"\"\"Min: {np.min(token_lengths)}; Max: {np.max(token_lengths)}; \n",
    "Mean: {np.mean(token_lengths)}; std: {np.std(token_lengths)}; \n",
    "Mode: {stats.mode(token_lengths)}; \n",
    "Q1: {np.quantile(token_lengths, 0.25)}; \n",
    "Q2: {np.quantile(token_lengths, 0.50)}; \n",
    "Q3: {np.quantile(token_lengths, 0.75)};\"\"\")\n",
    "print(f\"How many sentences longer than 128 tokens? {len(np.where(token_lengths > 128)[0])}\\n\")\n",
    "\n",
    "# Plotting distribution\n",
    "plt.figure(figsize=(10, 3))\n",
    "plt.hist(token_lengths, \\\n",
    "         bins=int(len(np.unique(token_lengths))/2))\n",
    "plt.xlabel(\"'descricao' Token-Length\")\n",
    "plt.ylabel(\"(Bin) Count\")\n",
    "plt.title(\"Distribution of 'descricao' Token-Lengths\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2ef15eb-d225-448e-b453-c55575cb7435",
   "metadata": {},
   "source": [
    "Due to the very small amount of sentences with more than 128 tokens, and to the fact that, even those, are normally very close to 128 tokens, we first decided to postpone the bulding of an LLM pipeline to summarize longer sentences. Having said that, because of the bad quality of embeddings we got, we later decided to try reducing the sentences to more key aspects of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6d9abc78-21a3-4e7b-be12-1032e50674d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenating the descriptions an then mapping them to file splits for later API request\n",
    "chunk_size=20\n",
    "for i in range(0, len(sentence_list), chunk_size):\n",
    "    chunk = sentence_list[i:i+chunk_size]\n",
    "    file_num = i//chunk_size + 1\n",
    "    file_name = f\"sentences_{file_num}.txt\"\n",
    "    file_path = os.path.join('../data/raw_descriptions', file_name)\n",
    "\n",
    "    # Removing unwanted line breaks in the middle of the sentences to prevent broken pipeline\n",
    "    for j, description in enumerate(chunk):\n",
    "        chunk[j] = description.replace('\\n', '. ')\n",
    "    \n",
    "    # Write the chunk to file, one sentence per line\n",
    "    with open(file_path, 'w', encoding='utf-8') as f:\n",
    "        f.write('\\n'.join(chunk))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "69b3886f-3277-46d2-bac4-216c57916fa1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|██████▉            | 386/1050 [2:59:00<4:03:22, 21.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|█████▌         | 387/1050 [7:43:05<944:33:44, 5128.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Server problem...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|█████▌         | 388/1050 [7:44:33<665:00:32, 3616.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|█████▌         | 389/1050 [7:48:55<479:15:06, 2610.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Server problem...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|█████▌         | 390/1050 [7:50:22<339:45:54, 1853.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Server problem...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|█████▌         | 391/1050 [7:51:49<242:16:23, 1323.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Server problem...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|█████▉          | 392/1050 [7:53:17<174:06:59, 952.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|█████▉          | 393/1050 [7:57:40<136:07:04, 745.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Server problem...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|██████▍          | 394/1050 [7:59:06<99:49:03, 547.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Server problem...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|██████▍          | 395/1050 [8:00:33<74:32:11, 409.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Server problem...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|██████▍          | 396/1050 [8:02:00<56:51:13, 312.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Server problem...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|██████▍          | 397/1050 [8:03:28<44:29:12, 245.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Server problem...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|██████▍          | 398/1050 [8:04:55<35:50:21, 197.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n",
      "Server problem...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|██████          | 398/1050 [29:11:25<47:49:10, 264.03s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPStatusError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/envs/ind_thesis/lib/python3.10/site-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, cast_to, options, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1013\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1014\u001b[0;31m                 \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1015\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mhttpx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mHTTPStatusError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# thrown on 4xx and 5xx status code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ind_thesis/lib/python3.10/site-packages/httpx/_models.py\u001b[0m in \u001b[0;36mraise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    760\u001b[0m         \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merror_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merror_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 761\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mHTTPStatusError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    762\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mHTTPStatusError\u001b[0m: Client error '429 Too Many Requests' for url 'https://api.groq.com/openai/v1/chat/completions'\nFor more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1664058/1662374458.py\u001b[0m in \u001b[0;36m<cell line: 15>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m             response = client.chat.completions.create(\n\u001b[0m\u001b[1;32m     24\u001b[0m                 \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'meta-llama/llama-4-maverick-17b-128e-instruct'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m                 messages=[\n",
      "\u001b[0;32m~/anaconda3/envs/ind_thesis/lib/python3.10/site-packages/openai/_utils/_utils.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    285\u001b[0m                         \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"Missing required argument: {quote(missing[0])}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 287\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    288\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    289\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ind_thesis/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, reasoning_effort, response_format, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    923\u001b[0m     ) -> ChatCompletion | Stream[ChatCompletionChunk]:\n\u001b[1;32m    924\u001b[0m         \u001b[0mvalidate_response_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse_format\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 925\u001b[0;31m         return self._post(\n\u001b[0m\u001b[1;32m    926\u001b[0m             \u001b[0;34m\"/chat/completions\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    927\u001b[0m             body=maybe_transform(\n",
      "\u001b[0;32m~/anaconda3/envs/ind_thesis/lib/python3.10/site-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mpost\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1237\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"post\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mto_httpx_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1238\u001b[0m         )\n\u001b[0;32m-> 1239\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mResponseT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream_cls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream_cls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1241\u001b[0m     def patch(\n",
      "\u001b[0;32m~/anaconda3/envs/ind_thesis/lib/python3.10/site-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, cast_to, options, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1018\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mremaining_retries\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_should_retry\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1019\u001b[0m                     \u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1020\u001b[0;31m                     self._sleep_for_retry(\n\u001b[0m\u001b[1;32m   1021\u001b[0m                         \u001b[0mretries_taken\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretries_taken\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1022\u001b[0m                         \u001b[0mmax_retries\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_retries\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ind_thesis/lib/python3.10/site-packages/openai/_base_client.py\u001b[0m in \u001b[0;36m_sleep_for_retry\u001b[0;34m(self, retries_taken, max_retries, options, response)\u001b[0m\n\u001b[1;32m   1058\u001b[0m         \u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Retrying request to %s in %f seconds\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1059\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1060\u001b[0;31m         \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1061\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1062\u001b[0m     def _process_response(\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Function to load files\n",
    "def load_file(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        return f.read()\n",
    "\n",
    "# Initializing API client\n",
    "groq_key = load_file('../data/groq').strip()\n",
    "os.environ['OPENAI_API_KEY'] = groq_key\n",
    "client = OpenAI(\n",
    "    base_url='https://api.groq.com/openai/v1',\n",
    "    api_key=os.environ['OPENAI_API_KEY'],\n",
    ")\n",
    "\n",
    "# Going through sentences, making request for LLM model and rewriting them\n",
    "for file in tqdm(os.listdir('../data/raw_descriptions')):\n",
    "    if 'sentences' not in file:\n",
    "        continue\n",
    "    descriptions = load_file(os.path.join('../data/raw_descriptions', file))\n",
    "\n",
    "    # Keep trying inspite of internal serveer errors\n",
    "    while True:\n",
    "        try:\n",
    "            response = client.chat.completions.create(\n",
    "                model='meta-llama/llama-4-maverick-17b-128e-instruct',\n",
    "                messages=[\n",
    "                    {\n",
    "                        'role': 'system',\n",
    "                        'content': \"Para CADA linha do texto (mesmo repetida), gere uma versão reduzida em até 62 tokens, preservando sentido e palavras chaves. Responda com exatamente todas as linhas reduzidas, na mesma ordem, sem introdução.\",\n",
    "                    },\n",
    "                    {\n",
    "                        'role': 'user',\n",
    "                        'content': f'{descriptions}',\n",
    "                    },\n",
    "                ],\n",
    "                temperature=0.3,\n",
    "                max_tokens=8192,\n",
    "            )\n",
    "\n",
    "            # Writing new sentences, but checking for the correct number of (summarized)\n",
    "            # sentences first\n",
    "            new_sentences = response.choices[0].message.content.strip()\n",
    "\n",
    "            if len(new_sentences.split('\\n')) != chunk_size:\n",
    "                # So we don't overload the server\n",
    "                time.sleep(15)\n",
    "                continue\n",
    "            \n",
    "            else:\n",
    "                with open(os.path.join('../data/summarized_descriptions', file), \\\n",
    "                          'w', encoding='utf-8') as f:\n",
    "                    f.write(new_sentences)\n",
    "                break\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f'Server problem... {e}')\n",
    "            time.sleep(30)\n",
    "    \n",
    "    # Don't exceeed API's (free) limit\n",
    "    time.sleep(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62961eb2-29b4-4170-90e3-181c175ee88d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for file in [i for i in os.listdir('../data/summarized_descriptions') if 'sentences' in i]:\n",
    "    descriptions = load_file(os.path.join('../data/summarized_descriptions', file))\n",
    "    if len(descriptions.split('\\n')) != 20:\n",
    "        descriptions = descriptions.split('\\n')\n",
    "        print(f\"{file} -> {len(descriptions)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "471bb771-df3e-41fb-950f-1f1a0b9be285",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Helper function to sort by the numeric suffix\n",
    "def extract_index(file):\n",
    "    return int(file.split('_')[-1].split('.')[0])\n",
    "\n",
    "# Getting all (summarized) sentences in proper order\n",
    "files = glob('../data/summarized_descriptions/sentences_*.txt')\n",
    "files = sorted(files, key=extract_index)\n",
    "\n",
    "summarized_sentence_list = []\n",
    "for file in files:\n",
    "    with open(file, 'r', encoding='utf-8') as f:\n",
    "        lines = [line.strip() for line in f if line.strip()]\n",
    "        summarized_sentence_list.extend(lines)\n",
    "\n",
    "# Creating column with summarized sentences\n",
    "ind_df['descricao_resumida'] = ''\n",
    "ind_df.loc[ind_df['descricao'].isna(), 'descricao_resumida'] = pd.NA\n",
    "ind_df.loc[ind_df['descricao'].notna(), 'descricao_resumida'] = summarized_sentence_list\n",
    "\n",
    "# Reanalyzing token distribution in sentences\n",
    "token_lengths = [len(tokenizer.tokenize(sentence)) for sentence in summarized_sentence_list]\n",
    "token_lengths = np.array(token_lengths)\n",
    "\n",
    "# Plotting distribution\n",
    "plt.figure(figsize=(10, 3))\n",
    "plt.hist(token_lengths, \\\n",
    "         bins=int(len(np.unique(token_lengths))/2))\n",
    "plt.xlabel(\"Summarized 'descricao' Token-Length\")\n",
    "plt.ylabel(\"(Bin) Count\")\n",
    "plt.title(\"Distribution of Summarized 'descricao' Token-Lengths\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "01a6d6ac-3836-44ee-9971-dc8878fc3a8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                 | 0/1050 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentences_129.txt positivo: gravata emblemática xavante confeccionada com fios de algodão torcidos e tufos nas extremidades, com pendente de fibras desconhecidas fixadas com resina\n",
      "negativo: fita frontal circular de olho de palmeira de babaçu, com motivos geometrizantes amarelos e pingente\n",
      "\n",
      "positivo: fita circular frontal feita a partir do olho da palmeira de babaçu, apresentando desenhos geométricos em preto e ocre\n",
      "negativo: gravata xavante feita com algodão torcido natural e fita de fibra de buriti. pendentes da mesma fibra, pintados de rósea\n",
      "\n",
      "positivo: fita circular feita do olho da palmeira babaçu com desenhos geométricos pretos\n",
      "negativo: gravata xavante feita com algodão entrançado e torcido, pintada de urucum, com pendentes de algodão e nós nas pontas\n",
      "\n",
      "positivo: gravata emblemática xavante feita de algodão entrançado e torcido, com pintura de urucum e cordões de algodão com nós nas pontas\n",
      "negativo: fita frontal circular de olho de palmeira de babaçu, com fibra natural e pendente de cabelo\n",
      "\n",
      "positivo: fita circular de olho de palmeira babaçu com desenhos geométricos pretos e ocre\n",
      "negativo: gravata xavante feita com algodão torcido natural e fita de fibra de buriti. pendentes da mesma fibra, pintados de rósea\n",
      "\n",
      "positivo: gravata xavante confeccionada com algodão torcido de cor natural e fita de fibra de buriti, com pendentes da mesma fibra pintados de rosa\n",
      "negativo: fita frontal circular de olho de palmeira de babaçu, com motivos geometrizantes pretos\n",
      "\n",
      "positivo: fita circular frontal de olho de palmeira babaçu com motivos geométricos pretos e ocre\n",
      "negativo: gravata xavante feita com algodão entrançado e torcido, pintada de urucum, com pendentes de algodão e nós\n",
      "\n",
      "positivo: fita circular feita a partir do olho da palmeira de babaçu com desenhos geométricos pretos e ocre\n",
      "negativo: gravata xavante feita com algodão torcido natural e fita de fibra de buriti. pendentes da mesma fibra, pintados de rósea\n",
      "\n",
      "positivo: gravata emblemática xavante feita de algodão entrançado e torcido, pintada com urucum e com pendentes de algodão e nós\n",
      "negativo: fita frontal circular de olho de palmeira de babaçu, com fibra natural e pendente de cabelo\n",
      "\n",
      "positivo: fita circular frontal feita do olho da palmeira babaçu com desenhos geométricos pretos e ocre\n",
      "negativo: gravata xavante feita com algodão torcido natural e fita de fibra de buriti. pendentes da mesma fibra, pintados de rósea\n",
      "\n",
      "positivo: fita circular de olho de palmeira babaçu com desenhos geométricos pretos e ocre\n",
      "negativo: gravata xavante feita com algodão entrançado e torcido, pintada de urucum, com pendentes de algodão e nós nas pontas\n",
      "\n",
      "positivo: gravata xavante feita de algodão entrançado e torcido, com pintura de urucum e pendentes de algodão com nós\n",
      "negativo: fita frontal circular de olho de palmeira de babaçu, com motivos geometrizantes amarelos e pingente\n",
      "\n",
      "positivo: fita circular frontal de olho de palmeira babaçu com motivos geométricos pretos e ocre\n",
      "negativo: gravata xavante feita com algodão torcido natural e fita de fibra de buriti. pendentes da mesma fibra, pintados de rósea\n",
      "\n",
      "positivo: gravata emblemática xavante feita de algodão entrançado e torcido, com pintura de urucum e cordões de algodão com nós\n",
      "negativo: fita frontal circular de olho de palmeira de babaçu, com fibra natural e pendente de cabelo\n",
      "\n",
      "positivo: gravata xavante feita com algodão torcido e tufos nas pontas, com pendente de fibras de embira fixadas com resina\n",
      "negativo: fita frontal circular de olho de palmeira de babaçu, com motivos geometrizantes amarelos e pingente\n",
      "\n",
      "positivo: fita circular frontal feita a partir do olho da palmeira de babaçu com desenhos geométricos em amarelo e um pingente\n",
      "negativo: gravata xavante feita com algodão entrançado e torcido, pintada de urucum, com pendentes de algodão e nós\n",
      "\n",
      "positivo: fita circular de olho de palmeira babaçu com desenhos geométricos pretos\n",
      "negativo: gravata xavante feita com algodão torcido natural e fita de fibra de buriti. pendentes da mesma fibra, pintados de rósea\n",
      "\n",
      "positivo: fita circular frontal de olho de palmeira babaçu com motivos geométricos pretos e ocre\n",
      "negativo: gravata xavante feita com algodão entrançado e torcido, pintada de urucum, com pendentes de algodão e nós\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                 | 0/1050 [00:13<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1664058/4018177405.py\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;31m# Don't exceeed API's (free) limit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m     \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Generating a dataset of positive and negative examples for the contrastive learning using \n",
    "# LLMs (so that we don't need to rely on dropout noise for contrastive signal)\n",
    "for file in tqdm(os.listdir('../data/summarized_descriptions')):\n",
    "    if 'sentences' not in file:\n",
    "        continue\n",
    "    descriptions = load_file(os.path.join('../data/summarized_descriptions', file))\n",
    "\n",
    "    # Keep trying inspite of internal serveer errors\n",
    "    while True:\n",
    "        try:\n",
    "            response = client.chat.completions.create(\n",
    "                model='meta-llama/llama-4-maverick-17b-128e-instruct',\n",
    "                messages=[\n",
    "                    {\n",
    "                        'role': 'system',\n",
    "                        'content': f\"Uma lista de frases será fornecida, uma por linha. Para CADA linha faça duas tarefas: gere uma paráfrase com até 62 tokens e escolha uma outra frase da lista que é a semanticamente mais diferente da frase alvo. Sua resposta deve conter exatamente uma paráfrase e uma negativa para CADA frase original. O formato deve ser 'positivo: <paráfrase> \\n negativo: <negativa> \\n\\n' para cada frase. Não responda mais nada.\",\n",
    "                    },\n",
    "                    {\n",
    "                        'role': 'user',\n",
    "                        'content': f'{descriptions}',\n",
    "                    },\n",
    "                ],\n",
    "                temperature=0.3,\n",
    "                max_tokens=2048,\n",
    "            )\n",
    "\n",
    "            # Writing new sentences, but checking for the correct number of sentences first\n",
    "            new_sentences = response.choices[0].message.content.strip()\n",
    "            \n",
    "            if len(new_sentences.split('\\n\\n')) != 2*chunk_size:\n",
    "                # So we don't overload the server\n",
    "                time.sleep(15)\n",
    "                continue\n",
    "            \n",
    "            else:\n",
    "                with open(os.path.join('../data/contrastive_descriptions', file), \\\n",
    "                          'w', encoding='utf-8') as f:\n",
    "                    f.write(new_sentences)\n",
    "                break\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f'Server problem... {e}')\n",
    "            time.sleep(30)\n",
    "    \n",
    "    # Don't exceeed API's (free) limit\n",
    "    time.sleep(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "728a3946-50d2-4207-b291-f949b5a09529",
   "metadata": {},
   "source": [
    "## BERTimbau"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "924a164f-928d-446a-9da5-1b47265fa48a",
   "metadata": {},
   "source": [
    "### Vanilla Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5ff6dff-82d0-4b1b-80be-bf1e25145b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting device\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Initializing model, turning it into eval mode and zeroing out the gradients\n",
    "model = AutoModel.from_pretrained('neuralmind/bert-large-portuguese-cased')\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "model.zero_grad()\n",
    "\n",
    "# Getting sentences' dataset, dataloader and splits\n",
    "text_ind_df = ind_df[~ind_df['descricao'].isna()]\n",
    "max_length = 64\n",
    "text_dataset = TextDataset(text_ind_df, tokenizer, max_length=max_length)\n",
    "full_batch_size = 1\n",
    "text_dataloader = get_dataloaders(text_dataset, full_batch_size)\n",
    "\n",
    "data_size = len(text_dataset)\n",
    "train_size = int(0.8*data_size)\n",
    "val_size = int(0.1*data_size)\n",
    "test_size = data_size - train_size - val_size\n",
    "splits = [train_size, val_size, test_size]\n",
    "split_batch_size = 16\n",
    "text_dataset_splits, text_dataloader_splits = get_dataloaders(text_dataset, \\\n",
    "                                                              split_batch_size, splits)\n",
    "\n",
    "# Initializing baseline input_ids and attention_mask, and computing its embedding\n",
    "baseline_input_ids = torch.full((1, max_length), tokenizer.pad_token_id).to(device)\n",
    "baseline_input_ids[:, 0] = tokenizer.cls_token_id\n",
    "baseline_input_ids[:, -1] = tokenizer.sep_token_id\n",
    "baseline_attention_mask = torch.zeros_like(baseline_input_ids).to(device)\n",
    "baseline_attention_mask[:, 0] = 1\n",
    "baseline_attention_mask[:, -1] = 1\n",
    "with torch.no_grad():\n",
    "    baseline_outputs = model(input_ids=baseline_input_ids, \\\n",
    "                             attention_mask=baseline_attention_mask)\n",
    "    baseline_embedding = baseline_outputs.last_hidden_state[:, 0]\n",
    "\n",
    "# Initializing captum compatible model and layer integrated gradients. We use layer integrated\n",
    "# gradients here because we can't compute gradients with respect to (discrete) indices\n",
    "# directly, so we compute gradients with respect to the embeddings of the input tokens\n",
    "vanilla_wrapped_model = CaptumWrappedModel(model, tokenizer.pad_token_id, \\\n",
    "                                          baseline_embedding, device, target_type='cos-sim')\n",
    "vanilla_wrapped_model.eval()\n",
    "\n",
    "lig = LayerIntegratedGradients(vanilla_wrapped_model, model.embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88390f55-9681-455d-86af-4050b3e5014d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Iterating through the dataloader to compute embeddings and token attributions\n",
    "analyzed_sample = 0\n",
    "vanilla_bertimbau_indices = []\n",
    "vanilla_bertimbau_embeddings = []\n",
    "vanilla_bertimbau_tokens = []\n",
    "vanilla_bertimbau_attributions = []\n",
    "for indices, input_ids, _ in tqdm(text_dataloader):\n",
    "    # Saving indices\n",
    "    vanilla_bertimbau_indices.append(indices)\n",
    "    \n",
    "    # Moving appropriate tensors to device\n",
    "    input_ids = input_ids.to(device)\n",
    "\n",
    "    # Computing [CLS] token embeddings\n",
    "    cls_embeddings = get_embeddings(model, tokenizer, input_ids, device, fine_tuned=False)\n",
    "    vanilla_bertimbau_embeddings.append(cls_embeddings.cpu().detach())\n",
    "\n",
    "    # Computing attributions\n",
    "    tokens, attributions, delta = get_attributions(lig, tokenizer, input_ids, \\\n",
    "                                                   baseline_input_ids, \\\n",
    "                                                   attrib_aggreg_type='l2-norm', \\\n",
    "                                                   return_tokens=True, \\\n",
    "                                                   verbose=False, \\\n",
    "                                                   sample_num=min(analyzed_sample, \\\n",
    "                                                                  full_batch_size-1))\n",
    "    vanilla_bertimbau_tokens.append(tokens)\n",
    "    vanilla_bertimbau_attributions.append(attributions)\n",
    "    \n",
    "# Concatenating the batches\n",
    "vanilla_bertimbau_indices = np.array(vanilla_bertimbau_indices).squeeze(-1)\n",
    "vanilla_bertimbau_embeddings = np.array(torch.cat(vanilla_bertimbau_embeddings, dim=0))\n",
    "vanilla_bertimbau_tokens = np.array(vanilla_bertimbau_tokens)\n",
    "vanilla_bertimbau_attributions = np.array(torch.cat(vanilla_bertimbau_attributions, dim=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2834f8c9-2de2-44c7-af0f-7e4c26588c04",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Getting data projections\n",
    "vanilla_bertimbau_trimap, vanilla_bertimbau_tsne, \\\n",
    "vanilla_bertimbau_umap = data_projections(vanilla_bertimbau_embeddings)\n",
    "\n",
    "# Normalizing data for later plot on tool\n",
    "norm_factor = 12\n",
    "vanilla_bertimbau_trimap = normalize(vanilla_bertimbau_trimap, norm_factor)\n",
    "vanilla_bertimbau_tsne = normalize(vanilla_bertimbau_tsne, norm_factor)\n",
    "vanilla_bertimbau_umap = normalize(vanilla_bertimbau_umap, norm_factor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff2a6de6-5213-4f55-951c-69ab95f85256",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility function to plot rows on projection comparison plot\n",
    "def row_scatter_plot(projs, proj_names, row, color, rows=1, cols=3):\n",
    "    for i, (proj, proj_name) in enumerate(zip(projs, proj_names)):\n",
    "        plt.subplot(rows, cols, i+1+(cols*(row-1)))\n",
    "        plt.scatter(proj[:, 0], proj[:, 1], c=color)\n",
    "        plt.title(f\"{proj_name}\")\n",
    "        plt.xlabel(\"\")\n",
    "        plt.ylabel(\"\")\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "\n",
    "# Visualizing resulting projections\n",
    "plt.figure(figsize=(10,4))\n",
    "# plt.suptitle('Comparing Projections of BERTimbau Models')\n",
    "\n",
    "# Plotting vanilla BERTimbau projections\n",
    "projs = [vanilla_bertimbau_trimap, vanilla_bertimbau_tsne, vanilla_bertimbau_umap]\n",
    "proj_names = ['Vanilla BERTimbau with TriMap', 'Vanilla BERTimbau with t-SNE', \\\n",
    "              'Vanilla BERTimbau with UMAP']\n",
    "row_scatter_plot(projs, proj_names, 1, 'b', 1, )\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41e8e1a7-e593-405e-8ba0-947a383506a6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Saving outputs for visualization tool (trimap)\n",
    "_ = saving_outputs(vanilla_bertimbau_trimap, vanilla_bertimbau_tokens, \\\n",
    "                   vanilla_bertimbau_attributions, vanilla_bertimbau_indices, \\\n",
    "                   save_file='vanilla_bertimbau_trimap.csv')\n",
    "\n",
    "# Saving outputs for visualization tool (umap)\n",
    "_ = saving_outputs(vanilla_bertimbau_umap, vanilla_bertimbau_tokens, \\\n",
    "                   vanilla_bertimbau_attributions, vanilla_bertimbau_indices, \\\n",
    "                   save_file='vanilla_bertimbau_umap.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4deb8fa1-b3cd-4e8c-add2-f595995de51b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning memory\n",
    "clean_mem([vanilla_bertimbau_embeddings, vanilla_bertimbau_trimap, vanilla_bertimbau_tsne, \\\n",
    "vanilla_bertimbau_umap, vanilla_bertimbau_attributions, vanilla_bertimbau_indices])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f632b765-ef9d-453e-a9fc-76b55f8cd45a",
   "metadata": {},
   "source": [
    "### Unsupervised SimCSE (Contrastive Learning with no Labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fac0bf6a-842d-4149-9979-910b2515111f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Initializing SimCSE model and auxiliar variables\n",
    "dropout_prob = 0.1\n",
    "model = SimCSEModel(model, tokenizer.pad_token_id, device, dropout_prob)\n",
    "lr = 2e-5\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "epochs = 15\n",
    "temperature = 0.05 # Test 0.01, 0.05, 0.1, 0.2\n",
    "patience = max(4, math.ceil(epochs*0.1))\n",
    "model_name = 'simcse_bertimbau'\n",
    "train_dataloader, val_dataloader, test_dataloader = text_dataloader_splits\n",
    "\n",
    "# Calling training process\n",
    "simcse_indices, simcse_embeddings, \\\n",
    "train_losses, val_losses = contrastive_training_loop(model, optimizer, train_dataloader, \\\n",
    "                                                     val_dataloader, device, epochs, \\\n",
    "                                                     temperature, patience, model_name)\n",
    "\n",
    "# Checking if the model is \"unlearning\" a lot\n",
    "# Pearson BERTimbau (large) -> 0.8842\n",
    "# Pearson Albertina (1.5B) -> 0.9007\n",
    "stsb_test(model, usimcse=True)\n",
    "\n",
    "plot_training_curves(train_losses, val_losses, model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8ef1adf-3894-4965-afef-a2e679eac9eb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Recomputing baseline embedding for fine-tuned model\n",
    "with torch.no_grad():\n",
    "    baseline_embedding = model(input_ids=baseline_input_ids, train=False)\n",
    "\n",
    "# Intanciating attribution computation class for fine-tuned model\n",
    "simcse_wrapped_model = CaptumWrappedModel(model.model, tokenizer.pad_token_id, \\\n",
    "                                          baseline_embedding, device, target_type='cos-sim')\n",
    "simcse_wrapped_model.eval()\n",
    "lig = LayerIntegratedGradients(simcse_wrapped_model, model.model.embeddings)\n",
    "\n",
    "# Iterating through the dataloader to compute token attributions for fine-tuned model\n",
    "analyzed_sample = 0\n",
    "simcse_bertimbau_indices = []\n",
    "simcse_bertimbau_embeddings = []\n",
    "simcse_bertimbau_attributions = []\n",
    "for indices, input_ids, _ in tqdm(text_dataloader):\n",
    "    # Saving indices\n",
    "    simcse_bertimbau_indices.append(indices)\n",
    "    \n",
    "    # Moving appropriate tensors to device\n",
    "    input_ids = input_ids.to(device)\n",
    "\n",
    "    # Computing [CLS] token embeddings\n",
    "    cls_embeddings = get_embeddings(model, tokenizer, input_ids, device, fine_tuned=True)\n",
    "    simcse_bertimbau_embeddings.append(cls_embeddings.cpu().detach())\n",
    "\n",
    "    # Computing attributions\n",
    "    attributions, delta = get_attributions(lig, tokenizer, input_ids, baseline_input_ids, \\\n",
    "                                           attrib_aggreg_type='l2-norm', \\\n",
    "                                           return_tokens=False, verbose=False, \\\n",
    "                                           sample_num=min(analyzed_sample, full_batch_size-1))\n",
    "    simcse_bertimbau_attributions.append(attributions)\n",
    "\n",
    "# Concatenating the batches\n",
    "simcse_bertimbau_indices = np.array(simcse_bertimbau_indices).squeeze(-1)\n",
    "simcse_bertimbau_embeddings = np.array(torch.cat(simcse_bertimbau_embeddings, dim=0))\n",
    "simcse_bertimbau_attributions = np.array(torch.cat(simcse_bertimbau_attributions, dim=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1b75b63-07ab-4a90-bd13-7c49207d2b34",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Getting data projections\n",
    "simcse_bertimbau_trimap, simcse_bertimbau_tsne, \\\n",
    "simcse_bertimbau_umap = data_projections(simcse_bertimbau_embeddings)\n",
    "\n",
    "# Normalizing data for later plot on tool\n",
    "norm_factor = 12\n",
    "simcse_bertimbau_trimap = normalize(simcse_bertimbau_trimap, norm_factor)\n",
    "simcse_bertimbau_tsne = normalize(simcse_bertimbau_tsne, norm_factor)\n",
    "simcse_bertimbau_umap = normalize(simcse_bertimbau_umap, norm_factor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e04054e-2c39-435a-bffd-58405a73fb8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing resulting projections\n",
    "plt.figure(figsize=(10,4))\n",
    "\n",
    "# Plotting SimCSE BERTimbau projections\n",
    "projs = [simcse_bertimbau_trimap, simcse_bertimbau_tsne, simcse_bertimbau_umap]\n",
    "proj_names = ['SimCSE BERTimbau with TriMap', 'SimCSE BERTimbau with t-SNE', \\\n",
    "              'SimCSE BERTimbau with UMAP']\n",
    "row_scatter_plot(projs, proj_names, 1, 'b', 1, )\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4877fdc2-2f86-4af4-818e-60a297f4bdbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving outputs for visualization tool (trimap)\n",
    "_ = saving_outputs(simcse_bertimbau_trimap, vanilla_bertimbau_tokens, \\\n",
    "                   simcse_bertimbau_attributions, simcse_bertimbau_indices, \\\n",
    "                   save_file='simcse_bertimbau_trimap.csv')\n",
    "\n",
    "# Saving outputs for visualization tool (umap)\n",
    "_ = saving_outputs(simcse_bertimbau_umap, vanilla_bertimbau_tokens, \\\n",
    "                   simcse_bertimbau_attributions, simcse_bertimbau_indices, \\\n",
    "                   save_file='simcse_bertimbau_umap.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d11843ef-fd4c-41c3-b452-8009b71e0f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning memory\n",
    "clean_mem([simcse_bertimbau_embeddings, simcse_bertimbau_trimap, simcse_bertimbau_tsne, \\\n",
    "simcse_bertimbau_umap, simcse_bertimbau_attributions, simcse_bertimbau_indices])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e15e53a6-f8b8-4ad7-b6e5-0c052f0cb244",
   "metadata": {},
   "source": [
    "## Albertina"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17bd2217-556b-4979-a3c8-b99da09acf04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting new tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('PORTULAN/albertina-1b5-portuguese-ptbr-encoder', \\\n",
    "                                          use_fast=True)\n",
    "\n",
    "# Initializing model, turning it into eval mode and zeroing out the gradients\n",
    "model = AutoModel.from_pretrained('PORTULAN/albertina-1b5-portuguese-ptbr-encoder')\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "model.zero_grad()\n",
    "\n",
    "# Getting sentences' dataset, dataloader and splits\n",
    "max_length = 64\n",
    "text_dataset = TextDataset(text_ind_df, tokenizer, max_length=max_length)\n",
    "full_batch_size = 1\n",
    "text_dataloader = get_dataloaders(text_dataset, full_batch_size)\n",
    "\n",
    "data_size = len(text_dataset)\n",
    "train_size = int(0.8*data_size)\n",
    "val_size = int(0.1*data_size)\n",
    "test_size = data_size - train_size - val_size\n",
    "splits = [train_size, val_size, test_size]\n",
    "split_batch_size = 16\n",
    "text_dataset_splits, text_dataloader_splits = get_dataloaders(text_dataset, \\\n",
    "                                                              split_batch_size, splits)\n",
    "\n",
    "# Initializing baseline input_ids and attention_mask, and computing its embedding\n",
    "baseline_input_ids = torch.full((1, max_length), tokenizer.pad_token_id).to(device)\n",
    "baseline_input_ids[:, 0] = tokenizer.cls_token_id\n",
    "baseline_input_ids[:, -1] = tokenizer.sep_token_id\n",
    "baseline_attention_mask = torch.zeros_like(baseline_input_ids).to(device)\n",
    "baseline_attention_mask[:, 0] = 1\n",
    "baseline_attention_mask[:, -1] = 1\n",
    "with torch.no_grad():\n",
    "    baseline_outputs = model(input_ids=baseline_input_ids, \\\n",
    "                             attention_mask=baseline_attention_mask)\n",
    "    baseline_embedding = baseline_outputs.last_hidden_state[:, 0]\n",
    "\n",
    "# Initializing captum compatible model and layer integrated gradients. We use layer integrated\n",
    "# gradients here because we can't compute gradients with respect to (discrete) indices\n",
    "# directly, so we compute gradients with respect to the embeddings of the input tokens\n",
    "vanilla_wrapped_model = CaptumWrappedModel(model, tokenizer.pad_token_id, \\\n",
    "                                          baseline_embedding, device, target_type='cos-sim')\n",
    "vanilla_wrapped_model.eval()\n",
    "\n",
    "lig = LayerIntegratedGradients(vanilla_wrapped_model, model.embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dbecce6-97a1-4fb4-8ad8-f4aa5b2e7e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterating through the dataloader to compute embeddings and token attributions\n",
    "analyzed_sample = 0\n",
    "vanilla_albertina_indices = []\n",
    "vanilla_albertina_embeddings = []\n",
    "vanilla_albertina_tokens = []\n",
    "vanilla_albertina_attributions = []\n",
    "for indices, input_ids, _ in tqdm(text_dataloader):\n",
    "    # Saving indices\n",
    "    vanilla_albertina_indices.append(indices)\n",
    "    \n",
    "    # Moving appropriate tensors to device\n",
    "    input_ids = input_ids.to(device)\n",
    "\n",
    "    # Computing [CLS] token embeddings\n",
    "    cls_embeddings = get_embeddings(model, tokenizer, input_ids, device, fine_tuned=False)\n",
    "    vanilla_albertina_embeddings.append(cls_embeddings.cpu().detach())\n",
    "\n",
    "    # Computing attributions\n",
    "    tokens, attributions, delta = get_attributions(lig, tokenizer, input_ids, \\\n",
    "                                                   baseline_input_ids, \\\n",
    "                                                   attrib_aggreg_type='l2-norm', \\\n",
    "                                                   return_tokens=True, \\\n",
    "                                                   verbose=False, \\\n",
    "                                                   sample_num=min(analyzed_sample, \\\n",
    "                                                                  full_batch_size-1))\n",
    "    vanilla_albertina_tokens.append(tokens)\n",
    "    vanilla_albertina_attributions.append(attributions)\n",
    "    \n",
    "# Concatenating the batches\n",
    "vanilla_albertina_indices = np.array(vanilla_albertina_indices).squeeze(-1)\n",
    "vanilla_albertina_embeddings = np.array(torch.cat(vanilla_albertina_embeddings, dim=0))\n",
    "vanilla_albertina_tokens = np.array(vanilla_albertina_tokens)\n",
    "vanilla_albertina_attributions = np.array(torch.cat(vanilla_albertina_attributions, dim=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a09e7ea9-6738-4a7c-9f87-5318420f7f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting data projections\n",
    "vanilla_albertina_trimap, vanilla_albertina_tsne, \\\n",
    "vanilla_albertina_umap = data_projections(vanilla_albertina_embeddings)\n",
    "\n",
    "# Normalizing data for later plot on tool\n",
    "norm_factor = 12\n",
    "vanilla_albertina_trimap = normalize(vanilla_albertina_trimap, norm_factor)\n",
    "vanilla_albertina_tsne = normalize(vanilla_albertina_tsne, norm_factor)\n",
    "vanilla_albertina_umap = normalize(vanilla_albertina_umap, norm_factor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "892f6f07-a7fc-4ce4-9623-2179ed4bc6c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing resulting projections\n",
    "plt.figure(figsize=(10,4))\n",
    "\n",
    "# Plotting SimCSE BERTimbau projections\n",
    "projs = [vanilla_albertina_trimap, vanilla_albertina_tsne, vanilla_albertina_umap]\n",
    "proj_names = ['Vanilla Albertina with TriMap', 'Vanilla Albertina with t-SNE', \\\n",
    "              'Vanilla Albertina with UMAP']\n",
    "row_scatter_plot(projs, proj_names, 1, 'b', 1, )\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0359e963-9800-4b93-bfae-6b0193b988ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving outputs for visualization tool (trimap)\n",
    "_ = saving_outputs(vanilla_albertina_trimap, vanilla_albertina_tokens, \\\n",
    "                   vanilla_albertina_attributions, vanilla_albertina_indices, \\\n",
    "                   save_file='vanilla_albertina_trimap.csv')\n",
    "\n",
    "# Saving outputs for visualization tool (umap)\n",
    "_ = saving_outputs(vanilla_albertina_umap, vanilla_albertina_tokens, \\\n",
    "                   vanilla_albertina_attributions, vanilla_albertina_indices, \\\n",
    "                   save_file='vanilla_albertina_umap.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f4656d7-219f-4034-b9ef-54183f54e39c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning memory\n",
    "clean_mem([vanilla_albertina_embeddings, vanilla_albertina_trimap, vanilla_albertina_tsne, \\\n",
    "vanilla_albertina_umap, vanilla_albertina_attributions, vanilla_albertina_indices])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
