{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e8d3d3-5137-4c49-a9c7-4ef40f64c737",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Loading dataset\n",
    "ind_df = pd.read_csv('../data/indigenous_collection_processed.csv', index_col='id')\n",
    "print(f'Dataframe columns: \\n{ind_df.columns}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7ccfb45-9155-48a9-b5fa-fbf4e585822c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "from IPython.core.magic import register_cell_magic\n",
    "\n",
    "# Creating skip cell command\n",
    "@register_cell_magic\n",
    "def skip(line, cell):\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0915c122-a4e2-4a09-896f-0c1579ae45c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Centralizing main imports so we can run the models separately\n",
    "import os\n",
    "import time\n",
    "import math\n",
    "import random\n",
    "from glob import glob\n",
    "from tqdm.notebook import tqdm\n",
    "from PIL import Image\n",
    "\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from captum.attr import LayerIntegratedGradients\n",
    "from openai import OpenAI\n",
    "\n",
    "from language_training_utils import *\n",
    "\n",
    "# import language_training_utils\n",
    "# importlib.reload(language_training_utils)\n",
    "# from language_training_utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a033d0f-bc94-45f8-8d78-5e6b93d6e582",
   "metadata": {},
   "source": [
    "# Language Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f46409cc-bfb6-4c8d-a12c-262e0b046f26",
   "metadata": {},
   "source": [
    "Clustering experiments with text feature extractors. The idea is to fine-tune some pre-trained transformer models on our dataset and then remove the last layer of the model to cluster on the embedding space projections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "126eed16-4cdc-4b27-9865-33531d0aae1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Studying the distribution of sentence token-lengths on dataframe 'descricao' to make a \n",
    "# decision regarding the max sequence length to use for models (also with memory restrictions)\n",
    "sentence_list = list(ind_df['descricao'].dropna())\n",
    "tokenizer = AutoTokenizer.from_pretrained('neuralmind/bert-large-portuguese-cased', \\\n",
    "                                          do_lower_case=False)\n",
    "token_lengths = [len(tokenizer.tokenize(sentence)) for sentence in sentence_list]\n",
    "token_lengths = np.array(token_lengths)\n",
    "\n",
    "# Printing statistics and finding out that, given the small amount of sentences with more than\n",
    "# 128 tokens, maybe we don't need to run an entire LLM pipeline to reduce sentence lengths. \n",
    "# Having said that, sequence length of 64 is pretty tight and would make us lose around 30%\n",
    "# of our data with no LLM pipeline to reduce sentences\n",
    "print(\"'descricao' token-length statistics:\")\n",
    "print(f\"\"\"Min: {np.min(token_lengths)}; Max: {np.max(token_lengths)}; \n",
    "Mean: {np.mean(token_lengths)}; std: {np.std(token_lengths)}; \n",
    "Mode: {stats.mode(token_lengths)}; \n",
    "Q1: {np.quantile(token_lengths, 0.25)}; \n",
    "Q2: {np.quantile(token_lengths, 0.50)}; \n",
    "Q3: {np.quantile(token_lengths, 0.75)};\"\"\")\n",
    "print(f\"How many sentences longer than 128 tokens? {len(np.where(token_lengths > 128)[0])}\\n\")\n",
    "\n",
    "# Plotting distribution\n",
    "plt.figure(figsize=(10, 3))\n",
    "plt.hist(token_lengths, \\\n",
    "         bins=int(len(np.unique(token_lengths))/2))\n",
    "plt.xlabel(\"'descricao' Token-Length\")\n",
    "plt.ylabel(\"(Bin) Count\")\n",
    "plt.title(\"Distribution of 'descricao' Token-Lengths\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2ef15eb-d225-448e-b453-c55575cb7435",
   "metadata": {},
   "source": [
    "Due to the very small amount of sentences with more than 128 tokens, and to the fact that, even those, are normally very close to 128 tokens, we first decided to postpone the bulding of an LLM pipeline to summarize longer sentences. Having said that, because of the bad quality of embeddings we got, we later decided to try reducing the sentences to more key aspects of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41057e94-fbec-4d86-86b7-55a8eeb64d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining chucnk_size because we use it later\n",
    "chunk_size = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d9abc78-21a3-4e7b-be12-1032e50674d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%skip\n",
    "\n",
    "# Concatenating the descriptions an then mapping them to file splits for later API request\n",
    "for i in range(0, len(sentence_list), chunk_size):\n",
    "    chunk = sentence_list[i:i+chunk_size]\n",
    "    file_num = i//chunk_size + 1\n",
    "    file_name = f\"sentences_{file_num}.txt\"\n",
    "    file_path = os.path.join('../data/raw_descriptions', file_name)\n",
    "\n",
    "    # Removing unwanted line breaks in the middle of the sentences to prevent broken pipeline\n",
    "    for j, description in enumerate(chunk):\n",
    "        chunk[j] = description.replace('\\n', '. ')\n",
    "    \n",
    "    # Write the chunk to file, one sentence per line\n",
    "    with open(file_path, 'w', encoding='utf-8') as f:\n",
    "        f.write('\\n'.join(chunk))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cf378a1-8101-4b60-9aac-94ace2e597bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load files\n",
    "def load_file(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        return f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b3886f-3277-46d2-bac4-216c57916fa1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%skip\n",
    "\n",
    "# Initializing API client\n",
    "groq_key = load_file('../data/groq').strip()\n",
    "os.environ['OPENAI_API_KEY'] = groq_key\n",
    "client = OpenAI(\n",
    "    base_url='https://api.groq.com/openai/v1',\n",
    "    api_key=os.environ['OPENAI_API_KEY'],\n",
    ")\n",
    "\n",
    "# Going through sentences, making request for LLM model and rewriting them\n",
    "for file in tqdm(os.listdir('../data/raw_descriptions')):\n",
    "    if 'sentences' not in file:\n",
    "        continue\n",
    "    descriptions = load_file(os.path.join('../data/raw_descriptions', file))\n",
    "\n",
    "    # Keep trying inspite of internal server errors\n",
    "    try_counter = 0\n",
    "    while True:\n",
    "        try:\n",
    "            response = client.chat.completions.create(\n",
    "                model='meta-llama/llama-4-maverick-17b-128e-instruct',\n",
    "                messages=[\n",
    "                    {\n",
    "                        'role': 'system',\n",
    "                        'content': \"Para CADA linha do texto (mesmo repetida), gere uma versão reduzida em até 62 tokens, preservando sentido e palavras chaves. Responda com exatamente todas as linhas reduzidas, na mesma ordem, sem introdução.\",\n",
    "                    },\n",
    "                    {\n",
    "                        'role': 'user',\n",
    "                        'content': f'{descriptions}',\n",
    "                    },\n",
    "                ],\n",
    "                temperature=0.3,\n",
    "                max_tokens=8192,\n",
    "            )\n",
    "\n",
    "            # Writing new sentences, but checking for the correct number of (summarized)\n",
    "            # sentences first\n",
    "            new_sentences = response.choices[0].message.content.strip()\n",
    "\n",
    "            if len(new_sentences.split('\\n')) != chunk_size and try_counter < 5:\n",
    "                # So we don't overload the server\n",
    "                try_counter += 1\n",
    "                time.sleep(2)\n",
    "                continue\n",
    "            \n",
    "            else:\n",
    "                try_counter = 0\n",
    "                with open(os.path.join('../data/summarized_descriptions', file), \\\n",
    "                          'w', encoding='utf-8') as f:\n",
    "                    f.write(new_sentences)\n",
    "                break\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(file)\n",
    "            print(f'Server problem... {e}')\n",
    "            time.sleep(15)\n",
    "    \n",
    "    # Don't exceeed API's (free) limit\n",
    "    time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62961eb2-29b4-4170-90e3-181c175ee88d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the files that don't have the exact chunk_size number of descriptions to manually\n",
    "# fix them\n",
    "for file in [i for i in os.listdir('../data/summarized_descriptions') if 'sentences' in i]:\n",
    "    descriptions = load_file(os.path.join('../data/summarized_descriptions', file))\n",
    "    if len(descriptions.strip().split('\\n')) != chunk_size:\n",
    "        descriptions = descriptions.strip().split('\\n')\n",
    "        print(f\"{file} -> {len(descriptions)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "471bb771-df3e-41fb-950f-1f1a0b9be285",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to sort by the numeric suffix\n",
    "def extract_index(file):\n",
    "    return int(file.split('_')[-1].split('.')[0])\n",
    "\n",
    "# Getting all (summarized) sentences in proper order\n",
    "files = glob('../data/summarized_descriptions/sentences_*.txt')\n",
    "files = sorted(files, key=extract_index)\n",
    "\n",
    "summarized_sentence_list = []\n",
    "for file in files:\n",
    "    with open(file, 'r', encoding='utf-8') as f:\n",
    "        lines = [line.strip().lower() for line in f if line.strip()]\n",
    "        summarized_sentence_list.extend(lines)\n",
    "\n",
    "# Creating column with summarized sentences\n",
    "ind_df['descricao_resumida'] = ''\n",
    "ind_df.loc[ind_df['descricao'].isna(), 'descricao_resumida'] = pd.NA\n",
    "ind_df.loc[ind_df['descricao'].notna(), 'descricao_resumida'] = summarized_sentence_list\n",
    "\n",
    "# Reanalyzing token distribution in sentences\n",
    "token_lengths = [len(tokenizer.tokenize(sentence)) for sentence in summarized_sentence_list]\n",
    "token_lengths = np.array(token_lengths)\n",
    "\n",
    "# Plotting distribution\n",
    "plt.figure(figsize=(10, 3))\n",
    "plt.hist(token_lengths, \\\n",
    "         bins=int(len(np.unique(token_lengths))/2))\n",
    "plt.xlabel(\"Summarized 'descricao' Token-Length\")\n",
    "plt.ylabel(\"(Bin) Count\")\n",
    "plt.title(\"Distribution of Summarized 'descricao' Token-Lengths\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01a6d6ac-3836-44ee-9971-dc8878fc3a8a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%skip\n",
    "\n",
    "# Generating a dataset of positive and negative examples for the contrastive learning using \n",
    "# LLMs (so that we don't need to rely on dropout noise for contrastive signal)\n",
    "for file in tqdm(os.listdir('../data/summarized_descriptions')):\n",
    "    if 'sentences' not in file:\n",
    "        continue\n",
    "    descriptions = load_file(os.path.join('../data/summarized_descriptions', file))\n",
    "\n",
    "    # Keep trying inspite of internal server errors\n",
    "    try_counter = 0\n",
    "    while True:\n",
    "        try:\n",
    "            response = client.chat.completions.create(\n",
    "                model='meta-llama/llama-4-maverick-17b-128e-instruct',\n",
    "                messages=[\n",
    "                    {\n",
    "                        'role': 'system',\n",
    "                        'content': f\"Uma lista de frases será fornecida, uma por linha. Para CADA linha faça duas tarefas: gere uma paráfrase com até 62 tokens e escolha uma outra frase da lista que é a semanticamente mais diferente da frase alvo. Sua resposta deve conter exatamente uma paráfrase e uma negativa para CADA frase original. O formato deve ser 'positivo: <paráfrase> \\n negativo: <negativa> \\n\\n' para cada frase. Não responda mais nada.\",\n",
    "                    },\n",
    "                    {\n",
    "                        'role': 'user',\n",
    "                        'content': f'{descriptions}',\n",
    "                    },\n",
    "                ],\n",
    "                temperature=0.3,\n",
    "                max_tokens=4096,\n",
    "            )\n",
    "\n",
    "            # Writing new sentences, but doing some processing first\n",
    "            new_sentences = response.choices[0].message.content.strip()\n",
    "\n",
    "            # Fixing file structure for the common case of no double line-break\n",
    "            if len(new_sentences.strip().split('\\n\\n')) == 1:\n",
    "                even = 0\n",
    "                fixed_new_sentences = []\n",
    "                for c in new_sentences.strip():\n",
    "                    fixed_new_sentences.append(c)\n",
    "                    if c == '\\n':\n",
    "                        if even:\n",
    "                            fixed_new_sentences.append('\\n')\n",
    "                        even = 1 - even\n",
    "                new_sentences = ''.join(fixed_new_sentences)\n",
    "                        \n",
    "            # Checking number of sentences\n",
    "            if len(new_sentences.strip().split('\\n\\n')) != chunk_size and try_counter < 5:\n",
    "                # So we don't overload the server\n",
    "                try_counter += 1\n",
    "                time.sleep(2)\n",
    "                continue\n",
    "            \n",
    "            else:\n",
    "                try_counter = 0\n",
    "                with open(os.path.join('../data/contrastive_triplets', file), \\\n",
    "                          'w', encoding='utf-8') as f:\n",
    "                    f.write(new_sentences)\n",
    "                break\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f'Server problem... {e}')\n",
    "            time.sleep(15)\n",
    "    \n",
    "    # Don't exceeed API's (free) limit\n",
    "    time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c10533f3-31cb-4e53-a316-fe214b938e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the files that don't have the exact chunk_size number of contrastive descriptions\n",
    "# to manually fix them\n",
    "for file in [i for i in os.listdir('../data/contrastive_triplets') if 'sentences' in i]:\n",
    "    descriptions = load_file(os.path.join('../data/contrastive_triplets', file))\n",
    "    if len(descriptions.strip().split('\\n\\n')) != chunk_size:\n",
    "        descriptions = descriptions.strip().split('\\n\\n')\n",
    "        print(f\"{file} -> {len(descriptions)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc720c35-e329-4112-aa22-cf23a88cc012",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting all (contrastive) sentences in proper order\n",
    "files = glob('../data/contrastive_triplets/sentences_*.txt')\n",
    "files = sorted(files, key=extract_index)\n",
    "\n",
    "# Creating multi-negative contrastive dataset with positive sentences coming from triplets, \n",
    "# but negative sentences from other categories\n",
    "positive_contrastive_sentence_list = []\n",
    "single_negative_contrastive_sentence_list = []\n",
    "for file in files:\n",
    "    with open(file, 'r', encoding='utf-8') as f:\n",
    "        lines = [line.strip() for line in f if line.strip()]\n",
    "        positive_contrastive_sentence_list.extend([line.replace('positivo: ', '').lower() \\\n",
    "                                                   for line in lines[::2]])\n",
    "        single_negative_contrastive_sentence_list.extend(\n",
    "                                                    [line.replace('negativo: ','').lower() \\\n",
    "                                                     for line in lines[1::2]]\n",
    "                                                  )\n",
    "\n",
    "# Creating columns for contrastive sentences\n",
    "ind_df['positive_contrastive'] = ''\n",
    "ind_df.loc[ind_df['descricao_resumida'].isna(), 'positive_contrastive'] = pd.NA\n",
    "ind_df.loc[ind_df['descricao_resumida'].notna(), \\\n",
    "           'positive_contrastive'] = positive_contrastive_sentence_list\n",
    "\n",
    "ind_df['single_negative_contrastive'] = ''\n",
    "ind_df.loc[ind_df['descricao_resumida'].isna(), 'single_negative_contrastive'] = pd.NA\n",
    "ind_df.loc[ind_df['descricao_resumida'].notna(), \\\n",
    "           'single_negative_contrastive'] = single_negative_contrastive_sentence_list\n",
    "\n",
    "# For multi-negative, we are going to find 5 sentences from other categories\n",
    "ind_df['multi_negative_contrastive'] = ''\n",
    "ind_df.loc[ind_df['descricao_resumida'].isna(), 'multi_negative_contrastive'] = pd.NA\n",
    "\n",
    "# Creating dictionary of samples to make sampling faster\n",
    "sampling_dict = {c: ind_df.loc[(ind_df['categoria'] != c) & \\\n",
    "                               (ind_df['descricao_resumida'].notna()), \\\n",
    "                               'descricao_resumida'] for c in ind_df['categoria'].unique()}\n",
    "\n",
    "# Iterating the dataframe and sampling from other 'categoria'\n",
    "n_samples = 10\n",
    "for index, row in ind_df.loc[ind_df['descricao_resumida'].notna()].iterrows():\n",
    "    current_categoria = row['categoria']\n",
    "    sampled_list = sampling_dict[current_categoria].sample(n=n_samples, replace=False).tolist()\n",
    "    ind_df.at[index, 'multi_negative_contrastive'] = sampled_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "728a3946-50d2-4207-b291-f949b5a09529",
   "metadata": {},
   "source": [
    "## BERTimbau"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "924a164f-928d-446a-9da5-1b47265fa48a",
   "metadata": {},
   "source": [
    "### Vanilla Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec994889-d58e-4bf4-bc27-07e7c9e885fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting device\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5ff6dff-82d0-4b1b-80be-bf1e25145b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing model, turning it into eval mode and zeroing out the gradients\n",
    "# base_model = AutoModel.from_pretrained('neuralmind/bert-large-portuguese-cased')\n",
    "base_model = AutoModel.from_pretrained('neuralmind/bert-base-portuguese-cased')\n",
    "base_model = base_model.to(device)\n",
    "base_model.eval()\n",
    "base_model.zero_grad()\n",
    "\n",
    "# Getting sentences' dataset, dataloader and splits\n",
    "text_ind_df = ind_df[~ind_df['descricao_resumida'].isna()]\n",
    "max_length = 64\n",
    "text_dataset = UnsupervisedTextDataset(text_ind_df, tokenizer, max_length=max_length)\n",
    "full_batch_size = 1\n",
    "text_dataloader = get_dataloaders(text_dataset, full_batch_size)\n",
    "\n",
    "data_size = len(text_dataset)\n",
    "train_size = int(0.8*data_size)\n",
    "val_size = int(0.1*data_size)\n",
    "test_size = data_size - train_size - val_size\n",
    "splits = [train_size, val_size, test_size]\n",
    "split_batch_size = 32\n",
    "text_dataset_splits, text_dataloader_splits = get_dataloaders(text_dataset, \\\n",
    "                                                              split_batch_size, splits)\n",
    "\n",
    "# Initializing baseline input_ids and attention_mask, and computing its embedding\n",
    "baseline_input_ids = torch.full((1, max_length), tokenizer.pad_token_id).to(device)\n",
    "baseline_input_ids[:, 0] = tokenizer.cls_token_id\n",
    "baseline_input_ids[:, -1] = tokenizer.sep_token_id\n",
    "baseline_attention_mask = torch.zeros_like(baseline_input_ids).to(device)\n",
    "baseline_attention_mask[:, 0] = 1\n",
    "baseline_attention_mask[:, -1] = 1\n",
    "with torch.no_grad():\n",
    "    baseline_outputs = base_model(input_ids=baseline_input_ids, \\\n",
    "                                  attention_mask=baseline_attention_mask)\n",
    "    baseline_embedding = baseline_outputs.last_hidden_state\n",
    "    sum_embedding = (baseline_embedding*baseline_attention_mask.unsqueeze(-1).float())\\\n",
    "                    .sum(dim=1)\n",
    "    lengths = baseline_attention_mask.unsqueeze(-1).float().sum(dim=1).clamp(min=1e-9)\n",
    "    baseline_embedding = sum_embedding/lengths\n",
    "\n",
    "# Initializing captum compatible model and layer integrated gradients. We use layer integrated\n",
    "# gradients here because we can't compute gradients with respect to (discrete) indices\n",
    "# directly, so we compute gradients with respect to the embeddings of the input tokens\n",
    "vanilla_wrapped_model = CaptumWrappedModel(base_model, tokenizer.pad_token_id, \\\n",
    "                                           baseline_embedding, device, target_type='cos-sim')\n",
    "vanilla_wrapped_model.eval()\n",
    "\n",
    "lig = LayerIntegratedGradients(vanilla_wrapped_model, base_model.embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fffd0d67-5caf-4ae3-8fb9-001018053b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking model's STS-B score for reference\n",
    "_ = stsb_test(base_model, device, tokenizer, max_length=max_length, model_loss='vanilla', \\\n",
    "              verbose=True)\n",
    "\n",
    "# Checking model's in-context STS-B score for reference too\n",
    "subset = text_dataset_splits[-1]\n",
    "positional_indices = subset.indices\n",
    "test_indices = subset.dataset.df.index[positional_indices].tolist()\n",
    "test_df = ind_df.loc[test_indices, ['descricao_resumida', 'positive_contrastive', \\\n",
    "                                'single_negative_contrastive']]\n",
    "_ = in_context_stsb_test(base_model, device, tokenizer, test_df, max_length=max_length, \\\n",
    "                         model_loss='vanilla', verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88390f55-9681-455d-86af-4050b3e5014d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterating through the dataloader to compute embeddings and token attributions\n",
    "analyzed_sample = 0\n",
    "vanilla_bertimbau_indices = []\n",
    "vanilla_bertimbau_embeddings = []\n",
    "vanilla_bertimbau_tokens = []\n",
    "vanilla_bertimbau_attributions = []\n",
    "for indices, input_ids in tqdm(text_dataloader):\n",
    "    # Saving indices\n",
    "    vanilla_bertimbau_indices.append(indices)\n",
    "    \n",
    "    # Moving appropriate tensors to device\n",
    "    input_ids = input_ids.to(device)\n",
    "\n",
    "    # Computing mean pooling embeddings\n",
    "    mp_embeddings = get_embeddings(base_model, tokenizer, input_ids, device, fine_tuned=False)\n",
    "    vanilla_bertimbau_embeddings.append(mp_embeddings.cpu().detach())\n",
    "\n",
    "    # Computing attributions\n",
    "    tokens, attributions, delta = get_attributions(lig, tokenizer, input_ids, \\\n",
    "                                                   baseline_input_ids, \\\n",
    "                                                   attrib_aggreg_type='l2-norm', \\\n",
    "                                                   return_tokens=True, \\\n",
    "                                                   verbose=False, \\\n",
    "                                                   sample_num=min(analyzed_sample, \\\n",
    "                                                                  full_batch_size-1))\n",
    "    vanilla_bertimbau_tokens.append(tokens)\n",
    "    vanilla_bertimbau_attributions.append(attributions)\n",
    "    \n",
    "# Concatenating the batches\n",
    "vanilla_bertimbau_indices = np.array(vanilla_bertimbau_indices).squeeze(-1)\n",
    "vanilla_bertimbau_embeddings = np.array(torch.cat(vanilla_bertimbau_embeddings, dim=0))\n",
    "vanilla_bertimbau_tokens = np.array(vanilla_bertimbau_tokens)\n",
    "vanilla_bertimbau_attributions = np.array(torch.cat(vanilla_bertimbau_attributions, dim=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2834f8c9-2de2-44c7-af0f-7e4c26588c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting data projections\n",
    "vanilla_bertimbau_trimap, vanilla_bertimbau_tsne, \\\n",
    "vanilla_bertimbau_umap = data_projections(vanilla_bertimbau_embeddings)\n",
    "\n",
    "# Normalizing data for later plot on tool\n",
    "norm_factor = 12\n",
    "vanilla_bertimbau_trimap = normalize(vanilla_bertimbau_trimap, norm_factor)\n",
    "vanilla_bertimbau_tsne = normalize(vanilla_bertimbau_tsne, norm_factor)\n",
    "vanilla_bertimbau_umap = normalize(vanilla_bertimbau_umap, norm_factor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff2a6de6-5213-4f55-951c-69ab95f85256",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility function to plot rows on projection comparison plot\n",
    "def row_scatter_plot(projs, proj_names, row, color, rows=1, cols=3):\n",
    "    for i, (proj, proj_name) in enumerate(zip(projs, proj_names)):\n",
    "        plt.subplot(rows, cols, i+1+(cols*(row-1)))\n",
    "        plt.scatter(proj[:, 0], proj[:, 1], c=color)\n",
    "        plt.title(f\"{proj_name}\")\n",
    "        plt.xlabel(\"\")\n",
    "        plt.ylabel(\"\")\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "\n",
    "# Visualizing resulting projections\n",
    "plt.figure(figsize=(10,4))\n",
    "# plt.suptitle('Comparing Projections of BERTimbau Models')\n",
    "\n",
    "# Plotting vanilla BERTimbau projections\n",
    "projs = [vanilla_bertimbau_trimap, vanilla_bertimbau_tsne, vanilla_bertimbau_umap]\n",
    "proj_names = ['Vanilla BERTimbau with TriMap', 'Vanilla BERTimbau with t-SNE', \\\n",
    "              'Vanilla BERTimbau with UMAP']\n",
    "row_scatter_plot(projs, proj_names, 1, 'b', 1, )\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41e8e1a7-e593-405e-8ba0-947a383506a6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Saving outputs for visualization tool (trimap)\n",
    "_ = saving_outputs(vanilla_bertimbau_trimap, vanilla_bertimbau_tokens, \\\n",
    "                   vanilla_bertimbau_attributions, vanilla_bertimbau_indices, \\\n",
    "                   save_file='vanilla_bertimbau_trimap.csv')\n",
    "\n",
    "# Saving outputs for visualization tool (umap)\n",
    "_ = saving_outputs(vanilla_bertimbau_umap, vanilla_bertimbau_tokens, \\\n",
    "                   vanilla_bertimbau_attributions, vanilla_bertimbau_indices, \\\n",
    "                   save_file='vanilla_bertimbau_umap.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4deb8fa1-b3cd-4e8c-add2-f595995de51b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning memory\n",
    "clean_mem([vanilla_bertimbau_embeddings, vanilla_bertimbau_trimap, vanilla_bertimbau_tsne, \\\n",
    "vanilla_bertimbau_umap, vanilla_bertimbau_attributions, vanilla_bertimbau_indices])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f632b765-ef9d-453e-a9fc-76b55f8cd45a",
   "metadata": {},
   "source": [
    "### Unsupervised SimCSE (Contrastive Learning with no Labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fac0bf6a-842d-4149-9979-910b2515111f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Initializing USimCSE model and auxiliar variables\n",
    "dropout_prob = 0.1\n",
    "model = USimCSEModel(base_model, tokenizer.pad_token_id, device, dropout_prob)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-6)\n",
    "epochs = 15\n",
    "temperature = 0.05 # Test 0.01, 0.05, 0.1, 0.2\n",
    "patience = max(3, math.ceil(epochs*0.1))\n",
    "model_name = 'usimcse_bertimbau'\n",
    "train_dataloader, val_dataloader, test_dataloader = text_dataloader_splits\n",
    "\n",
    "# Calling training process\n",
    "usimcse_indices, usimcse_embeddings, \\\n",
    "train_losses, val_losses, stsb_track, \\\n",
    "in_context_stsb_track = usimcse_training_loop(model, tokenizer, optimizer, train_dataloader, \\\n",
    "                                              val_dataloader, text_dataset_splits[-1], \\\n",
    "                                              ind_df, device, epochs, temperature, patience, \\\n",
    "                                              model_name)\n",
    "\n",
    "plot_training_curves(train_losses, val_losses, stsb_track, in_context_stsb_track, model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce5290b3-3c90-4ac9-9019-5e9699aaa2a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading best model\n",
    "model.load_state_dict(torch.load(f'../data/models_weights/{model_name}.pth', \\\n",
    "                                 map_location=device)['model_state_dict'])\n",
    "\n",
    "# Re-checking model's scores after fine-tuning for comparison\n",
    "_ = stsb_test(model, device, tokenizer, max_length=max_length, model_loss='usimcse', \\\n",
    "              verbose=True)\n",
    "_ = in_context_stsb_test(model, device, tokenizer, test_df, max_length=max_length, \\\n",
    "                         model_loss='usimcse', verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8ef1adf-3894-4965-afef-a2e679eac9eb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Recomputing baseline embedding for fine-tuned model\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    baseline_embedding = model(input_ids=baseline_input_ids)\n",
    "\n",
    "# Instanciating attribution computation class for fine-tuned model\n",
    "usimcse_wrapped_model = CaptumWrappedModel(model.model, tokenizer.pad_token_id, \\\n",
    "                                           baseline_embedding, device, target_type='cos-sim')\n",
    "usimcse_wrapped_model.eval()\n",
    "lig = LayerIntegratedGradients(usimcse_wrapped_model, model.model.embeddings)\n",
    "\n",
    "# Iterating through the dataloader to compute token attributions for fine-tuned model\n",
    "analyzed_sample = 0\n",
    "usimcse_bertimbau_indices = []\n",
    "usimcse_bertimbau_embeddings = []\n",
    "usimcse_bertimbau_attributions = []\n",
    "for indices, input_ids in tqdm(text_dataloader):\n",
    "    # Saving indices\n",
    "    usimcse_bertimbau_indices.append(indices)\n",
    "    \n",
    "    # Moving appropriate tensors to device\n",
    "    input_ids = input_ids.to(device)\n",
    "\n",
    "    # Computing mean pooling embeddings\n",
    "    mp_embeddings = get_embeddings(model, tokenizer, input_ids, device, fine_tuned=True)\n",
    "    usimcse_bertimbau_embeddings.append(mp_embeddings.cpu().detach())\n",
    "\n",
    "    # Computing attributions\n",
    "    attributions, delta = get_attributions(lig, tokenizer, input_ids, baseline_input_ids, \\\n",
    "                                           attrib_aggreg_type='l2-norm', \\\n",
    "                                           return_tokens=False, verbose=False, \\\n",
    "                                           sample_num=min(analyzed_sample, full_batch_size-1))\n",
    "    usimcse_bertimbau_attributions.append(attributions)\n",
    "\n",
    "# Concatenating the batches\n",
    "usimcse_bertimbau_indices = np.array(usimcse_bertimbau_indices).squeeze(-1)\n",
    "usimcse_bertimbau_embeddings = np.array(torch.cat(usimcse_bertimbau_embeddings, dim=0))\n",
    "usimcse_bertimbau_attributions = np.array(torch.cat(usimcse_bertimbau_attributions, dim=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1b75b63-07ab-4a90-bd13-7c49207d2b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting data projections\n",
    "usimcse_bertimbau_trimap, usimcse_bertimbau_tsne, \\\n",
    "usimcse_bertimbau_umap = data_projections(usimcse_bertimbau_embeddings)\n",
    "\n",
    "# Normalizing data for later plot on tool\n",
    "norm_factor = 12\n",
    "usimcse_bertimbau_trimap = normalize(usimcse_bertimbau_trimap, norm_factor)\n",
    "usimcse_bertimbau_tsne = normalize(usimcse_bertimbau_tsne, norm_factor)\n",
    "usimcse_bertimbau_umap = normalize(usimcse_bertimbau_umap, norm_factor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e04054e-2c39-435a-bffd-58405a73fb8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing resulting projections\n",
    "plt.figure(figsize=(10,4))\n",
    "\n",
    "# Plotting USimCSE BERTimbau projections\n",
    "projs = [usimcse_bertimbau_trimap, usimcse_bertimbau_tsne, usimcse_bertimbau_umap]\n",
    "proj_names = ['USimCSE BERTimbau with TriMap', 'USimCSE BERTimbau with t-SNE', \\\n",
    "              'USimCSE BERTimbau with UMAP']\n",
    "row_scatter_plot(projs, proj_names, 1, 'b', 1, )\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4877fdc2-2f86-4af4-818e-60a297f4bdbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving outputs for visualization tool (trimap)\n",
    "_ = saving_outputs(usimcse_bertimbau_trimap, vanilla_bertimbau_tokens, \\\n",
    "                   usimcse_bertimbau_attributions, usimcse_bertimbau_indices, \\\n",
    "                   save_file='usimcse_bertimbau_trimap.csv')\n",
    "\n",
    "# Saving outputs for visualization tool (umap)\n",
    "_ = saving_outputs(usimcse_bertimbau_umap, vanilla_bertimbau_tokens, \\\n",
    "                   usimcse_bertimbau_attributions, usimcse_bertimbau_indices, \\\n",
    "                   save_file='usimcse_bertimbau_umap.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d11843ef-fd4c-41c3-b452-8009b71e0f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning memory\n",
    "clean_mem([usimcse_bertimbau_embeddings, usimcse_bertimbau_trimap, usimcse_bertimbau_tsne, \\\n",
    "usimcse_bertimbau_umap, usimcse_bertimbau_attributions, usimcse_bertimbau_indices])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3d3a9bb-1928-4611-bbf3-e2853cb60e7e",
   "metadata": {},
   "source": [
    "###  Supervised InfoNCE (Contrastive Learning with Labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a67fb2f9-38e5-4c59-8d5d-2177b9cac768",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import language_training_utils\n",
    "importlib.reload(language_training_utils)\n",
    "from language_training_utils import *\n",
    "\n",
    "# Initializing InfoNCEModel model and auxiliar variables\n",
    "model = InfoNCEModel(base_model, tokenizer.pad_token_id, device, hidden_dim=768, proj_dim=512)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\n",
    "epochs = 15\n",
    "temperature = 0.07 # Test 0.03, 0.07, 0.1, 0.3\n",
    "patience = max(3, math.ceil(epochs*0.1))\n",
    "model_name = 'infonce_bertimbau'\n",
    "\n",
    "# Getting sentences' dataset, dataloader and splits again (now for a supervised dataset)\n",
    "max_length = 64\n",
    "text_dataset = SupervisedTextDataset(text_ind_df, tokenizer, max_length=max_length)\n",
    "full_batch_size = 1\n",
    "text_dataloader = get_dataloaders(text_dataset, full_batch_size)\n",
    "split_batch_size = 8\n",
    "text_dataset_splits, text_dataloader_splits = get_dataloaders(text_dataset, \\\n",
    "                                                              split_batch_size, splits)\n",
    "train_dataloader, val_dataloader, test_dataloader = text_dataloader_splits\n",
    "\n",
    "# Calling training process\n",
    "infonce_indices, infonce_embeddings, \\\n",
    "train_losses, val_losses, stsb_track, \\\n",
    "in_context_stsb_track = infonce_training_loop(model, tokenizer, optimizer, train_dataloader, \\\n",
    "                                              val_dataloader, text_dataset_splits[-1], \\\n",
    "                                              ind_df, device, epochs, temperature, patience, \\\n",
    "                                              model_name)\n",
    "\n",
    "plot_training_curves(train_losses, val_losses, stsb_track, in_context_stsb_track, model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c10d41e3-c402-4ac6-ba96-506a6a7f7438",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading best model\n",
    "model.load_state_dict(torch.load(f'../data/models_weights/{model_name}.pth', \\\n",
    "                                 map_location=device)['model_state_dict'])\n",
    "\n",
    "# Re-checking model's scores after fine-tuning for comparison\n",
    "_ = stsb_test(model, device, tokenizer, max_length=max_length, model_loss='infonce', \\\n",
    "              verbose=True)\n",
    "\n",
    "subset = text_dataset_splits[-1]\n",
    "positional_indices = subset.indices\n",
    "test_indices = subset.dataset.df.index[positional_indices].tolist()\n",
    "test_df = ind_df.loc[test_indices, ['descricao_resumida', 'positive_contrastive', \\\n",
    "                                'single_negative_contrastive']]\n",
    "_ = in_context_stsb_test(model, device, tokenizer, test_df, max_length=max_length, \\\n",
    "                         model_loss='infonce', verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b05725be-b4d5-460f-9baf-fb13806b7b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recomputing baseline embedding for fine-tuned model\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    baseline_embedding = model(input_ids=baseline_input_ids)\n",
    "\n",
    "# Instanciating attribution computation class for fine-tuned model\n",
    "infonce_wrapped_model = CaptumWrappedModel(model.model, tokenizer.pad_token_id, \\\n",
    "                                           baseline_embedding, device, target_type='cos-sim')\n",
    "infonce_wrapped_model.eval()\n",
    "lig = LayerIntegratedGradients(infonce_wrapped_model, model.model.embeddings)\n",
    "\n",
    "# Iterating through the dataloader to compute token attributions for fine-tuned model\n",
    "analyzed_sample = 0\n",
    "infonce_bertimbau_indices = []\n",
    "infonce_bertimbau_embeddings = []\n",
    "infonce_bertimbau_attributions = []\n",
    "for indices, input_ids in tqdm(text_dataloader):\n",
    "    # Saving indices\n",
    "    infonce_bertimbau_indices.append(indices)\n",
    "    \n",
    "    # Moving appropriate tensors to device\n",
    "    input_ids = input_ids.to(device)\n",
    "\n",
    "    # Computing mean pooling embeddings\n",
    "    mp_embeddings = get_embeddings(model, tokenizer, input_ids, device, fine_tuned=True)\n",
    "    infonce_bertimbau_embeddings.append(mp_embeddings.cpu().detach())\n",
    "\n",
    "    # Computing attributions\n",
    "    attributions, delta = get_attributions(lig, tokenizer, input_ids, baseline_input_ids, \\\n",
    "                                           attrib_aggreg_type='l2-norm', \\\n",
    "                                           return_tokens=False, verbose=False, \\\n",
    "                                           sample_num=min(analyzed_sample, full_batch_size-1))\n",
    "    infonce_bertimbau_attributions.append(attributions)\n",
    "\n",
    "# Concatenating the batches\n",
    "infonce_bertimbau_indices = np.array(infonce_bertimbau_indices).squeeze(-1)\n",
    "infonce_bertimbau_embeddings = np.array(torch.cat(infonce_bertimbau_embeddings, dim=0))\n",
    "infonce_bertimbau_attributions = np.array(torch.cat(infonce_bertimbau_attributions, dim=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5af338c9-73f5-4175-aa8f-661652ad8c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting data projections\n",
    "infonce_bertimbau_trimap, infonce_bertimbau_tsne, \\\n",
    "infonce_bertimbau_umap = data_projections(infonce_bertimbau_embeddings)\n",
    "\n",
    "# Normalizing data for later plot on tool\n",
    "norm_factor = 12\n",
    "infonce_bertimbau_trimap = normalize(infonce_bertimbau_trimap, norm_factor)\n",
    "infonce_bertimbau_tsne = normalize(infonce_bertimbau_tsne, norm_factor)\n",
    "infonce_bertimbau_umap = normalize(infonce_bertimbau_umap, norm_factor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fc30ed0-2a4b-4942-b776-01e3e9cdd461",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing resulting projections\n",
    "plt.figure(figsize=(10,4))\n",
    "\n",
    "# Plotting InfoNCE BERTimbau projections\n",
    "projs = [infonce_bertimbau_trimap, infonce_bertimbau_tsne, infonce_bertimbau_umap]\n",
    "proj_names = ['InfoNCE BERTimbau with TriMap', 'InfoNCE BERTimbau with t-SNE', \\\n",
    "              'InfoNCE BERTimbau with UMAP']\n",
    "row_scatter_plot(projs, proj_names, 1, 'b', 1, )\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a250b5b3-98e3-45ff-a624-3f41e81837c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving outputs for visualization tool (trimap)\n",
    "_ = saving_outputs(infonce_bertimbau_trimap, vanilla_bertimbau_tokens, \\\n",
    "                   infonce_bertimbau_attributions, infonce_bertimbau_indices, \\\n",
    "                   save_file='infonce_bertimbau_trimap.csv')\n",
    "\n",
    "# Saving outputs for visualization tool (umap)\n",
    "_ = saving_outputs(infonce_bertimbau_umap, vanilla_bertimbau_tokens, \\\n",
    "                   infonce_bertimbau_attributions, infonce_bertimbau_indices, \\\n",
    "                   save_file='infonce_bertimbau_umap.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd527a39-a03b-4d86-8e34-ba36bb22c1cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning memory\n",
    "clean_mem([infonce_bertimbau_embeddings, infonce_bertimbau_trimap, infonce_bertimbau_tsne, \\\n",
    "infonce_bertimbau_umap, infonce_bertimbau_attributions, infonce_bertimbau_indices])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e15e53a6-f8b8-4ad7-b6e5-0c052f0cb244",
   "metadata": {},
   "source": [
    "## Albertina"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32bc9662-f5aa-4239-a286-131b58ad1174",
   "metadata": {},
   "source": [
    "### Vanilla Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17bd2217-556b-4979-a3c8-b99da09acf04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting new tokenizer\n",
    "# tokenizer = AutoTokenizer.from_pretrained('PORTULAN/albertina-1b5-portuguese-ptbr-encoder', \\\n",
    "#                                           use_fast=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained('PORTULAN/albertina-100m-portuguese-ptbr-encoder', \\\n",
    "                                          use_fast=True)\n",
    "\n",
    "# Initializing model, turning it into eval mode and zeroing out the gradients\n",
    "# model = AutoModel.from_pretrained('PORTULAN/albertina-1b5-portuguese-ptbr-encoder')\n",
    "base_model = AutoModel.from_pretrained('PORTULAN/albertina-100m-portuguese-ptbr-encoder')\n",
    "base_model = base_model.to(device)\n",
    "base_model.eval()\n",
    "base_model.zero_grad()\n",
    "\n",
    "# Getting sentences' dataset, dataloader and splits\n",
    "max_length = 64\n",
    "text_dataset = UnsupervisedTextDataset(text_ind_df, tokenizer, max_length=max_length)\n",
    "full_batch_size = 1\n",
    "text_dataloader = get_dataloaders(text_dataset, full_batch_size)\n",
    "\n",
    "data_size = len(text_dataset)\n",
    "train_size = int(0.8*data_size)\n",
    "val_size = int(0.1*data_size)\n",
    "test_size = data_size - train_size - val_size\n",
    "splits = [train_size, val_size, test_size]\n",
    "split_batch_size = 32\n",
    "text_dataset_splits, text_dataloader_splits = get_dataloaders(text_dataset, \\\n",
    "                                                              split_batch_size, splits)\n",
    "\n",
    "# Initializing baseline input_ids and attention_mask, and computing its embedding\n",
    "baseline_input_ids = torch.full((1, max_length), tokenizer.pad_token_id).to(device)\n",
    "baseline_input_ids[:, 0] = tokenizer.cls_token_id\n",
    "baseline_input_ids[:, -1] = tokenizer.sep_token_id\n",
    "baseline_attention_mask = torch.zeros_like(baseline_input_ids).to(device)\n",
    "baseline_attention_mask[:, 0] = 1\n",
    "baseline_attention_mask[:, -1] = 1\n",
    "with torch.no_grad():\n",
    "    baseline_outputs = base_model(input_ids=baseline_input_ids, \\\n",
    "                                  attention_mask=baseline_attention_mask)\n",
    "    baseline_embedding = baseline_outputs.last_hidden_state\n",
    "    sum_embedding = (baseline_embedding*baseline_attention_mask.unsqueeze(-1).float())\n",
    "                    .sum(dim=1)\n",
    "    lengths = baseline_attention_mask.unsqueeze(-1).float().sum(dim=1).clamp(min=1e-9)\n",
    "    baseline_embedding = sum_embedding/lengths\n",
    "\n",
    "# Initializing captum compatible model and layer integrated gradients. We use layer integrated\n",
    "# gradients here because we can't compute gradients with respect to (discrete) indices\n",
    "# directly, so we compute gradients with respect to the embeddings of the input tokens\n",
    "vanilla_wrapped_model = CaptumWrappedModel(base_model, tokenizer.pad_token_id, \\\n",
    "                                           baseline_embedding, device, target_type='cos-sim')\n",
    "vanilla_wrapped_model.eval()\n",
    "\n",
    "lig = LayerIntegratedGradients(vanilla_wrapped_model, base_model.embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7caa3e93-0a23-4ec2-bcec-562e185c3e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking model's STS-B score for reference\n",
    "_ = stsb_test(base_model, device, tokenizer, max_length=max_length, model_loss='vanilla', \\\n",
    "              verbose=True)\n",
    "\n",
    "# Checking model's in-context STS-B score for reference too\n",
    "subset = text_dataset_splits[-1]\n",
    "positional_indices = subset.indices\n",
    "test_indices = subset.dataset.df.index[positional_indices].tolist()\n",
    "test_df = ind_df.loc[test_indices, ['descricao_resumida', 'positive_contrastive', \\\n",
    "                                'single_negative_contrastive']]\n",
    "_ = in_context_stsb_test(base_model, device, tokenizer, test_df, max_length=max_length, \\\n",
    "                         model_loss='vanilla', verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dbecce6-97a1-4fb4-8ad8-f4aa5b2e7e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterating through the dataloader to compute embeddings and token attributions\n",
    "analyzed_sample = 0\n",
    "vanilla_albertina_indices = []\n",
    "vanilla_albertina_embeddings = []\n",
    "vanilla_albertina_tokens = []\n",
    "vanilla_albertina_attributions = []\n",
    "for indices, input_ids, _ in tqdm(text_dataloader):\n",
    "    # Saving indices\n",
    "    vanilla_albertina_indices.append(indices)\n",
    "    \n",
    "    # Moving appropriate tensors to device\n",
    "    input_ids = input_ids.to(device)\n",
    "\n",
    "    # Computing mean pooling embeddings\n",
    "    mp_embeddings = get_embeddings(base_model, tokenizer, input_ids, device, fine_tuned=False)\n",
    "    vanilla_albertina_embeddings.append(mp_embeddings.cpu().detach())\n",
    "\n",
    "    # Computing attributions\n",
    "    tokens, attributions, delta = get_attributions(lig, tokenizer, input_ids, \\\n",
    "                                                   baseline_input_ids, \\\n",
    "                                                   attrib_aggreg_type='l2-norm', \\\n",
    "                                                   return_tokens=True, \\\n",
    "                                                   verbose=False, \\\n",
    "                                                   sample_num=min(analyzed_sample, \\\n",
    "                                                                  full_batch_size-1))\n",
    "    vanilla_albertina_tokens.append(tokens)\n",
    "    vanilla_albertina_attributions.append(attributions)\n",
    "    \n",
    "# Concatenating the batches\n",
    "vanilla_albertina_indices = np.array(vanilla_albertina_indices).squeeze(-1)\n",
    "vanilla_albertina_embeddings = np.array(torch.cat(vanilla_albertina_embeddings, dim=0))\n",
    "vanilla_albertina_tokens = np.array(vanilla_albertina_tokens)\n",
    "vanilla_albertina_attributions = np.array(torch.cat(vanilla_albertina_attributions, dim=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a09e7ea9-6738-4a7c-9f87-5318420f7f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting data projections\n",
    "vanilla_albertina_trimap, vanilla_albertina_tsne, \\\n",
    "vanilla_albertina_umap = data_projections(vanilla_albertina_embeddings)\n",
    "\n",
    "# Normalizing data for later plot on tool\n",
    "norm_factor = 12\n",
    "vanilla_albertina_trimap = normalize(vanilla_albertina_trimap, norm_factor)\n",
    "vanilla_albertina_tsne = normalize(vanilla_albertina_tsne, norm_factor)\n",
    "vanilla_albertina_umap = normalize(vanilla_albertina_umap, norm_factor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "892f6f07-a7fc-4ce4-9623-2179ed4bc6c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing resulting projections\n",
    "plt.figure(figsize=(10,4))\n",
    "\n",
    "# Plotting USimCSE BERTimbau projections\n",
    "projs = [vanilla_albertina_trimap, vanilla_albertina_tsne, vanilla_albertina_umap]\n",
    "proj_names = ['Vanilla Albertina with TriMap', 'Vanilla Albertina with t-SNE', \\\n",
    "              'Vanilla Albertina with UMAP']\n",
    "row_scatter_plot(projs, proj_names, 1, 'b', 1, )\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0359e963-9800-4b93-bfae-6b0193b988ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving outputs for visualization tool (trimap)\n",
    "_ = saving_outputs(vanilla_albertina_trimap, vanilla_albertina_tokens, \\\n",
    "                   vanilla_albertina_attributions, vanilla_albertina_indices, \\\n",
    "                   save_file='vanilla_albertina_trimap.csv')\n",
    "\n",
    "# Saving outputs for visualization tool (umap)\n",
    "_ = saving_outputs(vanilla_albertina_umap, vanilla_albertina_tokens, \\\n",
    "                   vanilla_albertina_attributions, vanilla_albertina_indices, \\\n",
    "                   save_file='vanilla_albertina_umap.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f4656d7-219f-4034-b9ef-54183f54e39c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning memory\n",
    "clean_mem([vanilla_albertina_embeddings, vanilla_albertina_trimap, vanilla_albertina_tsne, \\\n",
    "vanilla_albertina_umap, vanilla_albertina_attributions, vanilla_albertina_indices])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fe0f92b-46c8-436e-af89-12ee26924f28",
   "metadata": {},
   "source": [
    "### Unsupervised SimCSE (Contrastive Learning with no Labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8de4b278-d428-44ae-806b-65803e8cce0c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Initializing USimCSE model and auxiliar variables\n",
    "dropout_prob = 0.1\n",
    "model = USimCSEModel(base_model, tokenizer.pad_token_id, device, dropout_prob)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-6)\n",
    "epochs = 15\n",
    "temperature = 0.05 # Test 0.01, 0.05, 0.1, 0.2\n",
    "patience = max(3, math.ceil(epochs*0.1))\n",
    "model_name = 'usimcse_albertina'\n",
    "train_dataloader, val_dataloader, test_dataloader = text_dataloader_splits\n",
    "\n",
    "# Calling training process\n",
    "usimcse_indices, usimcse_embeddings, \\\n",
    "train_losses, val_losses, stsb_track, \\\n",
    "in_context_stsb_track = usimcse_training_loop(model, tokenizer, optimizer, train_dataloader, \\\n",
    "                                              val_dataloader, text_dataset_splits[-1], \\\n",
    "                                              ind_df, device, epochs, temperature, patience, \\\n",
    "                                              model_name)\n",
    "\n",
    "plot_training_curves(train_losses, val_losses, stsb_track, in_context_stsb_track, model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a92a49d-a3a9-43a9-94eb-31ceeb9f44d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading best model\n",
    "model.load_state_dict(torch.load(f'../data/models_weights/{model_name}.pth', \\\n",
    "                                 map_location=device)['model_state_dict'])\n",
    "\n",
    "# Re-checking model's scores after fine-tuning for comparison\n",
    "_ = stsb_test(model, device, tokenizer, max_length=max_length, model_loss='usimcse', \\\n",
    "              verbose=True)\n",
    "_ = in_context_stsb_test(model, device, tokenizer, test_df, max_length=max_length, \\\n",
    "                         model_loss='usimcse', verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed6a7218-9109-413a-b7d7-940a9ec81994",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Recomputing baseline embedding for fine-tuned model\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    baseline_embedding = model(input_ids=baseline_input_ids)\n",
    "\n",
    "# Instanciating attribution computation class for fine-tuned model\n",
    "usimcse_wrapped_model = CaptumWrappedModel(model.model, tokenizer.pad_token_id, \\\n",
    "                                           baseline_embedding, device, target_type='cos-sim')\n",
    "usimcse_wrapped_model.eval()\n",
    "lig = LayerIntegratedGradients(usimcse_wrapped_model, model.model.embeddings)\n",
    "\n",
    "# Iterating through the dataloader to compute token attributions for fine-tuned model\n",
    "analyzed_sample = 0\n",
    "usimcse_albertina_indices = []\n",
    "usimcse_albertina_embeddings = []\n",
    "usimcse_albertina_attributions = []\n",
    "for indices, input_ids in tqdm(text_dataloader):\n",
    "    # Saving indices\n",
    "    usimcse_albertina_indices.append(indices)\n",
    "    \n",
    "    # Moving appropriate tensors to device\n",
    "    input_ids = input_ids.to(device)\n",
    "\n",
    "    # Computing mean pooling embeddings\n",
    "    mp_embeddings = get_embeddings(model, tokenizer, input_ids, device, fine_tuned=True)\n",
    "    usimcse_albertina_embeddings.append(mp_embeddings.cpu().detach())\n",
    "\n",
    "    # Computing attributions\n",
    "    attributions, delta = get_attributions(lig, tokenizer, input_ids, baseline_input_ids, \\\n",
    "                                           attrib_aggreg_type='l2-norm', \\\n",
    "                                           return_tokens=False, verbose=False, \\\n",
    "                                           sample_num=min(analyzed_sample, full_batch_size-1))\n",
    "    usimcse_albertina_attributions.append(attributions)\n",
    "\n",
    "# Concatenating the batches\n",
    "usimcse_albertina_indices = np.array(usimcse_albertina_indices).squeeze(-1)\n",
    "usimcse_albertina_embeddings = np.array(torch.cat(usimcse_albertina_embeddings, dim=0))\n",
    "usimcse_albertina_attributions = np.array(torch.cat(usimcse_albertina_attributions, dim=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a7c6d6d-a493-4c37-8afe-989fbfc559b9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Getting data projections\n",
    "usimcse_albertina_trimap, usimcse_albertina_tsne, \\\n",
    "usimcse_albertina_umap = data_projections(usimcse_albertina_embeddings)\n",
    "\n",
    "# Normalizing data for later plot on tool\n",
    "norm_factor = 12\n",
    "usimcse_albertina_trimap = normalize(usimcse_albertina_trimap, norm_factor)\n",
    "usimcse_albertina_tsne = normalize(usimcse_albertina_tsne, norm_factor)\n",
    "usimcse_albertina_umap = normalize(usimcse_albertina_umap, norm_factor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f2a93f5-021d-4740-889d-b4dd74c3a1dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing resulting projections\n",
    "plt.figure(figsize=(10,4))\n",
    "\n",
    "# Plotting USimCSE Albertina projections\n",
    "projs = [usimcse_albertina_trimap, usimcse_albertina_tsne, usimcse_albertina_umap]\n",
    "proj_names = ['USimCSE Albertina with TriMap', 'USimCSE Albertina with t-SNE', \\\n",
    "              'USimCSE Albertina with UMAP']\n",
    "row_scatter_plot(projs, proj_names, 1, 'b', 1, )\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "630fc96f-21b5-46f9-9c20-3b581e333cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving outputs for visualization tool (trimap)\n",
    "_ = saving_outputs(usimcse_albertina_trimap, vanilla_albertina_tokens, \\\n",
    "                   usimcse_albertina_attributions, usimcse_albertina_indices, \\\n",
    "                   save_file='usimcse_albertina_trimap.csv')\n",
    "\n",
    "# Saving outputs for visualization tool (umap)\n",
    "_ = saving_outputs(usimcse_albertina_umap, vanilla_albertina_tokens, \\\n",
    "                   usimcse_albertina_attributions, usimcse_albertina_indices, \\\n",
    "                   save_file='usimcse_albertina_umap.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdfa7aa5-4249-4105-96e1-c4409f12ac79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning memory\n",
    "clean_mem([usimcse_albertina_embeddings, usimcse_albertina_trimap, usimcse_albertina_tsne, \\\n",
    "usimcse_albertina_umap, usimcse_albertina_attributions, usimcse_albertina_indices])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb997016-4942-4c63-b4a3-091b919026e5",
   "metadata": {},
   "source": [
    "###  Supervised InfoNCE (Contrastive Learning with Labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f24cfa-0e37-4760-a049-329ea150ecc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing InfoNCEModel model and auxiliar variables\n",
    "model = InfoNCEModel(base_model, tokenizer.pad_token_id, device, hidden_dim=768, proj_dim=512)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\n",
    "epochs = 15\n",
    "temperature = 0.07 # Test 0.03, 0.07, 0.1, 0.3\n",
    "patience = max(3, math.ceil(epochs*0.1))\n",
    "model_name = 'infonce_bertimbau'\n",
    "\n",
    "# Getting sentences' dataset, dataloader and splits again (now for a supervised dataset)\n",
    "max_length = 64\n",
    "text_dataset = SupervisedTextDataset(text_ind_df, tokenizer, max_length=max_length)\n",
    "full_batch_size = 1\n",
    "text_dataloader = get_dataloaders(text_dataset, full_batch_size)\n",
    "split_batch_size = 8\n",
    "text_dataset_splits, text_dataloader_splits = get_dataloaders(text_dataset, \\\n",
    "                                                              split_batch_size, splits)\n",
    "train_dataloader, val_dataloader, test_dataloader = text_dataloader_splits\n",
    "\n",
    "# Calling training process\n",
    "infonce_indices, infonce_embeddings, \\\n",
    "train_losses, val_losses, stsb_track, \\\n",
    "in_context_stsb_track = infonce_training_loop(model, tokenizer, optimizer, train_dataloader, \\\n",
    "                                              val_dataloader, text_dataset_splits[-1], \\\n",
    "                                              ind_df, device, epochs, temperature, patience, \\\n",
    "                                              model_name)\n",
    "\n",
    "plot_training_curves(train_losses, val_losses, stsb_track, in_context_stsb_track, model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef214b9-13f8-4ff7-9ba8-644b33dc7c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading best model\n",
    "model.load_state_dict(torch.load(f'../data/models_weights/{model_name}.pth', \\\n",
    "                                 map_location=device)['model_state_dict'])\n",
    "\n",
    "# Re-checking model's scores after fine-tuning for comparison\n",
    "_ = stsb_test(model, device, tokenizer, max_length=max_length, model_loss='infonce', \\\n",
    "              verbose=True)\n",
    "\n",
    "subset = text_dataset_splits[-1]\n",
    "positional_indices = subset.indices\n",
    "test_indices = subset.dataset.df.index[positional_indices].tolist()\n",
    "test_df = ind_df.loc[test_indices, ['descricao_resumida', 'positive_contrastive', \\\n",
    "                                'single_negative_contrastive']]\n",
    "_ = in_context_stsb_test(model, device, tokenizer, test_df, max_length=max_length, \\\n",
    "                         model_loss='infonce', verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74f494ca-f48b-4b98-a973-28abb9806c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recomputing baseline embedding for fine-tuned model\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    baseline_embedding = model(input_ids=baseline_input_ids)\n",
    "\n",
    "# Instanciating attribution computation class for fine-tuned model\n",
    "infonce_wrapped_model = CaptumWrappedModel(model.model, tokenizer.pad_token_id, \\\n",
    "                                           baseline_embedding, device, target_type='cos-sim')\n",
    "infonce_wrapped_model.eval()\n",
    "lig = LayerIntegratedGradients(infonce_wrapped_model, model.model.embeddings)\n",
    "\n",
    "# Iterating through the dataloader to compute token attributions for fine-tuned model\n",
    "analyzed_sample = 0\n",
    "infonce_albertina_indices = []\n",
    "infonce_albertina_embeddings = []\n",
    "infonce_albertina_attributions = []\n",
    "for indices, input_ids in tqdm(text_dataloader):\n",
    "    # Saving indices\n",
    "    infonce_albertina_indices.append(indices)\n",
    "    \n",
    "    # Moving appropriate tensors to device\n",
    "    input_ids = input_ids.to(device)\n",
    "\n",
    "    # Computing mean pooling embeddings\n",
    "    mp_embeddings = get_embeddings(model, tokenizer, input_ids, device, fine_tuned=True)\n",
    "    infonce_albertina_embeddings.append(mp_embeddings.cpu().detach())\n",
    "\n",
    "    # Computing attributions\n",
    "    attributions, delta = get_attributions(lig, tokenizer, input_ids, baseline_input_ids, \\\n",
    "                                           attrib_aggreg_type='l2-norm', \\\n",
    "                                           return_tokens=False, verbose=False, \\\n",
    "                                           sample_num=min(analyzed_sample, full_batch_size-1))\n",
    "    infonce_albertina_attributions.append(attributions)\n",
    "\n",
    "# Concatenating the batches\n",
    "infonce_albertina_indices = np.array(infonce_albertina_indices).squeeze(-1)\n",
    "infonce_albertina_embeddings = np.array(torch.cat(infonce_albertina_embeddings, dim=0))\n",
    "infonce_albertina_attributions = np.array(torch.cat(infonce_albertina_attributions, dim=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "476bc5e8-de90-4227-a670-90ef7219ae19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting data projections\n",
    "infonce_albertina_trimap, infonce_albertina_tsne, \\\n",
    "infonce_albertina_umap = data_projections(infonce_albertina_embeddings)\n",
    "\n",
    "# Normalizing data for later plot on tool\n",
    "norm_factor = 12\n",
    "infonce_albertina_trimap = normalize(infonce_albertina_trimap, norm_factor)\n",
    "infonce_albertina_tsne = normalize(infonce_albertina_tsne, norm_factor)\n",
    "infonce_albertina_umap = normalize(infonce_albertina_umap, norm_factor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c8a94f5-b241-49d6-92a7-c70af6666abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing resulting projections\n",
    "plt.figure(figsize=(10,4))\n",
    "\n",
    "# Plotting InfoNCE Albertina projections\n",
    "projs = [infonce_albertina_trimap, infonce_albertina_tsne, infonce_albertina_umap]\n",
    "proj_names = ['InfoNCE Albertina with TriMap', 'InfoNCE Albertina with t-SNE', \\\n",
    "              'InfoNCE Albertina with UMAP']\n",
    "row_scatter_plot(projs, proj_names, 1, 'b', 1, )\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59fc88f3-92c9-4e49-adff-8faae7dfdad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving outputs for visualization tool (trimap)\n",
    "_ = saving_outputs(infonce_albertina_trimap, vanilla_albertina_tokens, \\\n",
    "                   infonce_albertina_attributions, infonce_albertina_indices, \\\n",
    "                   save_file='infonce_albertina_trimap.csv')\n",
    "\n",
    "# Saving outputs for visualization tool (umap)\n",
    "_ = saving_outputs(infonce_albertina_umap, vanilla_albertina_tokens, \\\n",
    "                   infonce_albertina_attributions, infonce_albertina_indices, \\\n",
    "                   save_file='infonce_albertina_umap.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e24f02a1-049b-4dcc-977a-dea849918c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning memory\n",
    "clean_mem([infonce_albertina_embeddings, infonce_albertina_trimap, infonce_albertina_tsne, \\\n",
    "infonce_albertina_umap, infonce_albertina_attributions, infonce_albertina_indices])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
